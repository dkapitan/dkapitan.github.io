[
  {
    "objectID": "learn/altair/altair_interaction.html",
    "href": "learn/altair/altair_interaction.html",
    "title": "Interaction",
    "section": "",
    "text": "“A graphic is not ‘drawn’ once and for all; it is ‘constructed’ and reconstructed until it reveals all the relationships constituted by the interplay of the data. The best graphic operations are those carried out by the decision-maker themself.” — Jacques Bertin\nVisualization provides a powerful means of making sense of data. A single image, however, typically provides answers to, at best, a handful of questions. Through interaction we can transform static images into tools for exploration: highlighting points of interest, zooming in to reveal finer-grained patterns, and linking across multiple views to reason about multi-dimensional relationships.\nAt the core of interaction is the notion of a selection: a means of indicating to the computer which elements or regions we are interested in. For example, we might hover the mouse over a point, click multiple marks, or draw a bounding box around a region to highlight subsets of the data for further scrutiny.\nAlongside visual encodings and data transformations, Altair provides a selection abstraction for authoring interactions. These selections encompass three aspects:\nThis notebook introduces interactive selections and explores how to use them to author a variety of interaction techniques, such as dynamic queries, panning & zooming, details-on-demand, and brushing & linking.\nThis notebook is part of the data visualization curriculum.\nimport pandas as pd\nimport altair as alt"
  },
  {
    "objectID": "learn/altair/altair_interaction.html#datasets",
    "href": "learn/altair/altair_interaction.html#datasets",
    "title": "Interaction",
    "section": "Datasets",
    "text": "Datasets\nWe will visualize a variety of datasets from the vega-datasets collection:\n\nA dataset of cars from the 1970s and early 1980s,\nA dataset of movies, previously used in the Data Transformation notebook,\nA dataset containing ten years of S&P 500 (sp500) stock prices,\nA dataset of technology company stocks, and\nA dataset of flights, including departure time, distance, and arrival delay.\n\n\ncars = 'https://cdn.jsdelivr.net/npm/vega-datasets@1/data/cars.json'\nmovies = 'https://cdn.jsdelivr.net/npm/vega-datasets@1/data/movies.json'\nsp500 = 'https://cdn.jsdelivr.net/npm/vega-datasets@1/data/sp500.csv'\nstocks = 'https://cdn.jsdelivr.net/npm/vega-datasets@1/data/stocks.csv'\nflights = 'https://cdn.jsdelivr.net/npm/vega-datasets@1/data/flights-5k.json'"
  },
  {
    "objectID": "learn/altair/altair_interaction.html#introducing-selections",
    "href": "learn/altair/altair_interaction.html#introducing-selections",
    "title": "Interaction",
    "section": "Introducing Selections",
    "text": "Introducing Selections\nLet’s start with a basic selection: simply clicking a point to highlight it. Using the cars dataset, we’ll start with a scatter plot of horsepower versus miles per gallon, with a color encoding for the number cylinders in the car engine.\nIn addition, we’ll create a selection instance by calling alt.selection_single(), indicating we want a selection defined over a single value. By default, the selection uses a mouse click to determine the selected value. To register a selection with a chart, we must add it using the .add_selection() method.\nOnce our selection has been defined, we can use it as a parameter for conditional encodings, which apply a different encoding depending on whether a data record lies in or out of the selection. For example, consider the following code:\ncolor=alt.condition(selection, 'Cylinders:O', alt.value('grey'))\nThis encoding definition states that data points contained within the selection should be colored according to the Cylinder field, while non-selected data points should use a default grey. An empty selection includes all data points, and so initially all points will be colored.\nTry clicking different points in the chart below. What happens? (Click the background to clear the selection state and return to an “empty” selection.)\n\nselection = alt.selection_single();\n  \nalt.Chart(cars).mark_circle().add_selection(\n    selection\n).encode(\n    x='Horsepower:Q',\n    y='Miles_per_Gallon:Q',\n    color=alt.condition(selection, 'Cylinders:O', alt.value('grey')),\n    opacity=alt.condition(selection, alt.value(0.8), alt.value(0.1))\n)\n\n\n\n\n\n\nOf course, highlighting individual data points one-at-a-time is not particularly exciting! As we’ll see, however, single value selections provide a useful building block for more powerful interactions. Moreover, single value selections are just one of the three selection types provided by Altair:\n\nselection_single - select a single discrete value, by default on click events.\nselection_multi - select multiple discrete values. The first value is selected on mouse click and additional values toggled using shift-click.\nselection_interval - select a continuous range of values, initiated by mouse drag.\n\nLet’s compare each of these selection types side-by-side. To keep our code tidy we’ll first define a function (plot) that generates a scatter plot specification just like the one above. We can pass a selection to the plot function to have it applied to the chart:\n\ndef plot(selection):\n    return alt.Chart(cars).mark_circle().add_selection(\n        selection\n    ).encode(\n        x='Horsepower:Q',\n        y='Miles_per_Gallon:Q',\n        color=alt.condition(selection, 'Cylinders:O', alt.value('grey')),\n        opacity=alt.condition(selection, alt.value(0.8), alt.value(0.1))\n    ).properties(\n        width=240,\n        height=180\n    )\n\nLet’s use our plot function to create three chart variants, one per selection type.\nThe first (single) chart replicates our earlier example. The second (multi) chart supports shift-click interactions to toggle inclusion of multiple points within the selection. The third (interval) chart generates a selection region (or brush) upon mouse drag. Once created, you can drag the brush around to select different points, or scroll when the cursor is inside the brush to scale (zoom) the brush size.\nTry interacting with each of the charts below!\n\nalt.hconcat(\n  plot(alt.selection_single()).properties(title='Single (Click)'),\n  plot(alt.selection_multi()).properties(title='Multi (Shift-Click)'),\n  plot(alt.selection_interval()).properties(title='Interval (Drag)')\n)\n\n\n\n\n\n\nThe examples above use default interactions (click, shift-click, drag) for each selection type. We can further customize the interactions by providing input event specifications using Vega event selector syntax. For example, we can modify our single and multi charts to trigger upon mouseover events instead of click events.\nHold down the shift key in the second chart to “paint” with data!\n\nalt.hconcat(\n  plot(alt.selection_single(on='mouseover')).properties(title='Single (Mouseover)'),\n  plot(alt.selection_multi(on='mouseover')).properties(title='Multi (Shift-Mouseover)')\n)\n\n\n\n\n\n\nNow that we’ve covered the basics of Altair selections, let’s take a tour through the various interaction techniques they enable!"
  },
  {
    "objectID": "learn/altair/altair_interaction.html#dynamic-queries",
    "href": "learn/altair/altair_interaction.html#dynamic-queries",
    "title": "Interaction",
    "section": "Dynamic Queries",
    "text": "Dynamic Queries\nDynamic queries enables rapid, reversible exploration of data to isolate patterns of interest. As defined by Ahlberg, Williamson, & Shneiderman, a dynamic query:\n\nrepresents a query graphically,\nprovides visible limits on the query range,\nprovides a graphical representation of the data and query result,\ngives immediate feedback of the result after every query adjustment,\nand allows novice users to begin working with little training.\n\nA common approach is to manipulate query parameters using standard user interface widgets such as sliders, radio buttons, and drop-down menus. To generate dynamic query widgets, we can apply a selection’s bind operation to one or more data fields we wish to query.\nLet’s build an interactive scatter plot that uses a dynamic query to filter the display. Given a scatter plot of movie ratings (from Rotten Tomates and IMDB), we can add a selection over the Major_Genre field to enable interactive filtering by film genre.\nTo start, let’s extract the unique (non-null) genres from the movies data:\n\ndf = pd.read_json(movies) # load movies data\ngenres = df['Major_Genre'].unique() # get unique field values\ngenres = list(filter(lambda d: d is not None, genres)) # filter out None values\ngenres.sort() # sort alphabetically\n\nFor later use, let’s also define a list of unique MPAA_Rating values:\n\nmpaa = ['G', 'PG', 'PG-13', 'R', 'NC-17', 'Not Rated']\n\nNow let’s create a single selection bound to a drop-down menu.\nUse the dynamic query menu below to explore the data. How do ratings vary by genre? How would you revise the code to filter MPAA_Rating (G, PG, PG-13, etc.) instead of Major_Genre?\n\nselectGenre = alt.selection_single(\n    name='Select', # name the selection 'Select'\n    fields=['Major_Genre'], # limit selection to the Major_Genre field\n    init={'Major_Genre': genres[0]}, # use first genre entry as initial value\n    bind=alt.binding_select(options=genres) # bind to a menu of unique genre values\n)\n\nalt.Chart(movies).mark_circle().add_selection(\n    selectGenre\n).encode(\n    x='Rotten_Tomatoes_Rating:Q',\n    y='IMDB_Rating:Q',\n    tooltip='Title:N',\n    opacity=alt.condition(selectGenre, alt.value(0.75), alt.value(0.05))\n)\n\n\n\n\n\n\nOur construction above leverages multiple aspects of selections:\n\nWe give the selection a name ('Select'). This name is not required, but allows us to influence the label text of the generated dynamic query menu. (What happens if you remove the name? Try it!)\nWe constrain the selection to a specific data field (Major_Genre). Earlier when we used a single selection, the selection mapped to individual data points. By limiting the selection to a specific field, we can select all data points whose Major_Genre field value matches the single selected value.\nWe initialize init=... the selection to a starting value.\nWe bind the selection to an interface widget, in this case a drop-down menu via binding_select.\nAs before, we then use a conditional encoding to control the opacity channel.\n\n\nBinding Selections to Multiple Inputs\nOne selection instance can be bound to multiple dynamic query widgets. Let’s modify the example above to provide filters for both Major_Genre and MPAA_Rating, using radio buttons instead of a menu. Our single selection is now defined over a single pair of genre and MPAA rating values\nLook for surprising conjunctions of genre and rating. Are there any G or PG-rated horror films?\n\n# single-value selection over [Major_Genre, MPAA_Rating] pairs\n# use specific hard-wired values as the initial selected values\nselection = alt.selection_single(\n    name='Select',\n    fields=['Major_Genre', 'MPAA_Rating'],\n    init={'Major_Genre': 'Drama', 'MPAA_Rating': 'R'},\n    bind={'Major_Genre': alt.binding_select(options=genres), 'MPAA_Rating': alt.binding_radio(options=mpaa)}\n)\n  \n# scatter plot, modify opacity based on selection\nalt.Chart(movies).mark_circle().add_selection(\n    selection\n).encode(\n    x='Rotten_Tomatoes_Rating:Q',\n    y='IMDB_Rating:Q',\n    tooltip='Title:N',\n    opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05))\n)\n\n\n\n\n\n\nFun facts: The PG-13 rating didn’t exist when the movies Jaws and Jaws 2 were released. The first film to receive a PG-13 rating was 1984’s Red Dawn.\n\n\nUsing Visualizations as Dynamic Queries\nThough standard interface widgets show the possible query parameter values, they do not visualize the distribution of those values. We might also wish to use richer interactions, such as multi-value or interval selections, rather than input widgets that select only a single value at a time.\nTo address these issues, we can author additional charts to both visualize data and support dynamic queries. Let’s add a histogram of the count of films per year and use an interval selection to dynamically highlight films over selected time periods.\nInteract with the year histogram to explore films from different time periods. Do you seen any evidence of sampling bias across the years? (How do year and critics’ ratings relate?)\nThe years range from 1930 to 2040! Are future films in pre-production, or are there “off-by-one century” errors? Also, depending on which time zone you’re in, you may see a small bump in either 1969 or 1970. Why might that be? (See the end of the notebook for an explanation!)\n\nbrush = alt.selection_interval(\n    encodings=['x'] # limit selection to x-axis (year) values\n)\n\n# dynamic query histogram\nyears = alt.Chart(movies).mark_bar().add_selection(\n    brush\n).encode(\n    alt.X('year(Release_Date):T', title='Films by Release Year'),\n    alt.Y('count():Q', title=None)\n).properties(\n    width=650,\n    height=50\n)\n\n# scatter plot, modify opacity based on selection\nratings = alt.Chart(movies).mark_circle().encode(\n    x='Rotten_Tomatoes_Rating:Q',\n    y='IMDB_Rating:Q',\n    tooltip='Title:N',\n    opacity=alt.condition(brush, alt.value(0.75), alt.value(0.05))\n).properties(\n    width=650,\n    height=400\n)\n\nalt.vconcat(years, ratings).properties(spacing=5)\n\n\n\n\n\n\nThe example above provides dynamic queries using a linked selection between charts:\n\nWe create an interval selection (brush), and set encodings=['x'] to limit the selection to the x-axis only, resulting in a one-dimensional selection interval.\nWe register brush with our histogram of films per year via .add_selection(brush).\nWe use brush in a conditional encoding to adjust the scatter plot opacity.\n\nThis interaction technique of selecting elements in one chart and seeing linked highlights in one or more other charts is known as brushing & linking."
  },
  {
    "objectID": "learn/altair/altair_interaction.html#panning-zooming",
    "href": "learn/altair/altair_interaction.html#panning-zooming",
    "title": "Interaction",
    "section": "Panning & Zooming",
    "text": "Panning & Zooming\nThe movie rating scatter plot is a bit cluttered in places, making it hard to examine points in denser regions. Using the interaction techniques of panning and zooming, we can inspect dense regions more closely.\nLet’s start by thinking about how we might express panning and zooming using Altair selections. What defines the “viewport” of a chart? Axis scale domains!\nWe can change the scale domains to modify the visualized range of data values. To do so interactively, we can bind an interval selection to scale domains with the code bind='scales'. The result is that instead of an interval brush that we can drag and zoom, we instead can drag and zoom the entire plotting area!\nIn the chart below, click and drag to pan (translate) the view, or scroll to zoom (scale) the view. What can you discover about the precision of the provided rating values?\n\nalt.Chart(movies).mark_circle().add_selection(\n    alt.selection_interval(bind='scales')\n).encode(\n    x='Rotten_Tomatoes_Rating:Q',\n    y=alt.Y('IMDB_Rating:Q', axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement\n    tooltip=['Title:N', 'Release_Date:N', 'IMDB_Rating:Q', 'Rotten_Tomatoes_Rating:Q']\n).properties(\n    width=600,\n    height=400\n)\n\n\n\n\n\n\nZooming in, we can see that the rating values have limited precision! The Rotten Tomatoes ratings are integers, while the IMDB ratings are truncated to tenths. As a result, there is overplotting even when we zoom, with multiple movies sharing the same rating values.\nReading the code above, you may notice the code alt.Axis(minExtent=30) in the y encoding channel. The minExtent parameter ensures a minimum amount of space is reserved for axis ticks and labels. Why do this? When we pan and zoom, the axis labels may change and cause the axis title position to shift. By setting a minimum extent we can reduce distracting movements in the plot. Try changing the minExtent value, for example setting it to zero, and then zoom out to see what happens when longer axis labels enter the view.\nAltair also includes a shorthand for adding panning and zooming to a plot. Instead of directly creating a selection, you can call .interactive() to have Altair automatically generate an interval selection bound to the chart’s scales:\n\nalt.Chart(movies).mark_circle().encode(\n    x='Rotten_Tomatoes_Rating:Q',\n    y=alt.Y('IMDB_Rating:Q', axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement\n    tooltip=['Title:N', 'Release_Date:N', 'IMDB_Rating:Q', 'Rotten_Tomatoes_Rating:Q']\n).properties(\n    width=600,\n    height=400\n).interactive()\n\n\n\n\n\n\nBy default, scale bindings for selections include both the x and y encoding channels. What if we want to limit panning and zooming along a single dimension? We can invoke encodings=['x'] to constrain the selection to the x channel only:\n\nalt.Chart(movies).mark_circle().add_selection(\n    alt.selection_interval(bind='scales', encodings=['x'])\n).encode(\n    x='Rotten_Tomatoes_Rating:Q',\n    y=alt.Y('IMDB_Rating:Q', axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement\n    tooltip=['Title:N', 'Release_Date:N', 'IMDB_Rating:Q', 'Rotten_Tomatoes_Rating:Q']\n).properties(\n    width=600,\n    height=400\n)\n\n\n\n\n\n\nWhen zooming along a single axis only, the shape of the visualized data can change, potentially affecting our perception of relationships in the data. Choosing an appropriate aspect ratio is an important visualization design concern!"
  },
  {
    "objectID": "learn/altair/altair_interaction.html#navigation-overview-detail",
    "href": "learn/altair/altair_interaction.html#navigation-overview-detail",
    "title": "Interaction",
    "section": "Navigation: Overview + Detail",
    "text": "Navigation: Overview + Detail\nWhen panning and zooming, we directly adjust the “viewport” of a chart. The related navigation strategy of overview + detail instead uses an overview display to show all of the data, while supporting selections that pan and zoom a separate focus display.\nBelow we have two area charts showing a decade of price fluctuations for the S&P 500 stock index. Initially both charts show the same data range. Click and drag in the bottom overview chart to update the focus display and examine specific time spans.\n\nbrush = alt.selection_interval(encodings=['x']);\n\nbase = alt.Chart().mark_area().encode(\n    alt.X('date:T', title=None),\n    alt.Y('price:Q')\n).properties(\n    width=700\n)\n  \nalt.vconcat(\n    base.encode(alt.X('date:T', title=None, scale=alt.Scale(domain=brush))),\n    base.add_selection(brush).properties(height=60),\n    data=sp500\n)\n\n\n\n\n\n\nUnlike our earlier panning & zooming case, here we don’t want to bind a selection directly to the scales of a single interactive chart. Instead, we want to bind the selection to a scale domain in another chart. To do so, we update the x encoding channel for our focus chart, setting the scale domain property to reference our brush selection. If no interval is defined (the selection is empty), Altair ignores the brush and uses the underlying data to determine the domain. When a brush interval is created, Altair instead uses that as the scale domain for the focus chart."
  },
  {
    "objectID": "learn/altair/altair_interaction.html#details-on-demand",
    "href": "learn/altair/altair_interaction.html#details-on-demand",
    "title": "Interaction",
    "section": "Details on Demand",
    "text": "Details on Demand\nOnce we spot points of interest within a visualization, we often want to know more about them. Details-on-demand refers to interactively querying for more information about selected values. Tooltips are one useful means of providing details on demand. However, tooltips typically only show information for one data point at a time. How might we show more?\nThe movie ratings scatterplot includes a number of potentially interesting outliers where the Rotten Tomatoes and IMDB ratings disagree. Let’s create a plot that allows us to interactively select points and show their labels. To trigger the filter query on either the hover or click interaction, we will use the Altair composition operator | (“or”).\nMouse over points in the scatter plot below to see a highlight and title label. Shift-click points to make annotations persistent and view multiple labels at once. Which movies are loved by Rotten Tomatoes critics, but not the general audience on IMDB (or vice versa)? See if you can find possible errors, where two different movies with the same name were accidentally combined!\n\nhover = alt.selection_single(\n    on='mouseover',  # select on mouseover\n    nearest=True,    # select nearest point to mouse cursor\n    empty='none'     # empty selection should match nothing\n)\n\nclick = alt.selection_multi(\n    empty='none' # empty selection matches no points\n)\n\n# scatter plot encodings shared by all marks\nplot = alt.Chart().mark_circle().encode(\n    x='Rotten_Tomatoes_Rating:Q',\n    y='IMDB_Rating:Q'\n)\n  \n# shared base for new layers\nbase = plot.transform_filter(\n    hover | click # filter to points in either selection\n)\n\n# layer scatter plot points, halo annotations, and title labels\nalt.layer(\n    plot.add_selection(hover).add_selection(click),\n    base.mark_point(size=100, stroke='firebrick', strokeWidth=1),\n    base.mark_text(dx=4, dy=-8, align='right', stroke='white', strokeWidth=2).encode(text='Title:N'),\n    base.mark_text(dx=4, dy=-8, align='right').encode(text='Title:N'),\n    data=movies\n).properties(\n    width=600,\n    height=450\n)\n\n\n\n\n\n\nThe example above adds three new layers to the scatter plot: a circular annotation, white text to provide a legible background, and black text showing a film title. In addition, this example uses two selections in tandem:\n\nA single selection (hover) that includes nearest=True to automatically select the nearest data point as the mouse moves.\nA multi selection (click) to create persistent selections via shift-click.\n\nBoth selections include the set empty='none' to indicate that no points should be included if a selection is empty. These selections are then combined into a single filter predicate — the logical or of hover and click — to include points that reside in either selection. We use this predicate to filter the new layers to show annotations and labels for selected points only.\nUsing selections and layers, we can realize a number of different designs for details on demand! For example, here is a log-scaled time series of technology stock prices, annotated with a guideline and labels for the date nearest the mouse cursor:\n\n# select a point for which to provide details-on-demand\nlabel = alt.selection_single(\n    encodings=['x'], # limit selection to x-axis value\n    on='mouseover',  # select on mouseover events\n    nearest=True,    # select data point nearest the cursor\n    empty='none'     # empty selection includes no data points\n)\n\n# define our base line chart of stock prices\nbase = alt.Chart().mark_line().encode(\n    alt.X('date:T'),\n    alt.Y('price:Q', scale=alt.Scale(type='log')),\n    alt.Color('symbol:N')\n)\n\nalt.layer(\n    base, # base line chart\n    \n    # add a rule mark to serve as a guide line\n    alt.Chart().mark_rule(color='#aaa').encode(\n        x='date:T'\n    ).transform_filter(label),\n    \n    # add circle marks for selected time points, hide unselected points\n    base.mark_circle().encode(\n        opacity=alt.condition(label, alt.value(1), alt.value(0))\n    ).add_selection(label),\n\n    # add white stroked text to provide a legible background for labels\n    base.mark_text(align='left', dx=5, dy=-5, stroke='white', strokeWidth=2).encode(\n        text='price:Q'\n    ).transform_filter(label),\n\n    # add text labels for stock prices\n    base.mark_text(align='left', dx=5, dy=-5).encode(\n        text='price:Q'\n    ).transform_filter(label),\n    \n    data=stocks\n).properties(\n    width=700,\n    height=400\n)\n\n\n\n\n\n\nPutting into action what we’ve learned so far: can you modify the movie scatter plot above (the one with the dynamic query over years) to include a rule mark that shows the average IMDB (or Rotten Tomatoes) rating for the data contained within the year interval selection?"
  },
  {
    "objectID": "learn/altair/altair_interaction.html#brushing-linking-revisited",
    "href": "learn/altair/altair_interaction.html#brushing-linking-revisited",
    "title": "Interaction",
    "section": "Brushing & Linking, Revisited",
    "text": "Brushing & Linking, Revisited\nEarlier in this notebook we saw an example of brushing & linking: using a dynamic query histogram to highlight points in a movie rating scatter plot. Here, we’ll visit some additional examples involving linked selections.\nReturning to the cars dataset, we can use the repeat operator to build a scatter plot matrix (SPLOM) that shows associations between mileage, acceleration, and horsepower. We can define an interval selection and include it within our repeated scatter plot specification to enable linked selections among all the plots.\nClick and drag in any of the plots below to perform brushing & linking!\n\nbrush = alt.selection_interval(\n    resolve='global' # resolve all selections to a single global instance\n)\n\nalt.Chart(cars).mark_circle().add_selection(\n    brush\n).encode(\n    alt.X(alt.repeat('column'), type='quantitative'),\n    alt.Y(alt.repeat('row'), type='quantitative'),\n    color=alt.condition(brush, 'Cylinders:O', alt.value('grey')),\n    opacity=alt.condition(brush, alt.value(0.8), alt.value(0.1))\n).properties(\n    width=140,\n    height=140\n).repeat(\n    column=['Acceleration', 'Horsepower', 'Miles_per_Gallon'],\n    row=['Miles_per_Gallon', 'Horsepower', 'Acceleration']\n)\n\n\n\n\n\n\nNote above the use of resolve='global' on the interval selection. The default setting of 'global' indicates that across all plots only one brush can be active at a time. However, in some cases we might want to define brushes in multiple plots and combine the results. If we use resolve='union', the selection will be the union of all brushes: if a point resides within any brush it will be selected. Alternatively, if we use resolve='intersect', the selection will consist of the intersection of all brushes: only points that reside within all brushes will be selected.\nTry setting the resolve parameter to 'union' and 'intersect' and see how it changes the resulting selection logic.\n\nCross-Filtering\nThe brushing & linking examples we’ve looked at all use conditional encodings, for example to change opacity values in response to a selection. Another option is to use a selection defined in one view to filter the content of another view.\nLet’s build a collection of histograms for the flights dataset: arrival delay (how early or late a flight arrives, in minutes), distance flown (in miles), and time of departure (hour of the day). We’ll use the repeat operator to create the histograms, and add an interval selection for the x axis with brushes resolved via intersection.\nIn particular, each histogram will consist of two layers: a gray background layer and a blue foreground layer, with the foreground layer filtered by our intersection of brush selections. The result is a cross-filtering interaction across the three charts!\nDrag out brush intervals in the charts below. As you select flights with longer or shorter arrival delays, how do the distance and time distributions respond?\n\nbrush = alt.selection_interval(\n    encodings=['x'],\n    resolve='intersect'\n);\n\nhist = alt.Chart().mark_bar().encode(\n    alt.X(alt.repeat('row'), type='quantitative',\n        bin=alt.Bin(maxbins=100, minstep=1), # up to 100 bins\n        axis=alt.Axis(format='d', titleAnchor='start') # integer format, left-aligned title\n    ),\n    alt.Y('count():Q', title=None) # no y-axis title\n)\n  \nalt.layer(\n    hist.add_selection(brush).encode(color=alt.value('lightgrey')),\n    hist.transform_filter(brush)\n).properties(\n    width=900,\n    height=100\n).repeat(\n    row=['delay', 'distance', 'time'],\n    data=flights\n).transform_calculate(\n    delay='datum.delay &lt; 180 ? datum.delay : 180', # clamp delays &gt; 3 hours\n    time='hours(datum.date) + minutes(datum.date) / 60' # fractional hours\n).configure_view(\n    stroke='transparent' # no outline\n)\n\n\n\n\n\n\nBy cross-filtering you can observe that delayed flights are more likely to depart at later hours. This phenomenon is familiar to frequent fliers: a delay can propagate through the day, affecting subsequent travel by that plane. For the best odds of an on-time arrival, book an early flight!\nThe combination of multiple views and interactive selections can enable valuable forms of multi-dimensional reasoning, turning even basic histograms into powerful input devices for asking questions of a dataset!"
  },
  {
    "objectID": "learn/altair/altair_interaction.html#summary",
    "href": "learn/altair/altair_interaction.html#summary",
    "title": "Interaction",
    "section": "Summary",
    "text": "Summary\nFor more information about the supported interaction options in Altair, please consult the Altair interactive selection documentation. For details about customizing event handlers, for example to compose multiple interaction techniques or support touch-based input on mobile devices, see the Vega-Lite selection documentation.\nInterested in learning more? - The selection abstraction was introduced in the paper Vega-Lite: A Grammar of Interactive Graphics, by Satyanarayan, Moritz, Wongsuphasawat, & Heer. - The PRIM-9 system (for projection, rotation, isolation, and masking in up to 9 dimensions) is one of the earliest interactive visualization tools, built in the early 1970s by Fisherkeller, Tukey, & Friedman. A retro demo video survives! - The concept of brushing & linking was crystallized by Becker, Cleveland, & Wilks in their 1987 article Dynamic Graphics for Data Analysis. - For a comprehensive summary of interaction techniques for visualization, see Interactive Dynamics for Visual Analysis by Heer & Shneiderman. - Finally, for a treatise on what makes interaction effective, read the classic Direct Manipulation Interfaces paper by Hutchins, Hollan, & Norman.\n\nAppendix: On The Representation of Time\nEarlier we observed a small bump in the number of movies in either 1969 and 1970. Where does that bump come from? And why 1969 or 1970? The answer stems from a combination of missing data and how your computer represents time.\nInternally, dates and times are represented relative to the UNIX epoch, in which time “zero” corresponds to the stroke of midnight on January 1, 1970 in UTC time, which runs along the prime meridian. It turns out there are a few movies with missing (null) release dates. Those null values get interpreted as time 0, and thus map to January 1, 1970 in UTC time. If you live in the Americas – and thus in “earlier” time zones – this precise point in time corresponds to an earlier hour on December 31, 1969 in your local time zone. On the other hand, if you live near or east of the prime meridian, the date in your local time zone will be January 1, 1970.\nThe takeaway? Always be skeptical of your data, and be mindful that how data is represented (whether as date times, or floating point numbers, or latitudes and longitudes, etc.) can sometimes lead to artifacts that impact analysis!"
  },
  {
    "objectID": "learn/altair/altair_debugging.html",
    "href": "learn/altair/altair_debugging.html",
    "title": "Altair Debugging Guide",
    "section": "",
    "text": "In this notebook we show you common debugging techniques that you can use if you run into issues with Altair.\nYou can jump to the following sections:\nIn addition to this notebook, you might find the Frequently Asked Questions and Display Troubleshooting guides helpful.\nThis notebook is part of the data visualization curriculum."
  },
  {
    "objectID": "learn/altair/altair_debugging.html#installation",
    "href": "learn/altair/altair_debugging.html#installation",
    "title": "Altair Debugging Guide",
    "section": "Installation",
    "text": "Installation\nThese instructions follow the Altair documentation but focus on some specifics for this series of notebooks.\nIn every notebook, we will import the Altair and Vega Datasets packages. If you are running this notebook on Colab, Altair and Vega Datasets should be preinstalled and ready to go. The notebooks in this series are designed for Colab but should also work in Jupyter Lab or the Jupyter Notebook (the notebook requires a bit more setup described below) but additional packages are required.\nIf you are running in Jupyter Lab or Jupyter Notebooks, you have to install the necessary packages by running the following command in your terminal.\npip install altair vega_datasets\nOr if you use Conda\nconda install -c conda-forge altair vega_datasets\nYou can run command line commands from a code cell by prefixing it with !. For example, to install Altair and Vega Datasets with Pip, you can run the following cell.\n\n!pip install altair vega_datasets\n\nRequirement already satisfied: altair in /anaconda3/lib/python3.7/site-packages (4.0.1)\nRequirement already satisfied: vega_datasets in /anaconda3/lib/python3.7/site-packages (0.7.0)\nRequirement already satisfied: numpy in /anaconda3/lib/python3.7/site-packages (from altair) (1.15.4)\nRequirement already satisfied: jsonschema in /anaconda3/lib/python3.7/site-packages (from altair) (2.6.0)\nRequirement already satisfied: pandas in /anaconda3/lib/python3.7/site-packages (from altair) (0.23.4)\nRequirement already satisfied: toolz in /anaconda3/lib/python3.7/site-packages (from altair) (0.9.0)\nRequirement already satisfied: jinja2 in /anaconda3/lib/python3.7/site-packages (from altair) (2.10)\nRequirement already satisfied: entrypoints in /anaconda3/lib/python3.7/site-packages (from altair) (0.2.3)\nRequirement already satisfied: python-dateutil&gt;=2.5.0 in /anaconda3/lib/python3.7/site-packages (from pandas-&gt;altair) (2.7.5)\nRequirement already satisfied: pytz&gt;=2011k in /anaconda3/lib/python3.7/site-packages (from pandas-&gt;altair) (2018.7)\nRequirement already satisfied: MarkupSafe&gt;=0.23 in /anaconda3/lib/python3.7/site-packages (from jinja2-&gt;altair) (1.1.0)\nRequirement already satisfied: six&gt;=1.5 in /anaconda3/lib/python3.7/site-packages (from python-dateutil&gt;=2.5.0-&gt;pandas-&gt;altair) (1.12.0)\n\n\n\nimport altair as alt\nfrom vega_datasets import data\n\n\nMake sure you are Using the Latest Version of Altair\nIf you are running into issues with Altair, first make sure that you are running the latest version. To check the version of Altair that you have installed, run the cell below.\n\nalt.__version__\n\n'4.1.0'\n\n\nTo check what the latest version of altair is, go to this page or run the cell below (requires Python 3).\n\nimport urllib.request, json \nwith urllib.request.urlopen(\"https://pypi.org/pypi/altair/json\") as url:\n    print(json.loads(url.read().decode())['info']['version'])\n\n4.1.0\n\n\nIf you are not running the latest version, you can update it with pip. You can update Altair and Vega Datasets by running this command in your terminal.\npip install -U altair vega_datasets\n\n\nTry Making a Chart\nNow you can create an Altair chart.\n\ncars = data.cars()\n\nalt.Chart(cars).mark_point().encode(\n    x='Horsepower',\n    y='Displacement',\n    color='Origin'\n)\n\n\n\n\n\n\n\n\nSpecial Setup for the Jupyter Notebook\nIf you are running in Jupyter Lab, Jupyter Notebook, or Colab (and have a working Internet connection) you should be seeing a chart. If you are running in another environment (or offline), you will need to tell Altair to use a different renderer;\nTo activate a different renderer in a notebook cell:\n# to run in nteract, VSCode, or offline in JupyterLab\nalt.renderers.enable('mimebundle')\nTo run offline in Jupyter Notebook you must install an additional dependency, the vega package. Run this command in your terminal:\npip install vega\nThen activate the notebook renderer:\n# to run offline in Jupyter Notebook\nalt.renderers.enable('notebook')\nThese instruction follow the instructions on the Altair website."
  },
  {
    "objectID": "learn/altair/altair_debugging.html#display-troubleshooting",
    "href": "learn/altair/altair_debugging.html#display-troubleshooting",
    "title": "Altair Debugging Guide",
    "section": "Display Troubleshooting",
    "text": "Display Troubleshooting\nIf you are having issues with seeing a chart, make sure your setup is correct by following the debugging instruction above. If you are still having issues, follow the instruction about debugging display issues in the Altair documentation.\n\nNon Existent Fields\nA common error is accidentally using a field that does not exist.\n\nimport pandas as pd\n\ndf = pd.DataFrame({'x': [1, 2, 3],\n                     'y': [3, 1, 4]})\n\nalt.Chart(df).mark_point().encode(\n    x='x:Q',\n    y='y:Q',\n    color='color:Q'  # &lt;-- this field does not exist in the data!\n)\n\n\n\n\n\n\nCheck the spelling of your files and print the data source to confirm that the data and fields exist. For instance, here you see that color is not a vaid field.\n\ndf.head()\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n0\n1\n3\n\n\n1\n2\n1\n\n\n2\n3\n4"
  },
  {
    "objectID": "learn/altair/altair_debugging.html#invalid-specifications",
    "href": "learn/altair/altair_debugging.html#invalid-specifications",
    "title": "Altair Debugging Guide",
    "section": "Invalid Specifications",
    "text": "Invalid Specifications\nAnother common issue is creating an invalid specification and getting an error.\n\nInvalid Properties\nAltair might show an SchemaValidationError or ValueError. Read the error message carefully. Usually it will tell you what is going wrong.\nFor example, if you forget the mark type, you will see this SchemaValidationError.\n\nalt.Chart(cars).encode(\n    y='Horsepower'\n)\n\nSchemaValidationError: Invalid specification\n\n        altair.vegalite.v4.api.Chart, validating 'required'\n\n        'mark' is a required property\n        \n\n\nalt.Chart(...)\n\n\nOr if you use a non-existent channel, you get a ValueError.\n\nalt.Chart(cars)).mark_point().encode(\n    z='Horsepower'\n)\n\nSyntaxError: invalid syntax (&lt;ipython-input-9-fc84db126677&gt;, line 1)"
  },
  {
    "objectID": "learn/altair/altair_debugging.html#properties-are-being-ignored",
    "href": "learn/altair/altair_debugging.html#properties-are-being-ignored",
    "title": "Altair Debugging Guide",
    "section": "Properties are Being Ignored",
    "text": "Properties are Being Ignored\nAltair might ignore a property that you specified. In the chart below, we are using a text channel, which is only compatible with mark_text. You do not see an error or a warning about this in the notebook. However, the underlying Vega-Lite library will show a warning in the browser console. Press Alt+Cmd+I on Mac or Alt+Ctrl+I on Windows and Linux to open the developer tools and click on the Console tab. When you run the example in the cell below, you will see a the following warning.\nWARN text dropped as it is incompatible with \"bar\".\n\nalt.Chart(cars).mark_bar().encode(\n    y='mean(Horsepower)',\n    text='mean(Acceleration)'\n)\n\n\n\n\n\n\nIf you find yourself debugging issues related to Vega-Lite, you can open the chart in the Vega Editor either by clicking on the “Open in Vega Editor” link at the bottom of the chart or in the action menu (click to open) at the top right of a chart. The Vega Editor provides additional debugging but you will be writing Vega-Lite JSON instead of Altair in Python.\nNote: The Vega Editor may be using a newer version of Vega-Lite and so the behavior may vary."
  },
  {
    "objectID": "learn/altair/altair_debugging.html#asking-for-help",
    "href": "learn/altair/altair_debugging.html#asking-for-help",
    "title": "Altair Debugging Guide",
    "section": "Asking for Help",
    "text": "Asking for Help\nIf you find a problem with Altair and get stuck, you can ask a question on Stack Overflow. Ask your question with the altair and vega-lite tags. You can find a list of questions people have asked before here."
  },
  {
    "objectID": "learn/altair/altair_debugging.html#reporting-issues",
    "href": "learn/altair/altair_debugging.html#reporting-issues",
    "title": "Altair Debugging Guide",
    "section": "Reporting Issues",
    "text": "Reporting Issues\nIf you find a problem with Altair and believe it is a bug, please create an issue in the Altair GitHub repo with a description of your problem. If you believe the issue is related to the underlying Vega-Lite library, please create an issue in the Vega-Lite GitHub repo."
  },
  {
    "objectID": "learn/altair/altair_introduction.html",
    "href": "learn/altair/altair_introduction.html",
    "title": "Introduction to Altair",
    "section": "",
    "text": "Altair is a declarative statistical visualization library for Python. Altair offers a powerful and concise visualization grammar for quickly building a wide range of statistical graphics.\nBy declarative, we mean that you can provide a high-level specification of what you want the visualization to include, in terms of data, graphical marks, and encoding channels, rather than having to specify how to implement the visualization in terms of for-loops, low-level drawing commands, etc. The key idea is that you declare links between data fields and visual encoding channels, such as the x-axis, y-axis, color, etc. The rest of the plot details are handled automatically. Building on this declarative plotting idea, a surprising range of simple to sophisticated visualizations can be created using a concise grammar.\nAltair is based on Vega-Lite, a high-level grammar of interactive graphics. Altair provides a friendly Python API (Application Programming Interface) that generates Vega-Lite specifications in JSON (JavaScript Object Notation) format. Environments such as Jupyter Notebooks, JupyterLab, and Colab can then take this specification and render it directly in the web browser. To learn more about the motivation and basic concepts behind Altair and Vega-Lite, watch the Vega-Lite presentation video from OpenVisConf 2017.\nThis notebook will guide you through the basic process of creating visualizations in Altair. First, you will need to make sure you have the Altair package and its dependencies installed (for more, see the Altair installation documentation), or you are using a notebook environment that includes the dependencies pre-installed.\nThis notebook is part of the data visualization curriculum."
  },
  {
    "objectID": "learn/altair/altair_introduction.html#imports",
    "href": "learn/altair/altair_introduction.html#imports",
    "title": "Introduction to Altair",
    "section": "Imports",
    "text": "Imports\nTo start, we must import the necessary libraries: Pandas for data frames and Altair for visualization.\n\nimport pandas as pd\nimport altair as alt"
  },
  {
    "objectID": "learn/altair/altair_introduction.html#renderers",
    "href": "learn/altair/altair_introduction.html#renderers",
    "title": "Introduction to Altair",
    "section": "Renderers",
    "text": "Renderers\nDepending on your environment, you may need to specify a renderer for Altair. If you are using JupyterLab, Jupyter Notebook, or Google Colab with a live Internet connection you should not need to do anything. Otherwise, please read the documentation for Displaying Altair Charts."
  },
  {
    "objectID": "learn/altair/altair_introduction.html#data",
    "href": "learn/altair/altair_introduction.html#data",
    "title": "Introduction to Altair",
    "section": "Data",
    "text": "Data\nData in Altair is built around the Pandas data frame, which consists of a set of named data columns. We will also regularly refer to data columns as data fields.\nWhen using Altair, datasets are commonly provided as data frames. Alternatively, Altair can also accept a URL to load a network-accessible dataset. As we will see, the named columns of the data frame are an essential piece of plotting with Altair.\nWe will often use datasets from the vega-datasets repository. Some of these datasets are directly available as Pandas data frames:\n\nfrom vega_datasets import data  # import vega_datasets\ncars = data.cars()              # load cars data as a Pandas data frame\ncars.head()                     # display the first five rows\n\n\n\n\n\n\n\n\nName\nMiles_per_Gallon\nCylinders\nDisplacement\nHorsepower\nWeight_in_lbs\nAcceleration\nYear\nOrigin\n\n\n\n\n0\nchevrolet chevelle malibu\n18.0\n8\n307.0\n130.0\n3504\n12.0\n1970-01-01\nUSA\n\n\n1\nbuick skylark 320\n15.0\n8\n350.0\n165.0\n3693\n11.5\n1970-01-01\nUSA\n\n\n2\nplymouth satellite\n18.0\n8\n318.0\n150.0\n3436\n11.0\n1970-01-01\nUSA\n\n\n3\namc rebel sst\n16.0\n8\n304.0\n150.0\n3433\n12.0\n1970-01-01\nUSA\n\n\n4\nford torino\n17.0\n8\n302.0\n140.0\n3449\n10.5\n1970-01-01\nUSA\n\n\n\n\n\n\n\nDatasets in the vega-datasets collection can also be accessed via URLs:\n\ndata.cars.url\n\n'https://cdn.jsdelivr.net/npm/vega-datasets@v1.29.0/data/cars.json'\n\n\nDataset URLs can be passed directly to Altair (for supported formats like JSON and CSV), or loaded into a Pandas data frame like so:\n\npd.read_json(data.cars.url).head() # load JSON data into a data frame\n\n\n\n\n\n\n\n\nName\nMiles_per_Gallon\nCylinders\nDisplacement\nHorsepower\nWeight_in_lbs\nAcceleration\nYear\nOrigin\n\n\n\n\n0\nchevrolet chevelle malibu\n18.0\n8\n307.0\n130.0\n3504\n12.0\n1970-01-01\nUSA\n\n\n1\nbuick skylark 320\n15.0\n8\n350.0\n165.0\n3693\n11.5\n1970-01-01\nUSA\n\n\n2\nplymouth satellite\n18.0\n8\n318.0\n150.0\n3436\n11.0\n1970-01-01\nUSA\n\n\n3\namc rebel sst\n16.0\n8\n304.0\n150.0\n3433\n12.0\n1970-01-01\nUSA\n\n\n4\nford torino\n17.0\n8\n302.0\n140.0\n3449\n10.5\n1970-01-01\nUSA\n\n\n\n\n\n\n\nFor more information about data frames - and some useful transformations to prepare Pandas data frames for plotting with Altair! - see the Specifying Data with Altair documentation.\n\nWeather Data\nStatistical visualization in Altair begins with “tidy” data frames. Here, we’ll start by creating a simple data frame (df) containing the average precipitation (precip) for a given city and month :\n\ndf = pd.DataFrame({\n    'city': ['Seattle', 'Seattle', 'Seattle', 'New York', 'New York', 'New York', 'Chicago', 'Chicago', 'Chicago'],\n    'month': ['Apr', 'Aug', 'Dec', 'Apr', 'Aug', 'Dec', 'Apr', 'Aug', 'Dec'],\n    'precip': [2.68, 0.87, 5.31, 3.94, 4.13, 3.58, 3.62, 3.98, 2.56]\n})\n\ndf\n\n\n\n\n\n\n\n\ncity\nmonth\nprecip\n\n\n\n\n0\nSeattle\nApr\n2.68\n\n\n1\nSeattle\nAug\n0.87\n\n\n2\nSeattle\nDec\n5.31\n\n\n3\nNew York\nApr\n3.94\n\n\n4\nNew York\nAug\n4.13\n\n\n5\nNew York\nDec\n3.58\n\n\n6\nChicago\nApr\n3.62\n\n\n7\nChicago\nAug\n3.98\n\n\n8\nChicago\nDec\n2.56"
  },
  {
    "objectID": "learn/altair/altair_introduction.html#the-chart-object",
    "href": "learn/altair/altair_introduction.html#the-chart-object",
    "title": "Introduction to Altair",
    "section": "The Chart Object",
    "text": "The Chart Object\nThe fundamental object in Altair is the Chart, which takes a data frame as a single argument:\n\nchart = alt.Chart(df)\n\nSo far, we have defined the Chart object and passed it the simple data frame we generated above. We have not yet told the chart to do anything with the data."
  },
  {
    "objectID": "learn/altair/altair_introduction.html#marks-and-encodings",
    "href": "learn/altair/altair_introduction.html#marks-and-encodings",
    "title": "Introduction to Altair",
    "section": "Marks and Encodings",
    "text": "Marks and Encodings\nWith a chart object in hand, we can now specify how we would like the data to be visualized. We first indicate what kind of graphical mark (geometric shape) we want to use to represent the data. We can set the mark attribute of the chart object using the the Chart.mark_* methods.\nFor example, we can show the data as a point using Chart.mark_point():\n\nalt.Chart(df).mark_point()\n\n/Users/dkapitan/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\nHere the rendering consists of one point per row in the dataset, all plotted on top of each other, since we have not yet specified positions for these points.\nTo visually separate the points, we can map various encoding channels, or channels for short, to fields in the dataset. For example, we could encode the field city of the data using the y channel, which represents the y-axis position of the points. To specify this, use the encode method:\n\nalt.Chart(df).mark_point().encode(\n  y='city',\n)\n\n\n\n\n\n\nThe encode() method builds a key-value mapping between encoding channels (such as x, y, color, shape, size, etc.) to fields in the dataset, accessed by field name. For Pandas data frames, Altair automatically determines an appropriate data type for the mapped column, which in this case is the nominal type, indicating unordered, categorical values.\nThough we’ve now separated the data by one attribute, we still have multiple points overlapping within each category. Let’s further separate these by adding an x encoding channel, mapped to the 'precip' field:\n\nalt.Chart(df).mark_point().encode(\n    x='precip',\n    y='city'\n)\n\n\n\n\n\n\nSeattle exhibits both the least-rainiest and most-rainiest months!\nThe data type of the 'precip' field is again automatically inferred by Altair, and this time is treated as a quantitative type (that is, a real-valued number). We see that grid lines and appropriate axis titles are automatically added as well.\nAbove we have specified key-value pairs using keyword arguments (x='precip'). In addition, Altair provides construction methods for encoding definitions, using the syntax alt.X('precip'). This alternative is useful for providing more parameters to an encoding, as we will see later in this notebook.\n\nalt.Chart(df).mark_point().encode(\n    alt.X('precip'),\n    alt.Y('city')\n)\n\n\n\n\n\n\nThe two styles of specifying encodings can be interleaved: x='precip', alt.Y('city') is also a valid input to the encode function.\nIn the examples above, the data type for each field is inferred automatically based on its type within the Pandas data frame. We can also explicitly indicate the data type to Altair by annotating the field name:\n\n'b:N' indicates a nominal type (unordered, categorical data),\n'b:O' indicates an ordinal type (rank-ordered data),\n'b:Q' indicates a quantitative type (numerical data with meaningful magnitudes), and\n'b:T' indicates a temporal type (date/time data)\n\nFor example, alt.X('precip:N').\nExplicit annotation of data types is necessary when data is loaded from an external URL directly by Vega-Lite (skipping Pandas entirely), or when we wish to use a type that differs from the type that was automatically inferred.\nWhat do you think will happen to our chart above if we treat precip as a nominal or ordinal variable, rather than a quantitative variable? Modify the code above and find out!\nWe will take a closer look at data types and encoding channels in the next notebook of the data visualization curriculum."
  },
  {
    "objectID": "learn/altair/altair_introduction.html#data-transformation-aggregation",
    "href": "learn/altair/altair_introduction.html#data-transformation-aggregation",
    "title": "Introduction to Altair",
    "section": "Data Transformation: Aggregation",
    "text": "Data Transformation: Aggregation\nTo allow for more flexibility in how data are visualized, Altair has a built-in syntax for aggregation of data. For example, we can compute the average of all values by specifying an aggregation function along with the field name:\n\nalt.Chart(df).mark_point().encode(\n    x='average(precip)',\n    y='city'\n)\n\n\n\n\n\n\nNow within each x-axis category, we see a single point reflecting the average of the values within that category.\nDoes Seattle really have the lowest average precipitation of these cities? (It does!) Still, how might this plot mislead? Which months are included? What counts as precipitation?\nAltair supports a variety of aggregation functions, including count, min (minimum), max (maximum), average, median, and stdev (standard deviation). In a later notebook, we will take a tour of data transformations, including aggregation, sorting, filtering, and creation of new derived fields using calculation formulas."
  },
  {
    "objectID": "learn/altair/altair_introduction.html#changing-the-mark-type",
    "href": "learn/altair/altair_introduction.html#changing-the-mark-type",
    "title": "Introduction to Altair",
    "section": "Changing the Mark Type",
    "text": "Changing the Mark Type\nLet’s say we want to represent our aggregated values using rectangular bars rather than circular points. We can do this by replacing Chart.mark_point with Chart.mark_bar:\n\nalt.Chart(df).mark_bar().encode(\n    x='average(precip)',\n    y='city'\n)\n\n\n\n\n\n\nBecause the nominal field a is mapped to the y-axis, the result is a horizontal bar chart. To get a vertical bar chart, we can simply swap the x and y keywords:\n\nalt.Chart(df).mark_bar().encode(\n    x='city',\n    y='average(precip)'\n)"
  },
  {
    "objectID": "learn/altair/altair_introduction.html#customizing-a-visualization",
    "href": "learn/altair/altair_introduction.html#customizing-a-visualization",
    "title": "Introduction to Altair",
    "section": "Customizing a Visualization",
    "text": "Customizing a Visualization\nBy default Altair / Vega-Lite make some choices about properties of the visualization, but these can be changed using methods to customize the look of the visualization. For example, we can specify the axis titles using the axis attribute of channel classes, we can modify scale properties using the scale attribute, and we can specify the color of the marking by setting the color keyword of the Chart.mark_* methods to any valid CSS color string:\n\nalt.Chart(df).mark_point(color='firebrick').encode(\n  alt.X('precip', scale=alt.Scale(type='log'), axis=alt.Axis(title='Log-Scaled Values')),\n  alt.Y('city', axis=alt.Axis(title='Category')),\n)\n\n\n\n\n\n\nA subsequent module will explore the various options available for scales, axes, and legends to create customized charts."
  },
  {
    "objectID": "learn/altair/altair_introduction.html#multiple-views",
    "href": "learn/altair/altair_introduction.html#multiple-views",
    "title": "Introduction to Altair",
    "section": "Multiple Views",
    "text": "Multiple Views\nAs we’ve seen above, the Altair Chart object represents a plot with a single mark type. What about more complicated diagrams, involving multiple charts or layers? Using a set of view composition operators, Altair can take multiple chart definitions and combine them to create more complex views.\nAs a starting point, let’s plot the cars dataset in a line chart showing the average mileage by the year of manufacture:\n\nalt.Chart(cars).mark_line().encode(\n    alt.X('Year'),\n    alt.Y('average(Miles_per_Gallon)')\n)\n\n\n\n\n\n\nTo augment this plot, we might like to add circle marks for each averaged data point. (The circle mark is just a convenient shorthand for point marks that used filled circles.)\nWe can start by defining each chart separately: first a line plot, then a scatter plot. We can then use the layer operator to combine the two into a layered chart. Here we use the shorthand + (plus) operator to invoke layering:\n\nline = alt.Chart(cars).mark_line().encode(\n    alt.X('Year'),\n    alt.Y('average(Miles_per_Gallon)')\n)\n\npoint = alt.Chart(cars).mark_circle().encode(\n    alt.X('Year'),\n    alt.Y('average(Miles_per_Gallon)')\n)\n\nline + point\n\n\n\n\n\n\nWe can also create this chart by reusing and modifying a previous chart definition! Rather than completely re-write a chart, we can start with the line chart, then invoke the mark_point method to generate a new chart definition with a different mark type:\n\nmpg = alt.Chart(cars).mark_line().encode(\n    alt.X('Year'),\n    alt.Y('average(Miles_per_Gallon)')\n)\n\nmpg + mpg.mark_circle()\n\n\n\n\n\n\n(The need to place points on lines is so common, the line mark also includes a shorthand to generate a new layer for you. Trying adding the argument point=True to the mark_line method!)\nNow, what if we’d like to see this chart alongside other plots, such as the average horsepower over time?\nWe can use concatenation operators to place multiple charts side-by-side, either vertically or horizontally. Here, we’ll use the | (pipe) operator to perform horizontal concatenation of two charts:\n\nhp = alt.Chart(cars).mark_line().encode(\n    alt.X('Year'),\n    alt.Y('average(Horsepower)')\n)\n\n(mpg + mpg.mark_circle()) | (hp + hp.mark_circle())\n\n\n\n\n\n\nWe can see that, in this dataset, over the 1970s and early ’80s the average fuel efficiency improved while the average horsepower decreased.\nA later notebook will focus on view composition, including not only layering and concatenation, but also the facet operator for splitting data into sub-plots and the repeat operator to concisely generate concatenated charts from a template."
  },
  {
    "objectID": "learn/altair/altair_introduction.html#interactivity",
    "href": "learn/altair/altair_introduction.html#interactivity",
    "title": "Introduction to Altair",
    "section": "Interactivity",
    "text": "Interactivity\nIn addition to basic plotting and view composition, one of Altair and Vega-Lite’s most exciting features is its support for interaction.\nTo create a simple interactive plot that supports panning and zooming, we can invoke the interactive() method of the Chart object. In the chart below, click and drag to pan or use the scroll wheel to zoom:\n\nalt.Chart(cars).mark_point().encode(\n    x='Horsepower',\n    y='Miles_per_Gallon',\n    color='Origin',\n).interactive()\n\n\n\n\n\n\nTo provide more details upon mouse hover, we can use the tooltip encoding channel:\n\nalt.Chart(cars).mark_point().encode(\n    x='Horsepower',\n    y='Miles_per_Gallon',\n    color='Origin',\n    tooltip=['Name', 'Origin'] # show Name and Origin in a tooltip\n).interactive()\n\n\n\n\n\n\nFor more complex interactions, such as linked charts and cross-filtering, Altair provides a selection abstraction for defining interactive selections and then binding them to components of a chart. We will cover this is in detail in a later notebook.\nBelow is a more complex example. The upper histogram shows the count of cars per year and uses an interactive selection to modify the opacity of points in the lower scatter plot, which shows horsepower versus mileage.\nDrag out an interval in the upper chart and see how it affects the points in the lower chart. As you examine the code, don’t worry if parts don’t make sense yet! This is an aspirational example, and we will fill in all the needed details over the course of the different notebooks.\n\n# create an interval selection over an x-axis encoding\nbrush = alt.selection_interval(encodings=['x'])\n\n# determine opacity based on brush\nopacity = alt.condition(brush, alt.value(0.9), alt.value(0.1))\n\n# an overview histogram of cars per year\n# add the interval brush to select cars over time\noverview = alt.Chart(cars).mark_bar().encode(\n    alt.X('Year:O', timeUnit='year', # extract year unit, treat as ordinal\n      axis=alt.Axis(title=None, labelAngle=0) # no title, no label angle\n    ),\n    alt.Y('count()', title=None), # counts, no axis title\n    opacity=opacity\n).add_selection(\n    brush      # add interval brush selection to the chart\n).properties(\n    width=400, # set the chart width to 400 pixels\n    height=50  # set the chart height to 50 pixels\n)\n\n# a detail scatterplot of horsepower vs. mileage\n# modulate point opacity based on the brush selection\ndetail = alt.Chart(cars).mark_point().encode(\n    alt.X('Horsepower'),\n    alt.Y('Miles_per_Gallon'),\n    # set opacity based on brush selection\n    opacity=opacity\n).properties(width=400) # set chart width to match the first chart\n\n# vertically concatenate (vconcat) charts using the '&' operator\noverview & detail"
  },
  {
    "objectID": "learn/altair/altair_introduction.html#aside-examining-the-json-output",
    "href": "learn/altair/altair_introduction.html#aside-examining-the-json-output",
    "title": "Introduction to Altair",
    "section": "Aside: Examining the JSON Output",
    "text": "Aside: Examining the JSON Output\nAs a Python API to Vega-Lite, Altair’s main purpose is to convert plot specifications to a JSON string that conforms to the Vega-Lite schema. Using the Chart.to_json method, we can inspect the JSON specification that Altair is exporting and sending to Vega-Lite:\n\nchart = alt.Chart(df).mark_bar().encode(\n    x='average(precip)',\n    y='city',\n)\nprint(chart.to_json())\n\n{\n  \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\",\n  \"config\": {\n    \"view\": {\n      \"continuousHeight\": 300,\n      \"continuousWidth\": 400\n    }\n  },\n  \"data\": {\n    \"name\": \"data-fdfbb22e8e0e89f6556d8a3b434b0c97\"\n  },\n  \"datasets\": {\n    \"data-fdfbb22e8e0e89f6556d8a3b434b0c97\": [\n      {\n        \"city\": \"Seattle\",\n        \"month\": \"Apr\",\n        \"precip\": 2.68\n      },\n      {\n        \"city\": \"Seattle\",\n        \"month\": \"Aug\",\n        \"precip\": 0.87\n      },\n      {\n        \"city\": \"Seattle\",\n        \"month\": \"Dec\",\n        \"precip\": 5.31\n      },\n      {\n        \"city\": \"New York\",\n        \"month\": \"Apr\",\n        \"precip\": 3.94\n      },\n      {\n        \"city\": \"New York\",\n        \"month\": \"Aug\",\n        \"precip\": 4.13\n      },\n      {\n        \"city\": \"New York\",\n        \"month\": \"Dec\",\n        \"precip\": 3.58\n      },\n      {\n        \"city\": \"Chicago\",\n        \"month\": \"Apr\",\n        \"precip\": 3.62\n      },\n      {\n        \"city\": \"Chicago\",\n        \"month\": \"Aug\",\n        \"precip\": 3.98\n      },\n      {\n        \"city\": \"Chicago\",\n        \"month\": \"Dec\",\n        \"precip\": 2.56\n      }\n    ]\n  },\n  \"encoding\": {\n    \"x\": {\n      \"aggregate\": \"average\",\n      \"field\": \"precip\",\n      \"type\": \"quantitative\"\n    },\n    \"y\": {\n      \"field\": \"city\",\n      \"type\": \"nominal\"\n    }\n  },\n  \"mark\": \"bar\"\n}\n\n\nNotice here that encode(x='average(precip)') has been expanded to a JSON structure with a field name, a type for the data, and includes an aggregate field. The encode(y='city') statement has been expanded similarly.\nAs we saw earlier, Altair’s shorthand syntax includes a way to specify the type of the field as well:\n\nx = alt.X('average(precip):Q')\nprint(x.to_json())\n\n{\n  \"aggregate\": \"average\",\n  \"field\": \"precip\",\n  \"type\": \"quantitative\"\n}\n\n\nThis short-hand is equivalent to spelling-out the attributes by name:\n\nx = alt.X(aggregate='average', field='precip', type='quantitative')\nprint(x.to_json())\n\n{\n  \"aggregate\": \"average\",\n  \"field\": \"precip\",\n  \"type\": \"quantitative\"\n}"
  },
  {
    "objectID": "learn/altair/altair_introduction.html#publishing-a-visualization",
    "href": "learn/altair/altair_introduction.html#publishing-a-visualization",
    "title": "Introduction to Altair",
    "section": "Publishing a Visualization",
    "text": "Publishing a Visualization\nOnce you have visualized your data, perhaps you would like to publish it somewhere on the web. This can be done straightforwardly using the vega-embed JavaScript package. A simple example of a stand-alone HTML document can be generated for any chart using the Chart.save method:\nchart = alt.Chart(df).mark_bar().encode(\n    x='average(precip)',\n    y='city',\n)\nchart.save('chart.html')\nThe basic HTML template produces output that looks like this, where the JSON specification for your plot produced by Chart.to_json should be stored in the spec JavaScript variable:\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;head&gt;\n    &lt;script src=\"https://cdn.jsdelivr.net/npm/vega@5\"&gt;&lt;/script&gt;\n    &lt;script src=\"https://cdn.jsdelivr.net/npm/vega-lite@4\"&gt;&lt;/script&gt;\n    &lt;script src=\"https://cdn.jsdelivr.net/npm/vega-embed@6\"&gt;&lt;/script&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n  &lt;div id=\"vis\"&gt;&lt;/div&gt;\n  &lt;script&gt;\n    (function(vegaEmbed) {\n      var spec = {}; /* JSON output for your chart's specification */\n      var embedOpt = {\"mode\": \"vega-lite\"}; /* Options for the embedding */\n\n      function showError(el, error){\n          el.innerHTML = ('&lt;div style=\"color:red;\"&gt;'\n                          + '&lt;p&gt;JavaScript Error: ' + error.message + '&lt;/p&gt;'\n                          + \"&lt;p&gt;This usually means there's a typo in your chart specification. \"\n                          + \"See the javascript console for the full traceback.&lt;/p&gt;\"\n                          + '&lt;/div&gt;');\n          throw error;\n      }\n      const el = document.getElementById('vis');\n      vegaEmbed(\"#vis\", spec, embedOpt)\n        .catch(error =&gt; showError(el, error));\n    })(vegaEmbed);\n  &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\nThe Chart.save method provides a convenient way to save such HTML output to file. For more information on embedding Altair/Vega-Lite, see the documentation of the vega-embed project."
  },
  {
    "objectID": "learn/altair/altair_introduction.html#next-steps",
    "href": "learn/altair/altair_introduction.html#next-steps",
    "title": "Introduction to Altair",
    "section": "Next Steps",
    "text": "Next Steps\n🎉 Hooray, you’ve completed the introduction to Altair! In the next notebook, we will dive deeper into creating visualizations using Altair’s model of data types, graphical marks, and visual encoding channels."
  },
  {
    "objectID": "learn/altair/index.html",
    "href": "learn/altair/index.html",
    "title": "Altair",
    "section": "",
    "text": "This section contains a series of Python-based Jupyter notebooks:\n\nIntroduction to Altair\nData types, graphical marks and visual encoding channels\nData transformation\nScales, axes & legends\nMulti-views composition\nInteractions\nCarthographic visualization"
  },
  {
    "objectID": "learn/altair/index.html#contents",
    "href": "learn/altair/index.html#contents",
    "title": "Altair",
    "section": "",
    "text": "This section contains a series of Python-based Jupyter notebooks:\n\nIntroduction to Altair\nData types, graphical marks and visual encoding channels\nData transformation\nScales, axes & legends\nMulti-views composition\nInteractions\nCarthographic visualization"
  },
  {
    "objectID": "learn/altair/index.html#getting-started",
    "href": "learn/altair/index.html#getting-started",
    "title": "Altair",
    "section": "Getting Started",
    "text": "Getting Started\nThe visualization curriculum can be used either online or on your local computer. You can view and interact with the plots directly in this Jupyter Book. If you want to modify the code, you have a few different options:\n\nTo read JavaScript notebooks online using Observable, navigate to the “Observable” page above and click the corresponding notebook.\nTo read Python notebooks online using Colab, click the corresponding section in this book, hover over the little rocket ship at the top of the page, and select “Colab” from the menu.\nTo read Python notebooks locally, follow the instructions below.\n\n\nLocal Installation\n\nInstall Altair and a notebook environment. The most recent versions of these notebooks use Altair version 4.\nDownload the notebooks from the releases page. Typically you will want to use the most recent release. (If you wish to use notebooks for Altair version 3, download the Altair v3.2 release.)\nOpen the notebooks in your local notebook environment. For example, if you have JupyterLab installed (v1.0 or higher is required), run jupyter lab within the directory containing the notebooks.\n\nDepending on your programming environment (and whether or not you have a live internet connection), you may want to specify a particular renderer for Altair."
  },
  {
    "objectID": "learn/altair/index.html#credits",
    "href": "learn/altair/index.html#credits",
    "title": "Altair",
    "section": "Credits",
    "text": "Credits\nDeveloped at the University of Washington by Jeffrey Heer, Dominik Moritz, Jake VanderPlas, and Brock Craft. Thanks to the UW Interactive Data Lab and Arvind Satyanarayan for their valuable input and feedback! Thanks also to the students of UW CSE512 Spring 2019, the first group to use these notebooks within an integrated course curriculum."
  },
  {
    "objectID": "learn/altair/altair_cartographic.html",
    "href": "learn/altair/altair_cartographic.html",
    "title": "Cartographic Visualization",
    "section": "",
    "text": "“The making of maps is one of humanity’s longest established intellectual endeavors and also one of its most complex, with scientific theory, graphical representation, geographical facts, and practical considerations blended together in an unending variety of ways.” — H. J. Steward\nCartography – the study and practice of map-making – has a rich history spanning centuries of discovery and design. Cartographic visualization leverages mapping techniques to convey data containing spatial information, such as locations, routes, or trajectories on the surface of the Earth.\nApproximating the Earth as a sphere, we can denote positions using a spherical coordinate system of latitude (angle in degrees north or south of the equator) and longitude (angle in degrees specifying east-west position). In this system, a parallel is a circle of constant latitude and a meridian is a circle of constant longitude. The prime meridian lies at 0° longitude and by convention is defined to pass through the Royal Observatory in Greenwich, England.\nTo “flatten” a three-dimensional sphere on to a two-dimensional plane, we must apply a projection that maps (longitude, latitude) pairs to (x, y) coordinates. Similar to scales, projections map from a data domain (spatial position) to a visual range (pixel position). However, the scale mappings we’ve seen thus far accept a one-dimensional domain, whereas map projections are inherently two-dimensional.\nIn this notebook, we will introduce the basics of creating maps and visualizing spatial data with Altair, including:\nThis notebook is part of the data visualization curriculum.\nimport pandas as pd\nimport altair as alt\nfrom vega_datasets import data"
  },
  {
    "objectID": "learn/altair/altair_cartographic.html#geographic-data-geojson-and-topojson",
    "href": "learn/altair/altair_cartographic.html#geographic-data-geojson-and-topojson",
    "title": "Cartographic Visualization",
    "section": "Geographic Data: GeoJSON and TopoJSON",
    "text": "Geographic Data: GeoJSON and TopoJSON\nUp to this point, we have worked with JSON and CSV formatted datasets that correspond to data tables made up of rows (records) and columns (fields). In order to represent geographic regions (countries, states, etc.) and trajectories (flight paths, subway lines, etc.), we need to expand our repertoire with additional formats designed to support rich geometries.\nGeoJSON models geographic features within a specialized JSON format. A GeoJSON feature can include geometric data – such as longitude, latitude coordinates that make up a country boundary – as well as additional data attributes.\nHere is a GeoJSON feature object for the boundary of the U.S. state of Colorado:\n{\n  \"type\": \"Feature\",\n  \"id\": 8,\n  \"properties\": {\"name\": \"Colorado\"},\n  \"geometry\": {\n    \"type\": \"Polygon\",\n    \"coordinates\": [\n      [[-106.32056285448942,40.998675790862656],[-106.19134826714341,40.99813863734313],[-105.27607827344248,40.99813863734313],[-104.9422739227986,40.99813863734313],[-104.05212898774828,41.00136155846029],[-103.57475287338661,41.00189871197981],[-103.38093099236758,41.00189871197981],[-102.65589358559272,41.00189871197981],[-102.62000064466328,41.00189871197981],[-102.052892177978,41.00189871197981],[-102.052892177978,40.74889940428302],[-102.052892177978,40.69733266640851],[-102.052892177978,40.44003613055551],[-102.052892177978,40.3492571857556],[-102.052892177978,40.00333031918079],[-102.04930288388505,39.57414465707943],[-102.04930288388505,39.56823596836465],[-102.0457135897921,39.1331416175485],[-102.0457135897921,39.0466599009048],[-102.0457135897921,38.69751011321283],[-102.0457135897921,38.61478847120581],[-102.0457135897921,38.268861604631],[-102.0457135897921,38.262415762396685],[-102.04212429569915,37.738153927339205],[-102.04212429569915,37.64415206142214],[-102.04212429569915,37.38900413964724],[-102.04212429569915,36.99365914927603],[-103.00046581851544,37.00010499151034],[-103.08660887674611,37.00010499151034],[-104.00905745863294,36.99580776335414],[-105.15404227428235,36.995270609834606],[-105.2222388620483,36.995270609834606],[-105.7175614468747,36.99580776335414],[-106.00829426840322,36.995270609834606],[-106.47490250048605,36.99365914927603],[-107.4224761410235,37.00010499151034],[-107.48349414060355,37.00010499151034],[-108.38081766383978,36.99903068447129],[-109.04483707103458,36.99903068447129],[-109.04483707103458,37.484617466122884],[-109.04124777694163,37.88049961001363],[-109.04124777694163,38.15283644441336],[-109.05919424740635,38.49983761802722],[-109.05201565922046,39.36680339854235],[-109.05201565922046,39.49786885730673],[-109.05201565922046,39.66062637372313],[-109.05201565922046,40.22248895514744],[-109.05201565922046,40.653823231326896],[-109.05201565922046,41.000287251421234],[-107.91779872584989,41.00189871197981],[-107.3183866123281,41.00297301901887],[-106.85895696843116,41.00189871197981],[-106.32056285448942,40.998675790862656]]\n    ]\n  }\n}\nThe feature includes a properties object, which can include any number of data fields, plus a geometry object, which in this case contains a single polygon that consists of [longitude, latitude] coordinates for the state boundary. The coordinates continue off to the right for a while should you care to scroll…\nTo learn more about the nitty-gritty details of GeoJSON, see the official GeoJSON specification or read Tom MacWright’s helpful primer.\nOne drawback of GeoJSON as a storage format is that it can be redundant, resulting in larger file sizes. Consider: Colorado shares boundaries with six other states (seven if you include the corner touching Arizona). Instead of using separate, overlapping coordinate lists for each of those states, a more compact approach is to encode shared borders only once, representing the topology of geographic regions. Fortunately, this is precisely what the TopoJSON format does!\nLet’s load a TopoJSON file of world countries (at 110 meter resolution):\n\nworld = data.world_110m.url\nworld\n\n'https://vega.github.io/vega-datasets/data/world-110m.json'\n\n\n\nworld_topo = data.world_110m()\n\n\nworld_topo.keys()\n\ndict_keys(['type', 'transform', 'objects', 'arcs'])\n\n\n\nworld_topo['type']\n\n'Topology'\n\n\n\nworld_topo['objects'].keys()\n\ndict_keys(['land', 'countries'])\n\n\nInspect the world_topo TopoJSON dictionary object above to see its contents.\nIn the data above, the objects property indicates the named elements we can extract from the data: geometries for all countries, or a single polygon representing all land on Earth. Either of these can be unpacked to GeoJSON data we can then visualize.\nAs TopoJSON is a specialized format, we need to instruct Altair to parse the TopoJSON format, indicating which named faeture object we wish to extract from the topology. The following code indicates that we want to extract GeoJSON features from the world dataset for the countries object:\nalt.topo_feature(world, 'countries')\nThis alt.topo_feature method call expands to the following Vega-Lite JSON:\n{\n  \"values\": world,\n  \"format\": {\"type\": \"topojson\", \"feature\": \"countries\"}\n}\nNow that we can load geographic data, we’re ready to start making maps!"
  },
  {
    "objectID": "learn/altair/altair_cartographic.html#geoshape-marks",
    "href": "learn/altair/altair_cartographic.html#geoshape-marks",
    "title": "Cartographic Visualization",
    "section": "Geoshape Marks",
    "text": "Geoshape Marks\nTo visualize geographic data, Altair provides the geoshape mark type. To create a basic map, we can create a geoshape mark and pass it our TopoJSON data, which is then unpacked into GeoJSON features, one for each country of the world:\n\nalt.Chart(alt.topo_feature(world, 'countries')).mark_geoshape()\n\n\n\n\n\n\nIn the example above, Altair applies a default blue color and uses a default map projection (mercator). We can customize the colors and boundary stroke widths using standard mark properties. Using the project method we can also add our own map projection:\n\nalt.Chart(alt.topo_feature(world, 'countries')).mark_geoshape(\n    fill='#2a1d0c', stroke='#706545', strokeWidth=0.5\n).project(\n    type='mercator'\n)\n\n\n\n\n\n\nBy default Altair automatically adjusts the projection so that all the data fits within the width and height of the chart. We can also specify projection parameters, such as scale (zoom level) and translate (panning), to customize the projection settings. Here we adjust the scale and translate parameters to focus on Europe:\n\nalt.Chart(alt.topo_feature(world, 'countries')).mark_geoshape(\n    fill='#2a1d0c', stroke='#706545', strokeWidth=0.5\n).project(\n    type='mercator', scale=400, translate=[100, 550]\n)\n\n\n\n\n\n\nNote how the 110m resolution of the data becomes apparent at this scale. To see more detailed coast lines and boundaries, we need an input file with more fine-grained geometries. Adjust the scale and translate parameters to focus the map on other regions!\nSo far our map shows countries only. Using the layer operator, we can combine multiple map elements. Altair includes data generators we can use to create data for additional map layers:\n\nThe sphere generator ({'sphere': True}) provides a GeoJSON representation of the full sphere of the Earth. We can create an additional geoshape mark that fills in the shape of the Earth as a background layer.\nThe graticule generator ({'graticule': ...}) creates a GeoJSON feature representing a graticule: a grid formed by lines of latitude and longitude. The default graticule has meridians and parallels every 10° between ±80° latitude. For the polar regions, there are meridians every 90°. These settings can be customized using the stepMinor and stepMajor properties.\n\nLet’s layer sphere, graticule, and country marks into a reusable map specification:\n\nmap = alt.layer(\n    # use the sphere of the Earth as the base layer\n    alt.Chart({'sphere': True}).mark_geoshape(\n        fill='#e6f3ff'\n    ),\n    # add a graticule for geographic reference lines\n    alt.Chart({'graticule': True}).mark_geoshape(\n        stroke='#ffffff', strokeWidth=1\n    ),\n    # and then the countries of the world\n    alt.Chart(alt.topo_feature(world, 'countries')).mark_geoshape(\n        fill='#2a1d0c', stroke='#706545', strokeWidth=0.5\n    )\n).properties(\n    width=600,\n    height=400\n)\n\nWe can extend the map with a desired projection and draw the result. Here we apply a Natural Earth projection. The sphere layer provides the light blue background; the graticule layer provides the white geographic reference lines.\n\nmap.project(\n    type='naturalEarth1', scale=110, translate=[300, 200]\n).configure_view(stroke=None)"
  },
  {
    "objectID": "learn/altair/altair_cartographic.html#point-maps",
    "href": "learn/altair/altair_cartographic.html#point-maps",
    "title": "Cartographic Visualization",
    "section": "Point Maps",
    "text": "Point Maps\nIn addition to the geometric data provided by GeoJSON or TopoJSON files, many tabular datasets include geographic information in the form of fields for longitude and latitude coordinates, or references to geographic regions such as country names, state names, postal codes, etc., which can be mapped to coordinates using a geocoding service. In some cases, location data is rich enough that we can see meaningful patterns by projecting the data points alone — no base map required!\nLet’s look at a dataset of 5-digit zip codes in the United States, including longitude, latitude coordinates for each post office in addition to a zip_code field.\n\nzipcodes = data.zipcodes.url\nzipcodes\n\n'https://vega.github.io/vega-datasets/data/zipcodes.csv'\n\n\nWe can visualize each post office location using a small (1-pixel) square mark. However, to set the positions we do not use x and y channels. Why is that?\nWhile cartographic projections map (longitude, latitude) coordinates to (x, y) coordinates, they can do so in arbitrary ways. There is no guarantee, for example, that longitude → x and latitude → y! Instead, Altair includes special longitude and latitude encoding channels to handle geographic coordinates. These channels indicate which data fields should be mapped to longitude and latitude coordinates, and then applies a projection to map those coordinates to (x, y) positions.\n\nalt.Chart(zipcodes).mark_square(\n    size=1, opacity=1\n).encode(\n    longitude='longitude:Q', # apply the field named 'longitude' to the longitude channel\n    latitude='latitude:Q'    # apply the field named 'latitude' to the latitude channel\n).project(\n    type='albersUsa'\n).properties(\n    width=900,\n    height=500\n).configure_view(\n    stroke=None\n)\n\n\n\n\n\n\nPlotting zip codes only, we can see the outline of the United States and discern meaningful patterns in the density of post offices, without a base map or additional reference elements!\nWe use the albersUsa projection, which takes some liberties with the actual geometry of the Earth, with scaled versions of Alaska and Hawaii in the bottom-left corner. As we did not specify projection scale or translate parameters, Altair sets them automatically to fit the visualized data.\nWe can now go on to ask more questions of our dataset. For example, is there any rhyme or reason to the allocation of zip codes? To assess this question we can add a color encoding based on the first digit of the zip code. We first add a calculate transform to extract the first digit, and encode the result using the color channel:\n\nalt.Chart(zipcodes).transform_calculate(\n    digit='datum.zip_code[0]'\n).mark_square(\n    size=2, opacity=1\n).encode(\n    longitude='longitude:Q',\n    latitude='latitude:Q',\n    color='digit:N'\n).project(\n    type='albersUsa'\n).properties(\n    width=900,\n    height=500\n).configure_view(\n    stroke=None\n)\n\n\n\n\n\n\nTo zoom in on a specific digit, add a filter transform to limit the data shown! Try adding an interactive selection to filter to a single digit and dynamically update the map. And be sure to use strings (`‘1’`) instead of numbers (`1`) when filtering digit values!\n(This example is inspired by Ben Fry’s classic zipdecode visualization!)\nWe might further wonder what the sequence of zip codes might indicate. One way to explore this question is to connect each consecutive zip code using a line mark, as done in Robert Kosara’s ZipScribble visualization:\n\nalt.Chart(zipcodes).transform_filter(\n    '-150 &lt; datum.longitude && 22 &lt; datum.latitude && datum.latitude &lt; 55'\n).transform_calculate(\n    digit='datum.zip_code[0]'\n).mark_line(\n    strokeWidth=0.5\n).encode(\n    longitude='longitude:Q',\n    latitude='latitude:Q',\n    color='digit:N',\n    order='zip_code:O'\n).project(\n    type='albersUsa'\n).properties(\n    width=900,\n    height=500\n).configure_view(\n    stroke=None\n)\n\n\n\n\n\n\nWe can now see how zip codes further cluster into smaller areas, indicating a hierarchical allocation of codes by location, but with some notable variability within local clusters.\nIf you were paying careful attention to our earlier maps, you may have noticed that there are zip codes being plotted in the upper-left corner! These correspond to locations such as Puerto Rico or American Samoa, which contain U.S. zip codes but are mapped to null coordinates (0, 0) by the albersUsa projection. In addition, Alaska and Hawaii can complicate our view of the connecting line segments. In response, the code above includes an additional filter that removes points outside our chosen longitude and latitude spans.\nRemove the filter above to see what happens!"
  },
  {
    "objectID": "learn/altair/altair_cartographic.html#symbol-maps",
    "href": "learn/altair/altair_cartographic.html#symbol-maps",
    "title": "Cartographic Visualization",
    "section": "Symbol Maps",
    "text": "Symbol Maps\nNow let’s combine a base map and plotted data as separate layers. We’ll examine the U.S. commercial flight network, considering both airports and flight routes. To do so, we’ll need three datasets. For our base map, we’ll use a TopoJSON file for the United States at 10m resolution, containing features for states or counties:\n\nusa = data.us_10m.url\nusa\n\n'https://vega.github.io/vega-datasets/data/us-10m.json'\n\n\nFor the airports, we will use a dataset with fields for the longitude and latitude coordinates of each airport as well as the iata airport code — for example, 'SEA' for Seattle-Tacoma International Airport.\n\nairports = data.airports.url\nairports\n\n'https://vega.github.io/vega-datasets/data/airports.csv'\n\n\nFinally, we will use a dataset of flight routes, which contains origin and destination fields with the IATA codes for the corresponding airports:\n\nflights = data.flights_airport.url\nflights\n\n'https://vega.github.io/vega-datasets/data/flights-airport.csv'\n\n\nLet’s start by creating a base map using the albersUsa projection, and add a layer that plots circle marks for each airport:\n\nalt.layer(\n    alt.Chart(alt.topo_feature(usa, 'states')).mark_geoshape(\n        fill='#ddd', stroke='#fff', strokeWidth=1\n    ),\n    alt.Chart(airports).mark_circle(size=9).encode(\n        latitude='latitude:Q',\n        longitude='longitude:Q',\n        tooltip='iata:N'\n    )\n).project(\n    type='albersUsa'\n).properties(\n    width=900,\n    height=500\n).configure_view(\n    stroke=None\n)\n\n\n\n\n\n\nThat’s a lot of airports! Obviously, not all of them are major hubs.\nSimilar to our zip codes dataset, our airport data includes points that lie outside the continental United States. So we again see points in the upper-left corner. We might want to filter these points, but to do so we first need to know more about them.\nUpdate the map projection above to albers – side-stepping the idiosyncratic behavior of albersUsa – so that the actual locations of these additional points is revealed!\nNow, instead of showing all airports in an undifferentiated fashion, let’s identify major hubs by considering the total number of routes that originate at each airport. We’ll use the routes dataset as our primary data source: it contains a list of flight routes that we can aggregate to count the number of routes for each origin airport.\nHowever, the routes dataset does not include the locations of the airports! To augment the routes data with locations, we need a new data transformation: lookup. The lookup transform takes a field value in a primary dataset and uses it as a key to look up related information in another table. In this case, we want to match the origin airport code in our routes dataset against the iata field of the airports dataset, then extract the corresponding latitude and longitude fields.\n\nalt.layer(\n    alt.Chart(alt.topo_feature(usa, 'states')).mark_geoshape(\n        fill='#ddd', stroke='#fff', strokeWidth=1\n    ),\n    alt.Chart(flights).mark_circle().transform_aggregate(\n        groupby=['origin'],\n        routes='count()'\n    ).transform_lookup(\n        lookup='origin',\n        from_=alt.LookupData(data=airports, key='iata',\n                             fields=['state', 'latitude', 'longitude'])\n    ).transform_filter(\n        'datum.state !== \"PR\" && datum.state !== \"VI\"'\n    ).encode(\n        latitude='latitude:Q',\n        longitude='longitude:Q',\n        tooltip=['origin:N', 'routes:Q'],\n        size=alt.Size('routes:Q', scale=alt.Scale(range=[0, 1000]), legend=None),\n        order=alt.Order('routes:Q', sort='descending')\n    )\n).project(\n    type='albersUsa'\n).properties(\n    width=900,\n    height=500\n).configure_view(\n    stroke=None\n)\n\n\n\n\n\n\nWhich U.S. airports have the highest number of outgoing routes?\nNow that we can see the airports, which may wish to interact with them to better understand the structure of the air traffic network. We can add a rule mark layer to represent paths from origin airports to destination airports, which requires two lookup transforms to retreive coordinates for each end point. In addition, we can use a single selection to filter these routes, such that only the routes originating at the currently selected airport are shown.\nStarting from the static map above, can you build an interactive version? Feel free to skip the code below to engage with the interactive map first, and think through how you might build it on your own!\n\n# interactive selection for origin airport\n# select nearest airport to mouse cursor\norigin = alt.selection_single(\n    on='mouseover', nearest=True,\n    fields=['origin'], empty='none'\n)\n\n# shared data reference for lookup transforms\nforeign = alt.LookupData(data=airports, key='iata',\n                         fields=['latitude', 'longitude'])\n    \nalt.layer(\n    # base map of the United States\n    alt.Chart(alt.topo_feature(usa, 'states')).mark_geoshape(\n        fill='#ddd', stroke='#fff', strokeWidth=1\n    ),\n    # route lines from selected origin airport to destination airports\n    alt.Chart(flights).mark_rule(\n        color='#000', opacity=0.35\n    ).transform_filter(\n        origin # filter to selected origin only\n    ).transform_lookup(\n        lookup='origin', from_=foreign # origin lat/lon\n    ).transform_lookup(\n        lookup='destination', from_=foreign, as_=['lat2', 'lon2'] # dest lat/lon\n    ).encode(\n        latitude='latitude:Q',\n        longitude='longitude:Q',\n        latitude2='lat2',\n        longitude2='lon2',\n    ),\n    # size airports by number of outgoing routes\n    # 1. aggregate flights-airport data set\n    # 2. lookup location data from airports data set\n    # 3. remove Puerto Rico (PR) and Virgin Islands (VI)\n    alt.Chart(flights).mark_circle().transform_aggregate(\n        groupby=['origin'],\n        routes='count()'\n    ).transform_lookup(\n        lookup='origin',\n        from_=alt.LookupData(data=airports, key='iata',\n                             fields=['state', 'latitude', 'longitude'])\n    ).transform_filter(\n        'datum.state !== \"PR\" && datum.state !== \"VI\"'\n    ).add_selection(\n        origin\n    ).encode(\n        latitude='latitude:Q',\n        longitude='longitude:Q',\n        tooltip=['origin:N', 'routes:Q'],\n        size=alt.Size('routes:Q', scale=alt.Scale(range=[0, 1000]), legend=None),\n        order=alt.Order('routes:Q', sort='descending') # place smaller circles on top\n    )\n).project(\n    type='albersUsa'\n).properties(\n    width=900,\n    height=500\n).configure_view(\n    stroke=None\n)\n\n\n\n\n\n\nMouseover the map to probe the flight network!"
  },
  {
    "objectID": "learn/altair/altair_cartographic.html#choropleth-maps",
    "href": "learn/altair/altair_cartographic.html#choropleth-maps",
    "title": "Cartographic Visualization",
    "section": "Choropleth Maps",
    "text": "Choropleth Maps\nA choropleth map uses shaded or textured regions to visualize data values. Sized symbol maps are often more accurate to read, as people tend to be better at estimating proportional differences between the area of circles than between color shades. Nevertheless, choropleth maps are popular in practice and particularly useful when too many symbols become perceptually overwhelming.\nFor example, while the United States only has 50 states, it has thousands of counties within those states. Let’s build a choropleth map of the unemployment rate per county, back in the recession year of 2008. In some cases, input GeoJSON or TopoJSON files might include statistical data that we can directly visualize. In this case, however, we have two files: our TopoJSON file that includes county boundary features (usa), and a separate text file that contains unemployment statistics:\n\nunemp = data.unemployment.url\nunemp\n\n'https://vega.github.io/vega-datasets/data/unemployment.tsv'\n\n\nTo integrate our data sources, we will again need to use the lookup transform, augmenting our TopoJSON-based geoshape data with unemployment rates. We can then create a map that includes a color encoding for the looked-up rate field.\n\nalt.Chart(alt.topo_feature(usa, 'counties')).mark_geoshape(\n    stroke='#aaa', strokeWidth=0.25\n).transform_lookup(\n    lookup='id', from_=alt.LookupData(data=unemp, key='id', fields=['rate'])\n).encode(\n    alt.Color('rate:Q',\n              scale=alt.Scale(domain=[0, 0.3], clamp=True), \n              legend=alt.Legend(format='%')),\n    alt.Tooltip('rate:Q', format='.0%')\n).project(\n    type='albersUsa'\n).properties(\n    width=900,\n    height=500\n).configure_view(\n    stroke=None\n)\n\n\n\n\n\n\nExamine the unemployment rates by county. Higher values in Michigan may relate to the automotive industry. Counties in the Great Plains and Mountain states exhibit both low and high rates. Is this variation meaningful, or is it possibly an artifact of lower sample sizes? To explore further, try changing the upper scale domain (e.g., to 0.2) to adjust the color mapping.\nA central concern for choropleth maps is the choice of colors. Above, we use Altair’s default 'yellowgreenblue' scheme for heatmaps. Below we compare other schemes, including a single-hue sequential scheme (teals) that varies in luminance only, a multi-hue sequential scheme (viridis) that ramps in both luminance and hue, and a diverging scheme (blueorange) that uses a white mid-point:\n\n# utility function to generate a map specification for a provided color scheme\ndef map_(scheme):\n    return alt.Chart().mark_geoshape().project(type='albersUsa').encode(\n        alt.Color('rate:Q', scale=alt.Scale(scheme=scheme), legend=None)\n    ).properties(width=305, height=200)\n\nalt.hconcat(\n    map_('tealblues'), map_('viridis'), map_('blueorange'),\n    data=alt.topo_feature(usa, 'counties')\n).transform_lookup(\n    lookup='id', from_=alt.LookupData(data=unemp, key='id', fields=['rate'])\n).configure_view(\n    stroke=None\n).resolve_scale(\n    color='independent'\n)\n\n\n\n\n\n\nWhich color schemes do you find to be more effective? Why might that be? Modify the maps above to use other available schemes, as described in the Vega Color Schemes documentation."
  },
  {
    "objectID": "learn/altair/altair_cartographic.html#cartographic-projections",
    "href": "learn/altair/altair_cartographic.html#cartographic-projections",
    "title": "Cartographic Visualization",
    "section": "Cartographic Projections",
    "text": "Cartographic Projections\nNow that we have some experience creating maps, let’s take a closer look at cartographic projections. As explained by Wikipedia,\n\nAll map projections necessarily distort the surface in some fashion. Depending on the purpose of the map, some distortions are acceptable and others are not; therefore, different map projections exist in order to preserve some properties of the sphere-like body at the expense of other properties.\n\nSome of the properties we might wish to consider include:\n\nArea: Does the projection distort region sizes?\nBearing: Does a straight line correspond to a constant direction of travel?\nDistance: Do lines of equal length correspond to equal distances on the globe?\nShape: Does the projection preserve spatial relations (angles) between points?\n\nSelecting an appropriate projection thus depends on the use case for the map. For example, if we are assessing land use and the extent of land matters, we might choose an area-preserving projection. If we want to visualize shockwaves emanating from an earthquake, we might focus the map on the quake’s epicenter and preserve distances outward from that point. Or, if we wish to aid navigation, the preservation of bearing and shape may be more important.\nWe can also characterize projections in terms of the projection surface. Cylindrical projections, for example, project surface points of the sphere onto a surrounding cylinder; the “unrolled” cylinder then provides our map. As we further describe below, we might alternatively project onto the surface of a cone (conic projections) or directly onto a flat plane (azimuthal projections).\nLet’s first build up our intuition by interacting with a variety of projections! Open the online Vega-Lite Cartographic Projections notebook. Use the controls on that page to select a projection and explore projection parameters, such as the scale (zooming) and x/y translation (panning). The rotation (yaw, pitch, roll) controls determine the orientation of the globe relative to the surface being projected upon.\n\nA Tour of Specific Projection Types\nCylindrical projections map the sphere onto a surrounding cylinder, then unroll the cylinder. If the major axis of the cylinder is oriented north-south, meridians are mapped to straight lines. Pseudo-cylindrical projections represent a central meridian as a straight line, with other meridians “bending” away from the center.\n\nminimap = map.properties(width=225, height=225)\nalt.hconcat(\n    minimap.project(type='equirectangular').properties(title='equirectangular'),\n    minimap.project(type='mercator').properties(title='mercator'),\n    minimap.project(type='transverseMercator').properties(title='transverseMercator'),\n    minimap.project(type='naturalEarth1').properties(title='naturalEarth1')\n).properties(spacing=10).configure_view(stroke=None)\n\n\n\n\n\n\n\nEquirectangular (equirectangular): Scale lat, lon coordinate values directly.\nMercator (mercator): Project onto a cylinder, using lon directly, but subjecting lat to a non-linear transformation. Straight lines preserve constant compass bearings (rhumb lines), making this projection well-suited to navigation. However, areas in the far north or south can be greatly distorted.\nTransverse Mercator (transverseMercator): A mercator projection, but with the bounding cylinder rotated to a transverse axis. Whereas the standard Mercator projection has highest accuracy along the equator, the Transverse Mercator projection is most accurate along the central meridian.\nNatural Earth (naturalEarth1): A pseudo-cylindrical projection designed for showing the whole Earth in one view. \n\nConic projections map the sphere onto a cone, and then unroll the cone on to the plane. Conic projections are configured by two standard parallels, which determine where the cone intersects the globe.\n\nminimap = map.properties(width=180, height=130)\nalt.hconcat(\n    minimap.project(type='conicEqualArea').properties(title='conicEqualArea'),\n    minimap.project(type='conicEquidistant').properties(title='conicEquidistant'),\n    minimap.project(type='conicConformal', scale=35, translate=[90,65]).properties(title='conicConformal'),\n    minimap.project(type='albers').properties(title='albers'),\n    minimap.project(type='albersUsa').properties(title='albersUsa')\n).properties(spacing=10).configure_view(stroke=None)\n\n\n\n\n\n\n\nConic Equal Area (conicEqualArea): Area-preserving conic projection. Shape and distance are not preserved, but roughly accurate within standard parallels.\nConic Equidistant (conicEquidistant): Conic projection that preserves distance along the meridians and standard parallels.\nConic Conformal (conicConformal): Conic projection that preserves shape (local angles), but not area or distance.\nAlbers (albers): A variant of the conic equal area projection with standard parallels optimized for creating maps of the United States.\nAlbers USA (albersUsa): A hybrid projection for the 50 states of the United States of America. This projection stitches together three Albers projections with different parameters for the continental U.S., Alaska, and Hawaii. \n\nAzimuthal projections map the sphere directly onto a plane.\n\nminimap = map.properties(width=180, height=180)\nalt.hconcat(\n    minimap.project(type='azimuthalEqualArea').properties(title='azimuthalEqualArea'),\n    minimap.project(type='azimuthalEquidistant').properties(title='azimuthalEquidistant'),\n    minimap.project(type='orthographic').properties(title='orthographic'),\n    minimap.project(type='stereographic').properties(title='stereographic'),\n    minimap.project(type='gnomonic').properties(title='gnomonic')\n).properties(spacing=10).configure_view(stroke=None)\n\n\n\n\n\n\n\nAzimuthal Equal Area (azimuthalEqualArea): Accurately projects area in all parts of the globe, but does not preserve shape (local angles).\nAzimuthal Equidistant (azimuthalEquidistant): Preserves proportional distance from the projection center to all other points on the globe.\nOrthographic (orthographic): Projects a visible hemisphere onto a distant plane. Approximately matches a view of the Earth from outer space.\nStereographic (stereographic): Preserves shape, but not area or distance.\nGnomonic (gnomonic): Projects the surface of the sphere directly onto a tangent plane. Great circles around the Earth are projected to straight lines, showing the shortest path between points."
  },
  {
    "objectID": "learn/altair/altair_cartographic.html#coda-wrangling-geographic-data",
    "href": "learn/altair/altair_cartographic.html#coda-wrangling-geographic-data",
    "title": "Cartographic Visualization",
    "section": "Coda: Wrangling Geographic Data",
    "text": "Coda: Wrangling Geographic Data\nThe examples above all draw from the vega-datasets collection, including geometric (TopoJSON) and tabular (airports, unemployment rates) data. A common challenge to getting starting with geographic visualization is collecting the necessary data for your task. A number of data providers abound, including services such as the United States Geological Survey and U.S. Census Bureau.\nIn many cases you may have existing data with a geographic component, but require additional measures or geometry. To help you get started, here is one workflow:\n\nVisit Natural Earth Data and browse to select data for regions and resolutions of interest. Download the corresponding zip file(s).\nGo to MapShaper and drop your downloaded zip file onto the page. Revise the data as desired, and then “Export” generated TopoJSON or GeoJSON files.\nLoad the exported data from MapShaper for use with Altair!\n\nOf course, many other tools – both open-source and proprietary – exist for working with geographic data. For more about geo-data wrangling and map creation, see Mike Bostock’s tutorial series on Command-Line Cartography."
  },
  {
    "objectID": "learn/altair/altair_cartographic.html#summary",
    "href": "learn/altair/altair_cartographic.html#summary",
    "title": "Cartographic Visualization",
    "section": "Summary",
    "text": "Summary\nAt this point, we’ve only dipped our toes into the waters of map-making. (You didn’t expect a single notebook to impart centuries of learning, did you?) For example, we left untouched topics such as cartograms and conveying topography — as in Imhof’s illuminating book Cartographic Relief Presentation. Nevertheless, you should now be well-equipped to create a rich array of geo-visualizations. For more, MacEachren’s book How Maps Work: Representation, Visualization, and Design provides a valuable overview of map-making from the perspective of data visualization."
  },
  {
    "objectID": "learn/altair/altair_data_transformation.html",
    "href": "learn/altair/altair_data_transformation.html",
    "title": "Data Transformation",
    "section": "",
    "text": "In previous notebooks we learned how to use marks and visual encodings to represent individual data records. Here we will explore methods for transforming data, including the use of aggregates to summarize multiple records. Data transformation is an integral part of visualization: choosing the variables to show and their level of detail is just as important as choosing appropriate visual encodings. After all, it doesn’t matter how well chosen your visual encodings are if you are showing the wrong information!\nAs you work through this module, we recommend that you open the Altair Data Transformations documentation in another tab. It will be a useful resource if at any point you’d like more details or want to see what other transformations are available.\nThis notebook is part of the data visualization curriculum.\nimport pandas as pd\nimport altair as alt"
  },
  {
    "objectID": "learn/altair/altair_data_transformation.html#the-movies-dataset",
    "href": "learn/altair/altair_data_transformation.html#the-movies-dataset",
    "title": "Data Transformation",
    "section": "The Movies Dataset",
    "text": "The Movies Dataset\nWe will be working with a table of data about motion pictures, taken from the vega-datasets collection. The data includes variables such as the film name, director, genre, release date, ratings, and gross revenues. However, be careful when working with this data: the films are from unevenly sampled years, using data combined from multiple sources. If you dig in you will find issues with missing values and even some subtle errors! Nevertheless, the data should prove interesting to explore…\nLet’s retrieve the URL for the JSON data file from the vega_datasets package, and then read the data into a Pandas data frame so that we can inspect its contents.\n\nmovies_url = 'https://cdn.jsdelivr.net/npm/vega-datasets@1/data/movies.json'\nmovies = pd.read_json(movies_url)\n\nHow many rows (records) and columns (fields) are in the movies dataset?\n\nmovies.shape\n\n(3201, 16)\n\n\nNow let’s peek at the first 5 rows of the table to get a sense of the fields and data types…\n\nmovies.head(5)\n\n\n\n\n\n\n\n\nTitle\nUS_Gross\nWorldwide_Gross\nUS_DVD_Sales\nProduction_Budget\nRelease_Date\nMPAA_Rating\nRunning_Time_min\nDistributor\nSource\nMajor_Genre\nCreative_Type\nDirector\nRotten_Tomatoes_Rating\nIMDB_Rating\nIMDB_Votes\n\n\n\n\n0\nThe Land Girls\n146083.0\n146083.0\nNaN\n8000000.0\nJun 12 1998\nR\nNaN\nGramercy\nNone\nNone\nNone\nNone\nNaN\n6.1\n1071.0\n\n\n1\nFirst Love, Last Rites\n10876.0\n10876.0\nNaN\n300000.0\nAug 07 1998\nR\nNaN\nStrand\nNone\nDrama\nNone\nNone\nNaN\n6.9\n207.0\n\n\n2\nI Married a Strange Person\n203134.0\n203134.0\nNaN\n250000.0\nAug 28 1998\nNone\nNaN\nLionsgate\nNone\nComedy\nNone\nNone\nNaN\n6.8\n865.0\n\n\n3\nLet's Talk About Sex\n373615.0\n373615.0\nNaN\n300000.0\nSep 11 1998\nNone\nNaN\nFine Line\nNone\nComedy\nNone\nNone\n13.0\nNaN\nNaN\n\n\n4\nSlam\n1009819.0\n1087521.0\nNaN\n1000000.0\nOct 09 1998\nR\nNaN\nTrimark\nOriginal Screenplay\nDrama\nContemporary Fiction\nNone\n62.0\n3.4\n165.0"
  },
  {
    "objectID": "learn/altair/altair_data_transformation.html#histograms",
    "href": "learn/altair/altair_data_transformation.html#histograms",
    "title": "Data Transformation",
    "section": "Histograms",
    "text": "Histograms\nWe’ll start our transformation tour by binning data into discrete groups and counting records to summarize those groups. The resulting plots are known as histograms.\nLet’s first look at unaggregated data: a scatter plot showing movie ratings from Rotten Tomatoes versus ratings from IMDB users. We’ll provide data to Altair by passing the movies data URL to the Chart method. (We could also pass the Pandas data frame directly to get the same result.) We can then encode the Rotten Tomatoes and IMDB ratings fields using the x and y channels:\n\nalt.Chart(movies_url).mark_circle().encode(\n    alt.X('Rotten_Tomatoes_Rating:Q'),\n    alt.Y('IMDB_Rating:Q')\n)\n\n\n\n\n\n\nTo summarize this data, we can bin a data field to group numeric values into discrete groups. Here we bin along the x-axis by adding bin=True to the x encoding channel. The result is a set of ten bins of equal step size, each corresponding to a span of ten ratings points.\n\nalt.Chart(movies_url).mark_circle().encode(\n    alt.X('Rotten_Tomatoes_Rating:Q', bin=True),\n    alt.Y('IMDB_Rating:Q')\n)\n\n\n\n\n\n\nSetting bin=True uses default binning settings, but we can exercise more control if desired. Let’s instead set the maximum bin count (maxbins) to 20, which has the effect of doubling the number of bins. Now each bin corresponds to a span of five ratings points.\n\nalt.Chart(movies_url).mark_circle().encode(\n    alt.X('Rotten_Tomatoes_Rating:Q', bin=alt.BinParams(maxbins=20)),\n    alt.Y('IMDB_Rating:Q')\n)\n\n\n\n\n\n\nWith the data binned, let’s now summarize the distribution of Rotten Tomatoes ratings. We will drop the IMDB ratings for now and instead use the y encoding channel to show an aggregate count of records, so that the vertical position of each point indicates the number of movies per Rotten Tomatoes rating bin.\nAs the count aggregate counts the number of total records in each bin regardless of the field values, we do not need to include a field name in the y encoding.\n\nalt.Chart(movies_url).mark_circle().encode(\n    alt.X('Rotten_Tomatoes_Rating:Q', bin=alt.BinParams(maxbins=20)),\n    alt.Y('count()')\n)\n\n\n\n\n\n\nTo arrive at a standard histogram, let’s change the mark type from circle to bar:\n\nalt.Chart(movies_url).mark_bar().encode(\n    alt.X('Rotten_Tomatoes_Rating:Q', bin=alt.BinParams(maxbins=20)),\n    alt.Y('count()')\n)\n\n\n\n\n\n\nWe can now examine the distribution of ratings more clearly: we can see fewer movies on the negative end, and a bit more movies on the high end, but a generally uniform distribution overall. Rotten Tomatoes ratings are determined by taking “thumbs up” and “thumbs down” judgments from film critics and calculating the percentage of positive reviews. It appears this approach does a good job of utilizing the full range of rating values.\nSimilarly, we can create a histogram for IMDB ratings by changing the field in the x encoding channel:\n\nalt.Chart(movies_url).mark_bar().encode(\n    alt.X('IMDB_Rating:Q', bin=alt.BinParams(maxbins=20)),\n    alt.Y('count()')\n)\n\n\n\n\n\n\nIn contrast to the more uniform distribution we saw before, IMDB ratings exhibit a bell-shaped (though negatively skewed) distribution. IMDB ratings are formed by averaging scores (ranging from 1 to 10) provided by the site’s users. We can see that this form of measurement leads to a different shape than the Rotten Tomatoes ratings. We can also see that the mode of the distribution is between 6.5 and 7: people generally enjoy watching movies, potentially explaining the positive bias!\nNow let’s turn back to our scatter plot of Rotten Tomatoes and IMDB ratings. Here’s what happens if we bin both axes of our original plot.\n\nalt.Chart(movies_url).mark_circle().encode(\n    alt.X('Rotten_Tomatoes_Rating:Q', bin=alt.BinParams(maxbins=20)),\n    alt.Y('IMDB_Rating:Q', bin=alt.BinParams(maxbins=20)),\n)\n\n\n\n\n\n\nDetail is lost due to overplotting, with many points drawn directly on top of each other.\nTo form a two-dimensional histogram we can add a count aggregate as before. As both the x and y encoding channels are already taken, we must use a different encoding channel to convey the counts. Here is the result of using circular area by adding a size encoding channel.\n\nalt.Chart(movies_url).mark_circle().encode(\n    alt.X('Rotten_Tomatoes_Rating:Q', bin=alt.BinParams(maxbins=20)),\n    alt.Y('IMDB_Rating:Q', bin=alt.BinParams(maxbins=20)),\n    alt.Size('count()')\n)\n\n\n\n\n\n\nAlternatively, we can encode counts using the color channel and change the mark type to bar. The result is a two-dimensional histogram in the form of a heatmap.\n\nalt.Chart(movies_url).mark_bar().encode(\n    alt.X('Rotten_Tomatoes_Rating:Q', bin=alt.BinParams(maxbins=20)),\n    alt.Y('IMDB_Rating:Q', bin=alt.BinParams(maxbins=20)),\n    alt.Color('count()')\n)\n\n\n\n\n\n\nCompare the size and color-based 2D histograms above. Which encoding do you think should be preferred? Why? In which plot can you more precisely compare the magnitude of individual values? In which plot can you more accurately see the overall density of ratings?"
  },
  {
    "objectID": "learn/altair/altair_data_transformation.html#aggregation",
    "href": "learn/altair/altair_data_transformation.html#aggregation",
    "title": "Data Transformation",
    "section": "Aggregation",
    "text": "Aggregation\nCounts are just one type of aggregate. We might also calculate summaries using measures such as the average, median, min, or max. The Altair documentation includes the full set of available aggregation functions.\nLet’s look at some examples!\n\nAverages and Sorting\nDo different genres of films receive consistently different ratings from critics? As a first step towards answering this question, we might examine the average (a.k.a. the arithmetic mean) rating for each genre of movie.\nLet’s visualize genre along the y axis and plot average Rotten Tomatoes ratings along the x axis.\n\nalt.Chart(movies_url).mark_bar().encode(\n    alt.X('average(Rotten_Tomatoes_Rating):Q'),\n    alt.Y('Major_Genre:N')\n)\n\n\n\n\n\n\nThere does appear to be some interesting variation, but looking at the data as an alphabetical list is not very helpful for ranking critical reactions to the genres.\nFor a tidier picture, let’s sort the genres in descending order of average rating. To do so, we will add a sort parameter to the y encoding channel, stating that we wish to sort by the average (op, the aggregate operation) Rotten Tomatoes rating (the field) in descending order.\n\nalt.Chart(movies_url).mark_bar().encode(\n    alt.X('average(Rotten_Tomatoes_Rating):Q'),\n    alt.Y('Major_Genre:N', sort=alt.EncodingSortField(\n        op='average', field='Rotten_Tomatoes_Rating', order='descending')\n    )\n)\n\n\n\n\n\n\nThe sorted plot suggests that critics think highly of documentaries, musicals, westerns, and dramas, but look down upon romantic comedies and horror films… and who doesn’t love null movies!?\n\n\nMedians and the Inter-Quartile Range\nWhile averages are a common way to summarize data, they can sometimes mislead. For example, very large or very small values (outliers) might skew the average. To be safe, we can compare the genres according to the median ratings as well.\nThe median is a point that splits the data evenly, such that half of the values are less than the median and the other half are greater. The median is less sensitive to outliers and so is referred to as a robust statistic. For example, arbitrarily increasing the largest rating value will not cause the median to change.\nLet’s update our plot to use a median aggregate and sort by those values:\n\nalt.Chart(movies_url).mark_bar().encode(\n    alt.X('median(Rotten_Tomatoes_Rating):Q'),\n    alt.Y('Major_Genre:N', sort=alt.EncodingSortField(\n        op='median', field='Rotten_Tomatoes_Rating', order='descending')\n    )\n)\n\n\n\n\n\n\nWe can see that some of the genres with similar averages have swapped places (films of unknown genre, or null, are now rated highest!), but the overall groups have stayed stable. Horror films continue to get little love from professional film critics.\nIt’s a good idea to stay skeptical when viewing aggregate statistics. So far we’ve only looked at point estimates. We have not examined how ratings vary within a genre.\nLet’s visualize the variation among the ratings to add some nuance to our rankings. Here we will encode the inter-quartile range (IQR) for each genre. The IQR is the range in which the middle half of data values reside. A quartile contains 25% of the data values. The inter-quartile range consists of the two middle quartiles, and so contains the middle 50%.\nTo visualize ranges, we can use the x and x2 encoding channels to indicate the starting and ending points. We use the aggregate functions q1 (the lower quartile boundary) and q3 (the upper quartile boundary) to provide the inter-quartile range. (In case you are wondering, q2 would be the median.)\n\nalt.Chart(movies_url).mark_bar().encode(\n    alt.X('q1(Rotten_Tomatoes_Rating):Q'),\n    alt.X2('q3(Rotten_Tomatoes_Rating):Q'),\n    alt.Y('Major_Genre:N', sort=alt.EncodingSortField(\n        op='median', field='Rotten_Tomatoes_Rating', order='descending')\n    )\n)\n\n\n\n\n\n\n\n\nTime Units\nNow let’s ask a completely different question: do box office returns vary by season?\nTo get an initial answer, let’s plot the median U.S. gross revenue by month.\nTo make this chart, use the timeUnit transform to map release dates to the month of the year. The result is similar to binning, but using meaningful time intervals. Other valid time units include year, quarter, date (numeric day in month), day (day of the week), and hours, as well as compound units such as yearmonth or hoursminutes. See the Altair documentation for a complete list of time units.\n\nalt.Chart(movies_url).mark_area().encode(\n    alt.X('month(Release_Date):T'),\n    alt.Y('median(US_Gross):Q')\n)\n\n\n\n\n\n\nLooking at the resulting plot, median movie sales in the U.S. appear to spike around the summer blockbuster season and the end of year holiday period. Of course, people around the world (not just the U.S.) go out to the movies. Does a similar pattern arise for worldwide gross revenue?\n\nalt.Chart(movies_url).mark_area().encode(\n    alt.X('month(Release_Date):T'),\n    alt.Y('median(Worldwide_Gross):Q')\n)\n\n\n\n\n\n\nYes!"
  },
  {
    "objectID": "learn/altair/altair_data_transformation.html#advanced-data-transformation",
    "href": "learn/altair/altair_data_transformation.html#advanced-data-transformation",
    "title": "Data Transformation",
    "section": "Advanced Data Transformation",
    "text": "Advanced Data Transformation\nThe examples above all use transformations (bin, timeUnit, aggregate, sort) that are defined relative to an encoding channel. However, at times you may want to apply a chain of multiple transformations prior to visualization, or use transformations that don’t integrate into encoding definitions. For such cases, Altair and Vega-Lite support data transformations defined separately from encodings. These transformations are applied to the data before any encodings are considered.\nWe could also perform transformations using Pandas directly, and then visualize the result. However, using the built-in transforms allows our visualizations to be published more easily in other contexts; for example, exporting the Vega-Lite JSON to use in a stand-alone web interface. Let’s look at the built-in transforms supported by Altair, such as calculate, filter, aggregate, and window.\n\nCalculate\nThink back to our comparison of U.S. gross and worldwide gross. Doesn’t worldwide revenue include the U.S.? (Indeed it does.) How might we get a better sense of trends outside the U.S.?\nWith the calculate transform we can derive new fields. Here we want to subtract U.S. gross from worldwide gross. The calculate transform takes a Vega expression string to define a formula over a single record. Vega expressions use JavaScript syntax. The datum. prefix accesses a field value on the input record.\n\nalt.Chart(movies).mark_area().transform_calculate(\n    NonUS_Gross='datum.Worldwide_Gross - datum.US_Gross'\n).encode(\n    alt.X('month(Release_Date):T'),\n    alt.Y('median(NonUS_Gross):Q')\n)\n\n\n\n\n\n\n\nWe can see that seasonal trends hold outside the U.S., but with a more pronounced decline in the non-peak months.\n\n\nFilter\nThe filter transform creates a new table with a subset of the original data, removing rows that fail to meet a provided predicate test. Similar to the calculate transform, filter predicates are expressed using the Vega expression language.\nBelow we add a filter to limit our initial scatter plot of IMDB vs. Rotten Tomatoes ratings to only films in the major genre of “Romantic Comedy”.\n\nalt.Chart(movies_url).mark_circle().encode(\n    alt.X('Rotten_Tomatoes_Rating:Q'),\n    alt.Y('IMDB_Rating:Q')\n).transform_filter('datum.Major_Genre == \"Romantic Comedy\"')\n\n\n\n\n\n\nHow does the plot change if we filter to view other genres? Edit the filter expression to find out.\nNow let’s filter to look at films released before 1970.\n\nalt.Chart(movies_url).mark_circle().encode(\n    alt.X('Rotten_Tomatoes_Rating:Q'),\n    alt.Y('IMDB_Rating:Q')\n).transform_filter('year(datum.Release_Date) &lt; 1970')\n\n\n\n\n\n\nThey seem to score unusually high! Are older films simply better, or is there a selection bias towards more highly-rated older films in this dataset?\n\n\nAggregate\nWe have already seen aggregate transforms such as count and average in the context of encoding channels. We can also specify aggregates separately, as a pre-processing step for other transforms (as in the window transform examples below). The output of an aggregate transform is a new data table with records that contain both the groupby fields and the computed aggregate measures.\nLet’s recreate our plot of average ratings by genre, but this time using a separate aggregate transform. The output table from the aggregate transform contains 13 rows, one for each genre.\nTo order the y axis we must include a required aggregate operation in our sorting instructions. Here we use the max operator, which works fine because there is only one output record per genre. We could similarly use the min operator and end up with the same plot.\n\nalt.Chart(movies_url).mark_bar().transform_aggregate(\n    groupby=['Major_Genre'],\n    Average_Rating='average(Rotten_Tomatoes_Rating)'\n).encode(\n    alt.X('Average_Rating:Q'),\n    alt.Y('Major_Genre:N', sort=alt.EncodingSortField(\n        op='max', field='Average_Rating', order='descending'\n      )\n    )\n)\n\n\n\n\n\n\n\n\nWindow\nThe window transform performs calculations over sorted groups of data records. Window transforms are quite powerful, supporting tasks such as ranking, lead/lag analysis, cumulative totals, and running sums or averages. Values calculated by a window transform are written back to the input data table as new fields. Window operations include the aggregate operations we’ve seen earlier, as well as specialized operations such as rank, row_number, lead, and lag. The Vega-Lite documentation lists all valid window operations.\nOne use case for a window transform is to calculate top-k lists. Let’s plot the top 20 directors in terms of total worldwide gross.\nWe first use a filter transform to remove records for which we don’t know the director. Otherwise, the director null would dominate the list! We then apply an aggregate to sum up the worldwide gross for all films, grouped by director. At this point we could plot a sorted bar chart, but we’d end up with hundreds and hundreds of directors. How can we limit the display to the top 20?\nThe window transform allows us to determine the top directors by calculating their rank order. Within our window transform definition we can sort by gross and use the rank operation to calculate rank scores according to that sort order. We can then add a subsequent filter transform to limit the data to only records with a rank value less than or equal to 20.\n\nalt.Chart(movies_url).mark_bar().transform_filter(\n    'datum.Director != null'\n).transform_aggregate(\n    Gross='sum(Worldwide_Gross)',\n    groupby=['Director']\n).transform_window(\n    Rank='rank()',\n    sort=[alt.SortField('Gross', order='descending')]\n).transform_filter(\n    'datum.Rank &lt; 20'\n).encode(\n    alt.X('Gross:Q'),\n    alt.Y('Director:N', sort=alt.EncodingSortField(\n        op='max', field='Gross', order='descending'\n    ))\n)\n\n\n\n\n\n\nWe can see that Steven Spielberg has been quite successful in his career! However, showing sums might favor directors who have had longer careers, and so have made more movies and thus more money. What happens if we change the choice of aggregate operation? Who is the most successful director in terms of average or median gross per film? Modify the aggregate transform above!\nEarlier in this notebook we looked at histograms, which approximate the probability density function of a set of values. A complementary approach is to look at the cumulative distribution. For example, think of a histogram in which each bin includes not only its own count but also the counts from all previous bins — the result is a running total, with the last bin containing the total number of records. A cumulative chart directly shows us, for a given reference value, how many data values are less than or equal to that reference.\nAs a concrete example, let’s look at the cumulative distribution of films by running time (in minutes). Only a subset of records actually include running time information, so we first filter down to the subset of films for which we have running times. Next, we apply an aggregate to count the number of films per duration (implicitly using “bins” of 1 minute each). We then use a window transform to compute a running total of counts across bins, sorted by increasing running time.\n\nalt.Chart(movies_url).mark_line(interpolate='step-before').transform_filter(\n    'datum.Running_Time_min != null'\n).transform_aggregate(\n    groupby=['Running_Time_min'],\n    Count='count()',\n).transform_window(\n    Cumulative_Sum='sum(Count)',\n    sort=[alt.SortField('Running_Time_min', order='ascending')]\n).encode(\n    alt.X('Running_Time_min:Q', axis=alt.Axis(title='Duration (min)')),\n    alt.Y('Cumulative_Sum:Q', axis=alt.Axis(title='Cumulative Count of Films'))\n)\n\n\n\n\n\n\nLet’s examine the cumulative distribution of film lengths. We can see that films under 110 minutes make up about half of all the films for which we have running times. We see a steady accumulation of films between 90 minutes and 2 hours, after which the distribution begins to taper off. Though rare, the dataset does contain multiple films more than 3 hours long!"
  },
  {
    "objectID": "learn/altair/altair_data_transformation.html#summary",
    "href": "learn/altair/altair_data_transformation.html#summary",
    "title": "Data Transformation",
    "section": "Summary",
    "text": "Summary\nWe’ve only scratched the surface of what data transformations can do! For more details, including all the available transformations and their parameters, see the Altair data transformation documentation.\nSometimes you will need to perform significant data transformation to prepare your data prior to using visualization tools. To engage in data wrangling right here in Python, you can use the Pandas library."
  },
  {
    "objectID": "learn/altair/altair_marks_encoding.html",
    "href": "learn/altair/altair_marks_encoding.html",
    "title": "Data Types, Graphical Marks, and Visual Encoding Channels",
    "section": "",
    "text": "A visualization represents data using a collection of graphical marks (bars, lines, points, etc.). The attributes of a mark — such as its position, shape, size, or color — serve as channels through which we can encode underlying data values.\nWith a basic framework of data types, marks, and encoding channels, we can concisely create a wide variety of visualizations. In this notebook, we explore each of these elements and show how to use them to create custom statistical graphics.\nThis notebook is part of the data visualization curriculum.\nimport pandas as pd\nimport altair as alt"
  },
  {
    "objectID": "learn/altair/altair_marks_encoding.html#global-development-data",
    "href": "learn/altair/altair_marks_encoding.html#global-development-data",
    "title": "Data Types, Graphical Marks, and Visual Encoding Channels",
    "section": "Global Development Data",
    "text": "Global Development Data\nWe will be visualizing global health and population data for a number of countries, over the time period of 1955 to 2005. The data was collected by the Gapminder Foundation and shared in Hans Rosling’s popular TED talk. If you haven’t seen the talk, we encourage you to watch it first!\nLet’s first load the dataset from the vega-datasets collection into a Pandas data frame.\n\nfrom vega_datasets import data as vega_data\ndata = vega_data.gapminder()\n\nHow big is the data?\n\ndata.shape\n\n(693, 6)\n\n\n693 rows and 6 columns! Let’s take a peek at the data content:\n\ndata.head(5)\n\n\n\n\n\n\n\n\nyear\ncountry\ncluster\npop\nlife_expect\nfertility\n\n\n\n\n0\n1955\nAfghanistan\n0\n8891209\n30.332\n7.7\n\n\n1\n1960\nAfghanistan\n0\n9829450\n31.997\n7.7\n\n\n2\n1965\nAfghanistan\n0\n10997885\n34.020\n7.7\n\n\n3\n1970\nAfghanistan\n0\n12430623\n36.088\n7.7\n\n\n4\n1975\nAfghanistan\n0\n14132019\n38.438\n7.7\n\n\n\n\n\n\n\nFor each country and year (in 5-year intervals), we have measures of fertility in terms of the number of children per woman (fertility), life expectancy in years (life_expect), and total population (pop).\nWe also see a cluster field with an integer code. What might this represent? We’ll try and solve this mystery as we visualize the data!\nLet’s also create a smaller data frame, filtered down to values for the year 2000 only:\n\ndata2000 = data.loc[data['year'] == 2000]\n\n\ndata2000.head(5)\n\n\n\n\n\n\n\n\nyear\ncountry\ncluster\npop\nlife_expect\nfertility\n\n\n\n\n9\n2000\nAfghanistan\n0\n23898198\n42.129\n7.4792\n\n\n20\n2000\nArgentina\n3\n37497728\n74.340\n2.3500\n\n\n31\n2000\nAruba\n3\n69539\n73.451\n2.1240\n\n\n42\n2000\nAustralia\n4\n19164620\n80.370\n1.7560\n\n\n53\n2000\nAustria\n1\n8113413\n78.980\n1.3820"
  },
  {
    "objectID": "learn/altair/altair_marks_encoding.html#data-types",
    "href": "learn/altair/altair_marks_encoding.html#data-types",
    "title": "Data Types, Graphical Marks, and Visual Encoding Channels",
    "section": "Data Types",
    "text": "Data Types\nThe first ingredient in effective visualization is the input data. Data values can represent different forms of measurement. What kinds of comparisons do those measurements support? And what kinds of visual encodings then support those comparisons?\nWe will start by looking at the basic data types that Altair uses to inform visual encoding choices. These data types determine the kinds of comparisons we can make, and thereby guide our visualization design decisions.\n\nNominal (N)\nNominal data (also called categorical data) consist of category names.\nWith nominal data we can compare the equality of values: is value A the same or different than value B? (A = B), supporting statements like “A is equal to B” or “A is not equal to B”. In the dataset above, the country field is nominal.\nWhen visualizing nominal data we should readily be able to see if values are the same or different: position, color hue (blue, red, green, etc.), and shape can help. However, using a size channel to encode nominal data might mislead us, suggesting rank-order or magnitude differences among values that do not exist!\n\n\nOrdinal (O)\nOrdinal data consist of values that have a specific ordering.\nWith ordinal data we can compare the rank-ordering of values: does value A come before or after value B? (A &lt; B), supporting statements like “A is less than B” or “A is greater than B”. In the dataset above, we can treat the year field as ordinal.\nWhen visualizing ordinal data, we should perceive a sense of rank-order. Position, size, or color value (brightness) might be appropriate, where as color hue (which is not perceptually ordered) would be less appropriate.\n\n\nQuantitative (Q)\nWith quantitative data we can measure numerical differences among values. There are multiple sub-types of quantitative data:\nFor interval data we can measure the distance (interval) between points: what is the distance to value A from value B? (A - B), supporting statements such as “A is 12 units away from B”.\nFor ratio data the zero-point is meaningful and so we can also measure proportions or scale factors: value A is what proportion of value B? (A / B), supporting statements such as “A is 10% of B” or “B is 7 times larger than A”.\nIn the dataset above, year is a quantitative interval field (the value of year “zero” is subjective), whereas fertility and life_expect are quantitative ratio fields (zero is meaningful for calculating proportions). Vega-Lite represents quantitative data, but does not make a distinction between interval and ratio types.\nQuantitative values can be visualized using position, size, or color value, among other channels. An axis with a zero baseline is essential for proportional comparisons of ratio values, but can be safely omitted for interval comparisons.\n\n\nTemporal (T)\nTemporal values measure time points or intervals. This type is a special case of quantitative values (timestamps) with rich semantics and conventions (i.e., the Gregorian calendar). The temporal type in Vega-Lite supports reasoning about time units (year, month, day, hour, etc.), and provides methods for requesting specific time intervals.\nExample temporal values include date strings such as “2019-01-04” and “Jan 04 2019”, as well as standardized date-times such as the ISO date-time format: “2019-01-04T17:50:35.643Z”.\nThere are no temporal values in our global development dataset above, as the year field is simply encoded as an integer. For more details about using temporal data in Altair, see the Times and Dates documentation.\n\n\nSummary\nThese data types are not mutually exclusive, but rather form a hierarchy: ordinal data support nominal (equality) comparisons, while quantitative data support ordinal (rank-order) comparisons.\nMoreover, these data types do not provide a fixed categorization. Just because a data field is represented using a number doesn’t mean we have to treat it as a quantitative type! For example, we might interpret a set of ages (10 years old, 20 years old, etc) as nominal (underage or overage), ordinal (grouped by year), or quantitative (calculate average age).\nNow let’s examine how to visually encode these data types!"
  },
  {
    "objectID": "learn/altair/altair_marks_encoding.html#encoding-channels",
    "href": "learn/altair/altair_marks_encoding.html#encoding-channels",
    "title": "Data Types, Graphical Marks, and Visual Encoding Channels",
    "section": "Encoding Channels",
    "text": "Encoding Channels\nAt the heart of Altair is the use of encodings that bind data fields (with a given data type) to available encoding channels of a chosen mark type. In this notebook we’ll examine the following encoding channels:\n\nx: Horizontal (x-axis) position of the mark.\ny: Vertical (y-axis) position of the mark.\nsize: Size of the mark. May correspond to area or length, depending on the mark type.\ncolor: Mark color, specified as a legal CSS color.\nopacity: Mark opacity, ranging from 0 (fully transparent) to 1 (fully opaque).\nshape: Plotting symbol shape for point marks.\ntooltip: Tooltip text to display upon mouse hover over the mark.\norder: Mark ordering, determines line/area point order and drawing order.\ncolumn: Facet the data into horizontally-aligned subplots.\nrow: Facet the data into vertically-aligned subplots.\n\nFor a complete list of available channels, see the Altair encoding documentation.\n\nX\nThe x encoding channel sets a mark’s horizontal position (x-coordinate). In addition, default choices of axis and title are made automatically. In the chart below, the choice of a quantitative data type results in a continuous linear axis scale:\n\nalt.Chart(data2000).mark_point().encode(\n    alt.X('fertility:Q')\n)\n\n\n\n\n\n\n\n\nY\nThe y encoding channel sets a mark’s vertical position (y-coordinate). Here we’ve added the cluster field using an ordinal (O) data type. The result is a discrete axis that includes a sized band, with a default step size, for each unique value:\n\nalt.Chart(data2000).mark_point().encode(\n    alt.X('fertility:Q'),\n    alt.Y('cluster:O')\n)\n\n\n\n\n\n\nWhat happens to the chart above if you swap the O and Q field types?\nIf we instead add the life_expect field as a quantitative (Q) variable, the result is a scatter plot with linear scales for both axes:\n\nalt.Chart(data2000).mark_point().encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q')\n)\n\n\n\n\n\n\nBy default, axes for linear quantitative scales include zero to ensure a proper baseline for comparing ratio-valued data. In some cases, however, a zero baseline may be meaningless or you may want to focus on interval comparisons. To disable automatic inclusion of zero, configure the scale mapping using the encoding scale attribute:\n\nalt.Chart(data2000).mark_point().encode(\n    alt.X('fertility:Q', scale=alt.Scale(zero=False)),\n    alt.Y('life_expect:Q', scale=alt.Scale(zero=False))\n)\n\n\n\n\n\n\nNow the axis scales no longer include zero by default. Some padding still remains, as the axis domain end points are automatically snapped to nice numbers like multiples of 5 or 10.\nWhat happens if you also add nice=False to the scale attribute above?\n\n\nSize\nThe size encoding channel sets a mark’s size or extent. The meaning of the channel can vary based on the mark type. For point marks, the size channel maps to the pixel area of the plotting symbol, such that the diameter of the point matches the square root of the size value.\nLet’s augment our scatter plot by encoding population (pop) on the size channel. As a result, the chart now also includes a legend for interpreting the size values.\n\nalt.Chart(data2000).mark_point().encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q')\n)\n\n\n\n\n\n\nIn some cases we might be unsatisfied with the default size range. To provide a customized span of sizes, set the range parameter of the scale attribute to an array indicating the smallest and largest sizes. Here we update the size encoding to range from 0 pixels (for zero values) to 1,000 pixels (for the maximum value in the scale domain):\n\nalt.Chart(data2000).mark_point().encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000]))\n)\n\n\n\n\n\n\n\n\nColor and Opacity\nThe color encoding channel sets a mark’s color. The style of color encoding is highly dependent on the data type: nominal data will default to a multi-hued qualitative color scheme, whereas ordinal and quantitative data will use perceptually ordered color gradients.\nHere, we encode the cluster field using the color channel and a nominal (N) data type, resulting in a distinct hue for each cluster value. Can you start to guess what the cluster field might indicate?\n\nalt.Chart(data2000).mark_point().encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000])),\n    alt.Color('cluster:N')\n)\n\n\n\n\n\n\nIf we prefer filled shapes, we can can pass a filled=True parameter to the mark_point method:\n\nalt.Chart(data2000).mark_point(filled=True).encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000])),\n    alt.Color('cluster:N')\n)\n\n\n\n\n\n\nBy default, Altair uses a bit of transparency to help combat over-plotting. We are free to further adjust the opacity, either by passing a default value to the mark_* method, or using a dedicated encoding channel.\nHere we demonstrate how to provide a constant value to an encoding channel instead of binding a data field:\n\nalt.Chart(data2000).mark_point(filled=True).encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000])),\n    alt.Color('cluster:N'),\n    alt.OpacityValue(0.5)\n)\n\n\n\n\n\n\n\n\nShape\nThe shape encoding channel sets the geometric shape used by point marks. Unlike the other channels we have seen so far, the shape channel can not be used by other mark types. The shape encoding channel should only be used with nominal data, as perceptual rank-order and magnitude comparisons are not supported.\nLet’s encode the cluster field using shape as well as color. Using multiple channels for the same underlying data field is known as a redundant encoding. The resulting chart combines both color and shape information into a single symbol legend:\n\nalt.Chart(data2000).mark_point(filled=True).encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000])),\n    alt.Color('cluster:N'),\n    alt.OpacityValue(0.5),\n    alt.Shape('cluster:N')\n)\n\n\n\n\n\n\n\n\nTooltips & Ordering\nBy this point, you might feel a bit frustrated: we’ve built up a chart, but we still don’t know what countries the visualized points correspond to! Let’s add interactive tooltips to enable exploration.\nThe tooltip encoding channel determines tooltip text to show when a user moves the mouse cursor over a mark. Let’s add a tooltip encoding for the country field, then investigate which countries are being represented.\n\nalt.Chart(data2000).mark_point(filled=True).encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000])),\n    alt.Color('cluster:N'),\n    alt.OpacityValue(0.5),\n    alt.Tooltip('country')\n)\n\n\n\n\n\n\nAs you mouse around you may notice that you can not select some of the points. For example, the largest dark blue circle corresponds to India, which is drawn on top of a country with a smaller population, preventing the mouse from hovering over that country. To fix this problem, we can use the order encoding channel.\nThe order encoding channel determines the order of data points, affecting both the order in which they are drawn and, for line and area marks, the order in which they are connected to one another.\nLet’s order the values in descending rank order by the population (pop), ensuring that smaller circles are drawn later than larger circles:\n\nalt.Chart(data2000).mark_point(filled=True).encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000])),\n    alt.Color('cluster:N'),\n    alt.OpacityValue(0.5),\n    alt.Tooltip('country:N'),\n    alt.Order('pop:Q', sort='descending')\n)\n\n\n\n\n\n\nNow we can identify the smaller country being obscured by India: it’s Bangladesh!\nWe can also now figure out what the cluster field represents. Mouse over the various colored points to formulate your own explanation.\nAt this point we’ve added tooltips that show only a single property of the underlying data record. To show multiple values, we can provide the tooltip channel an array of encodings, one for each field we want to include:\n\nalt.Chart(data2000).mark_point(filled=True).encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000])),\n    alt.Color('cluster:N'),\n    alt.OpacityValue(0.5),\n    alt.Order('pop:Q', sort='descending'),\n    tooltip = [\n        alt.Tooltip('country:N'),\n        alt.Tooltip('fertility:Q'),\n        alt.Tooltip('life_expect:Q')\n    ]   \n)\n\n\n\n\n\n\nNow we can see multiple data fields upon mouse over!\n\n\nColumn and Row Facets\nSpatial position is one of the most powerful and flexible channels for visual encoding, but what can we do if we already have assigned fields to the x and y channels? One valuable technique is to create a trellis plot, consisting of sub-plots that show a subset of the data. A trellis plot is one example of the more general technique of presenting data using small multiples of views.\nThe column and row encoding channels generate either a horizontal (columns) or vertical (rows) set of sub-plots, in which the data is partitioned according to the provided data field.\nHere is a trellis plot that divides the data into one column per `cluster` value:\n\nalt.Chart(data2000).mark_point(filled=True).encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000])),\n    alt.Color('cluster:N'),\n    alt.OpacityValue(0.5),\n    alt.Tooltip('country:N'),\n    alt.Order('pop:Q', sort='descending'),\n    alt.Column('cluster:N')\n)\n\n\n\n\n\n\nThe plot above does not fit on screen, making it difficult to compare all the sub-plots to each other! We can set the default width and height properties to create a smaller set of multiples. Also, as the column headers already label the cluster values, let’s remove our color legend by setting it to None. To make better use of space we can also orient our size legend to the 'bottom' of the chart.\n\nalt.Chart(data2000).mark_point(filled=True).encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000]),\n             legend=alt.Legend(orient='bottom', titleOrient='left')),\n    alt.Color('cluster:N', legend=None),\n    alt.OpacityValue(0.5),\n    alt.Tooltip('country:N'),\n    alt.Order('pop:Q', sort='descending'),\n    alt.Column('cluster:N')\n).properties(width=135, height=135)\n\n\n\n\n\n\nUnderneath the hood, the column and row encodings are translated into a new specification that uses the facet view composition operator. We will re-visit faceting in greater depth later on!\nIn the meantime, can you rewrite the chart above to facet into rows instead of columns?\n\n\nA Peek Ahead: Interactive Filtering\nIn later modules, we’ll dive into interaction techniques for data exploration. Here is a sneak peak: binding a range slider to the year field to enable interactive scrubbing through each year of data. Don’t worry if the code below is a bit confusing at this point, as we will cover interaction in detail later.\nDrag the slider back and forth to see how the data values change over time!\n\nselect_year = alt.selection_single(\n    name='select', fields=['year'], init={'year': 1955},\n    bind=alt.binding_range(min=1955, max=2005, step=5)\n)\n\nalt.Chart(data).mark_point(filled=True).encode(\n    alt.X('fertility:Q', scale=alt.Scale(domain=[0,9])),\n    alt.Y('life_expect:Q', scale=alt.Scale(domain=[0,90])),\n    alt.Size('pop:Q', scale=alt.Scale(domain=[0, 1200000000], range=[0,1000])),\n    alt.Color('cluster:N', legend=None),\n    alt.OpacityValue(0.5),\n    alt.Tooltip('country:N'),\n    alt.Order('pop:Q', sort='descending')\n).add_selection(select_year).transform_filter(select_year)"
  },
  {
    "objectID": "learn/altair/altair_marks_encoding.html#graphical-marks",
    "href": "learn/altair/altair_marks_encoding.html#graphical-marks",
    "title": "Data Types, Graphical Marks, and Visual Encoding Channels",
    "section": "Graphical Marks",
    "text": "Graphical Marks\nOur exploration of encoding channels above exclusively uses point marks to visualize the data. However, the point mark type is only one of the many geometric shapes that can be used to visually represent data. Altair includes a number of built-in mark types, including:\n\nmark_area() - Filled areas defined by a top-line and a baseline.\nmark_bar() - Rectangular bars.\nmark_circle() - Scatter plot points as filled circles.\nmark_line() - Connected line segments.\nmark_point() - Scatter plot points with configurable shapes.\nmark_rect() - Filled rectangles, useful for heatmaps.\nmark_rule() - Vertical or horizontal lines spanning the axis.\nmark_square() - Scatter plot points as filled squares.\nmark_text() - Scatter plot points represented by text.\nmark_tick() - Vertical or horizontal tick marks.\n\nFor a complete list, and links to examples, see the Altair marks documentation. Next, we will step through a number of the most commonly used mark types for statistical graphics.\n\nPoint Marks\nThe point mark type conveys specific points, as in scatter plots and dot plots. In addition to x and y encoding channels (to specify 2D point positions), point marks can use color, size, and shape encodings to convey additional data fields.\nBelow is a dot plot of fertility, with the cluster field redundantly encoded using both the y and shape channels.\n\nalt.Chart(data2000).mark_point().encode(\n    alt.X('fertility:Q'),\n    alt.Y('cluster:N'),\n    alt.Shape('cluster:N')\n)\n\n\n\n\n\n\nIn addition to encoding channels, marks can be stylized by providing values to the mark_*() methods.\nFor example: point marks are drawn with stroked outlines by default, but can be specified to use filled shapes instead. Similarly, you can set a default size to set the total pixel area of the point mark.\n\nalt.Chart(data2000).mark_point(filled=True, size=100).encode(\n    alt.X('fertility:Q'),\n    alt.Y('cluster:N'),\n    alt.Shape('cluster:N')\n)\n\n\n\n\n\n\n\n\nCircle Marks\nThe circle mark type is a convenient shorthand for point marks drawn as filled circles.\n\nalt.Chart(data2000).mark_circle(size=100).encode(\n    alt.X('fertility:Q'),\n    alt.Y('cluster:N'),\n    alt.Shape('cluster:N')\n)\n\n\n\n\n\n\n\n\nSquare Marks\nThe square mark type is a convenient shorthand for point marks drawn as filled squares.\n\nalt.Chart(data2000).mark_square(size=100).encode(\n    alt.X('fertility:Q'),\n    alt.Y('cluster:N'),\n    alt.Shape('cluster:N')\n)\n\n\n\n\n\n\n\n\nTick Marks\nThe tick mark type conveys a data point using a short line segment or “tick”. These are particularly useful for comparing values along a single dimension with minimal overlap. A dot plot drawn with tick marks is sometimes referred to as a strip plot.\n\nalt.Chart(data2000).mark_tick().encode(\n    alt.X('fertility:Q'),\n    alt.Y('cluster:N'),\n    alt.Shape('cluster:N')\n)\n\n\n\n\n\n\n\n\nBar Marks\nThe `bar` mark type draws a rectangle with a position, width, and height.\nThe plot below is a simple bar chart of the population (`pop`) of each country.\n\nalt.Chart(data2000).mark_bar().encode(\n    alt.X('country:N'),\n    alt.Y('pop:Q')\n)\n\n\n\n\n\n\nThe bar width is set to a default size. We will discuss how to adjust the bar width later in this notebook. (A subsequent notebook will take a closer look at configuring axes, scales, and legends.)\nBars can also be stacked. Let’s change the x encoding to use the cluster field, and encode country using the color channel. We’ll also disable the legend (which would be very long with colors for all countries!) and use tooltips for the country name.\n\nalt.Chart(data2000).mark_bar().encode(\n    alt.X('cluster:N'),\n    alt.Y('pop:Q'),\n    alt.Color('country:N', legend=None),\n    alt.Tooltip('country:N')\n)\n\n\n\n\n\n\nIn the chart above, the use of the color encoding channel causes Altair / Vega-Lite to automatically stack the bar marks. Otherwise, bars would be drawn on top of each other! Try adding the parameter stack=None to the y encoding channel to see what happens if we don’t apply stacking…\nThe examples above create bar charts from a zero-baseline, and the y channel only encodes the non-zero value (or height) of the bar. However, the bar mark also allows you to specify starting and ending points to convey ranges.\nThe chart below uses the x (starting point) and x2 (ending point) channels to show the range of life expectancies within each regional cluster. Below we use the min and max aggregation functions to determine the end points of the range; we will discuss aggregation in greater detail in the next notebook!\nAlternatively, you can use x and width to provide a starting point plus offset, such that x2 = x + width.\n\nalt.Chart(data2000).mark_bar().encode(\n    alt.X('min(life_expect):Q'),\n    alt.X2('max(life_expect):Q'),\n    alt.Y('cluster:N')\n)\n\n\n\n\n\n\n\n\nLine Marks\nThe line mark type connects plotted points with line segments, for example so that a line’s slope conveys information about the rate of change.\nLet’s plot a line chart of fertility per country over the years, using the full, unfiltered global development data frame. We’ll again hide the legend and use tooltips instead.\n\nalt.Chart(data).mark_line().encode(\n    alt.X('year:O'),\n    alt.Y('fertility:Q'),\n    alt.Color('country:N', legend=None),\n    alt.Tooltip('country:N')\n).properties(\n    width=400\n)\n\n\n\n\n\n\nWe can see interesting variations per country, but overall trends for lower numbers of children per family over time. Also note that we set a custom width of 400 pixels. Try changing (or removing) the widths and see what happens!\nLet’s change some of the default mark parameters to customize the plot. We can set the strokeWidth to determine the thickness of the lines and the opacity to add some transparency. By default, the line mark uses straight line segments to connect data points. In some cases we might want to smooth the lines. We can adjust the interpolation used to connect data points by setting the interpolate mark parameter. Let’s use 'monotone' interpolation to provide smooth lines that are also guaranteed not to inadvertently generate “false” minimum or maximum values as a result of the interpolation.\n\nalt.Chart(data).mark_line(\n    strokeWidth=3,\n    opacity=0.5,\n    interpolate='monotone'\n).encode(\n    alt.X('year:O'),\n    alt.Y('fertility:Q'),\n    alt.Color('country:N', legend=None),\n    alt.Tooltip('country:N')\n).properties(\n    width=400\n)\n\n\n\n\n\n\nThe line mark can also be used to create slope graphs, charts that highlight the change in value between two comparison points using line slopes.\nBelow let’s create a slope graph comparing the populations of each country at minimum and maximum years in our full dataset: 1955 and 2005. We first create a new Pandas data frame filtered to those years, then use Altair to create the slope graph.\nBy default, Altair places the years close together. To better space out the years along the x-axis, we can indicate the size (in pixels) of discrete steps along the width of our chart as indicated by the comment below. Try adjusting the width step value below and see how the chart changes in response.\n\ndataTime = data.loc[(data['year'] == 1955) | (data['year'] == 2005)]\n\nalt.Chart(dataTime).mark_line(opacity=0.5).encode(\n    alt.X('year:O'),\n    alt.Y('pop:Q'),\n    alt.Color('country:N', legend=None),\n    alt.Tooltip('country:N')\n).properties(\n    width={\"step\": 50} # adjust the step parameter\n)\n\n\n\n\n\n\n\n\nArea Marks\nThe area mark type combines aspects of line and bar marks: it visualizes connections (slopes) among data points, but also shows a filled region, with one edge defaulting to a zero-valued baseline.\nThe chart below is an area chart of population over time for just the United States:\n\ndataUS = data.loc[data['country'] == 'United States']\n\nalt.Chart(dataUS).mark_area().encode(\n    alt.X('year:O'),\n    alt.Y('fertility:Q')\n)\n\n\n\n\n\n\nSimilar to line marks, area marks support an interpolate parameter.\n\nalt.Chart(dataUS).mark_area(interpolate='monotone').encode(\n    alt.X('year:O'),\n    alt.Y('fertility:Q')\n)\n\n\n\n\n\n\nSimilar to bar marks, area marks also support stacking. Here we create a new data frame with data for the three North American countries, then plot them using an area mark and a color encoding channel to stack by country.\n\ndataNA = data.loc[\n    (data['country'] == 'United States') |\n    (data['country'] == 'Canada') |\n    (data['country'] == 'Mexico')\n]\n\nalt.Chart(dataNA).mark_area().encode(\n    alt.X('year:O'),\n    alt.Y('pop:Q'),\n    alt.Color('country:N')\n)\n\n\n\n\n\n\nBy default, stacking is performed relative to a zero baseline. However, other stack options are available:\n\ncenter - to stack relative to a baseline in the center of the chart, creating a streamgraph visualization, and\nnormalize - to normalize the summed data at each stacking point to 100%, enabling percentage comparisons.\n\nBelow we adapt the chart by setting the y encoding stack attribute to center. What happens if you instead set it normalize?\n\nalt.Chart(dataNA).mark_area().encode(\n    alt.X('year:O'),\n    alt.Y('pop:Q', stack='center'),\n    alt.Color('country:N')\n)\n\n\n\n\n\n\nTo disable stacking altogether, set the stack attribute to None. We can also add opacity as a default mark parameter to ensure we see the overlapping areas!\n\nalt.Chart(dataNA).mark_area(opacity=0.5).encode(\n    alt.X('year:O'),\n    alt.Y('pop:Q', stack=None),\n    alt.Color('country:N')\n)\n\n\n\n\n\n\nThe area mark type also supports data-driven baselines, with both the upper and lower series determined by data fields. As with bar marks, we can use the x and x2 (or y and y2) channels to provide end points for the area mark.\nThe chart below visualizes the range of minimum and maximum fertility, per year, for North American countries:\n\nalt.Chart(dataNA).mark_area().encode(\n    alt.X('year:O'),\n    alt.Y('min(fertility):Q'),\n    alt.Y2('max(fertility):Q')\n).properties(\n    width={\"step\": 40}\n)\n\n\n\n\n\n\nWe can see a larger range of values in 1995, from just under 4 to just under 7. By 2005, both the overall fertility values and the variability have declined, centered around 2 children per familty.\nAll the area mark examples above use a vertically oriented area. However, Altair and Vega-Lite support horizontal areas as well. Let’s transpose the chart above, simply by swapping the x and y channels.\n\nalt.Chart(dataNA).mark_area().encode(\n    alt.Y('year:O'),\n    alt.X('min(fertility):Q'),\n    alt.X2('max(fertility):Q')\n).properties(\n    width={\"step\": 40}\n)"
  },
  {
    "objectID": "learn/altair/altair_marks_encoding.html#summary-1",
    "href": "learn/altair/altair_marks_encoding.html#summary-1",
    "title": "Data Types, Graphical Marks, and Visual Encoding Channels",
    "section": "Summary",
    "text": "Summary\nWe’ve completed our tour of data types, encoding channels, and graphical marks! You should now be well-equipped to further explore the space of encodings, mark types, and mark parameters. For a comprehensive reference – including features we’ve skipped over here! – see the Altair marks and encoding documentation.\nIn the next module, we will look at the use of data transformations to create charts that summarize data or visualize new derived fields. In a later module, we’ll examine how to further customize your charts by modifying scales, axes, and legends.\nInterested in learning more about visual encoding?\n\nBertin’s taxonomy of visual encodings from Sémiologie Graphique, as adapted by Mike Bostock.\n\nThe systematic study of marks, visual encodings, and backing data types was initiated by Jacques Bertin in his pioneering 1967 work Sémiologie Graphique (The Semiology of Graphics). The image above illustrates position, size, value (brightness), texture, color (hue), orientation, and shape channels, alongside Bertin’s recommendations for the data types they support.\nThe framework of data types, marks, and channels also guides automated visualization design tools, starting with Mackinlay’s APT (A Presentation Tool) in 1986 and continuing in more recent systems such as Voyager and Draco.\nThe identification of nominal, ordinal, interval, and ratio types dates at least as far back as S. S. Steven’s 1947 article On the theory of scales of measurement."
  },
  {
    "objectID": "learn/altair/altair_scales_axes_legends.html",
    "href": "learn/altair/altair_scales_axes_legends.html",
    "title": "Scales, Axes, and Legends",
    "section": "",
    "text": "Visual encoding – mapping data to visual variables such as position, size, shape, or color – is the beating heart of data visualization. The workhorse that actually performs this mapping is the scale: a function that takes a data value as input (the scale domain) and returns a visual value, such as a pixel position or RGB color, as output (the scale range). Of course, a visualization is useless if no one can figure out what it conveys! In addition to graphical marks, a chart needs reference elements, or guides, that allow readers to decode the graphic. Guides such as axes (which visualize scales with spatial ranges) and legends (which visualize scales with color, size, or shape ranges), are the unsung heroes of effective data visualization!\nIn this notebook, we will explore the options Altair provides to support customized designs of scale mappings, axes, and legends, using a running example about the effectiveness of antibiotic drugs.\nThis notebook is part of the data visualization curriculum.\nimport pandas as pd\nimport altair as alt"
  },
  {
    "objectID": "learn/altair/altair_scales_axes_legends.html#antibiotics-data",
    "href": "learn/altair/altair_scales_axes_legends.html#antibiotics-data",
    "title": "Scales, Axes, and Legends",
    "section": "Antibiotics Data",
    "text": "Antibiotics Data\nAfter World War II, antibiotics were considered “wonder drugs”, as they were an easy remedy for what had been intractable ailments. To learn which drug worked most effectively for which bacterial infection, performance of the three most popular antibiotics on 16 bacteria were gathered.\nWe will be using an antibiotics dataset from the vega-datasets collection. In the examples below, we will pass the URL directly to Altair:\n\nantibiotics = 'https://cdn.jsdelivr.net/npm/vega-datasets@1/data/burtin.json'\n\nWe can first load the data with Pandas to view the dataset in its entirety and get acquainted with the available fields:\n\npd.read_json(antibiotics)\n\n\n\n\n\n\n\n\nBacteria\nPenicillin\nStreptomycin\nNeomycin\nGram_Staining\nGenus\n\n\n\n\n0\nAerobacter aerogenes\n870.000\n1.00\n1.600\nnegative\nother\n\n\n1\nBacillus anthracis\n0.001\n0.01\n0.007\npositive\nother\n\n\n2\nBrucella abortus\n1.000\n2.00\n0.020\nnegative\nother\n\n\n3\nDiplococcus pneumoniae\n0.005\n11.00\n10.000\npositive\nother\n\n\n4\nEscherichia coli\n100.000\n0.40\n0.100\nnegative\nother\n\n\n5\nKlebsiella pneumoniae\n850.000\n1.20\n1.000\nnegative\nother\n\n\n6\nMycobacterium tuberculosis\n800.000\n5.00\n2.000\nnegative\nother\n\n\n7\nProteus vulgaris\n3.000\n0.10\n0.100\nnegative\nother\n\n\n8\nPseudomonas aeruginosa\n850.000\n2.00\n0.400\nnegative\nother\n\n\n9\nSalmonella (Eberthella) typhosa\n1.000\n0.40\n0.008\nnegative\nSalmonella\n\n\n10\nSalmonella schottmuelleri\n10.000\n0.80\n0.090\nnegative\nSalmonella\n\n\n11\nStaphylococcus albus\n0.007\n0.10\n0.001\npositive\nStaphylococcus\n\n\n12\nStaphylococcus aureus\n0.030\n0.03\n0.001\npositive\nStaphylococcus\n\n\n13\nStreptococcus fecalis\n1.000\n1.00\n0.100\npositive\nStreptococcus\n\n\n14\nStreptococcus hemolyticus\n0.001\n14.00\n10.000\npositive\nStreptococcus\n\n\n15\nStreptococcus viridans\n0.005\n10.00\n40.000\npositive\nStreptococcus\n\n\n\n\n\n\n\nThe numeric values in the table indicate the minimum inhibitory concentration (MIC), a measure of the effectiveness of the antibiotic, which represents the concentration of antibiotic (in micrograms per milliliter) required to prevent growth in vitro. The reaction of the bacteria to a procedure called Gram staining is described by the nominal field Gram_Staining. Bacteria that turn dark blue or violet are Gram-positive. Otherwise, they are Gram-negative.\nAs we examine different visualizations of this dataset, ask yourself: What might we learn about the relative effectiveness of the antibiotics? What might we learn about the bacterial species based on their antibiotic response?"
  },
  {
    "objectID": "learn/altair/altair_scales_axes_legends.html#configuring-scales-and-axes",
    "href": "learn/altair/altair_scales_axes_legends.html#configuring-scales-and-axes",
    "title": "Scales, Axes, and Legends",
    "section": "Configuring Scales and Axes",
    "text": "Configuring Scales and Axes\n\nPlotting Antibiotic Resistance: Adjusting the Scale Type\nLet’s start by looking at a simple dot plot of the MIC for Neomycin.\n\nalt.Chart(antibiotics).mark_circle().encode(\n    alt.X('Neomycin:Q')\n)\n\n\n\n\n\n\nWe can see that the MIC values span orders of magnitude: most points to cluster on the left, with a few large outliers to the right.\nBy default Altair uses a linear mapping between the domain values (MIC) and the range values (pixels). To get a better overview of the data, we can apply a different scale transformation.\nTo change the scale type, we’ll set the scale attribute, using the alt.Scale method and type parameter.\nHere’s the result of using a square root (sqrt) scale type. Distances in the pixel range now correspond to the square root of distances in the data domain.\n\nalt.Chart(antibiotics).mark_circle().encode(\n    alt.X('Neomycin:Q',\n          scale=alt.Scale(type='sqrt'))\n)\n\n\n\n\n\n\nThe points on the left are now better differentiated, but we still see some heavy skew.\nLet’s try using a logarithmic scale (log) instead:\n\nalt.Chart(antibiotics).mark_circle().encode(\n    alt.X('Neomycin:Q',\n          scale=alt.Scale(type='log'))\n)\n\n\n\n\n\n\nNow the data is much more evenly distributed and we can see the very large differences in concentrations required for different bacteria.\nIn a standard linear scale, a visual (pixel) distance of 10 units might correspond to an addition of 10 units in the data domain. A logarithmic transform maps between multiplication and addition, such that log(u) + log(v) = log(u*v). As a result, in a logarithmic scale, a visual distance of 10 units instead corresponds to multiplication by 10 units in the data domain, assuming a base 10 logarithm. The log scale above defaults to using the logarithm base 10, but we can adjust this by providing a base parameter to the scale.\n\n\nStyling an Axis\nLower dosages indicate higher effectiveness. However, some people may expect values that are “better” to be “up and to the right” within a chart. If we want to cater to this convention, we can reverse the axis to encode “effectiveness” as a reversed MIC scale.\nTo do this, we can set the encoding sort property to 'descending':\n\nalt.Chart(antibiotics).mark_circle().encode(\n    alt.X('Neomycin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log'))\n)\n\n\n\n\n\n\nUnfortunately the axis is starting to get a bit confusing: we’re plotting data on a logarithmic scale, in the reverse direction, and without a clear indication of what our units are!\nLet’s add a more informative axis title: we’ll use the title property of the encoding to provide the desired title text:\n\nalt.Chart(antibiotics).mark_circle().encode(\n    alt.X('Neomycin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log'),\n          title='Neomycin MIC (μg/ml, reverse log scale)')\n)\n\n\n\n\n\n\nMuch better!\nBy default, Altair places the x-axis along the bottom of the chart. To change these defaults, we can add an axis attribute with orient='top':\n\nalt.Chart(antibiotics).mark_circle().encode(\n    alt.X('Neomycin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log'),\n          axis=alt.Axis(orient='top'),\n          title='Neomycin MIC (μg/ml, reverse log scale)')\n)\n\n\n\n\n\n\nSimilarly, the y-axis defaults to a 'left' orientation, but can be set to 'right'.\n\n\nComparing Antibiotics: Adjusting Grid Lines, Tick Counts, and Sizing\nHow does neomycin compare to other antibiotics, such as streptomycin and penicillin?\nTo start answering this question, we can create scatter plots, adding a y-axis encoding for another antibiotic that mirrors the design of our x-axis for neomycin.\n\nalt.Chart(antibiotics).mark_circle().encode(\n    alt.X('Neomycin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log'),\n          title='Neomycin MIC (μg/ml, reverse log scale)'),\n    alt.Y('Streptomycin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log'),\n          title='Streptomycin MIC (μg/ml, reverse log scale)')\n)\n\n\n\n\n\n\nWe can see that neomycin and streptomycin appear highly correlated, as the bacterial strains respond similarly to both antibiotics.\nLet’s move on and compare neomycin with penicillin:\n\nalt.Chart(antibiotics).mark_circle().encode(\n    alt.X('Neomycin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log'),\n          title='Neomycin MIC (μg/ml, reverse log scale)'),\n    alt.Y('Penicillin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log'),\n          title='Penicillin MIC (μg/ml, reverse log scale)')\n)\n\n\n\n\n\n\nNow we see a more differentiated response: some bacteria respond well to neomycin but not penicillin, and vice versa!\nWhile this plot is useful, we can make it better. The x and y axes use the same units, but have different extents (the chart width is larger than the height) and different domains (0.001 to 100 for the x-axis, and 0.001 to 1,000 for the y-axis).\nLet’s equalize the axes: we can add explicit width and height settings for the chart, and specify matching domains using the scale domain property.\n\nalt.Chart(antibiotics).mark_circle().encode(\n    alt.X('Neomycin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          title='Neomycin MIC (μg/ml, reverse log scale)'),\n    alt.Y('Penicillin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          title='Penicillin MIC (μg/ml, reverse log scale)')\n).properties(width=250, height=250)\n\n\n\n\n\n\nThe resulting plot is more balanced, and less prone to subtle misinterpretations!\nHowever, the grid lines are now rather dense. If we want to remove grid lines altogether, we can add grid=False to the axis attribute. But what if we instead want to reduce the number of tick marks, for example only including grid lines for each order of magnitude?\nTo change the number of ticks, we can specify a target tickCount property for an Axis object. The tickCount is treated as a suggestion to Altair, to be considered alongside other aspects such as using nice, human-friendly intervals. We may not get exactly the number of tick marks we request, but we should get something close.\n\nalt.Chart(antibiotics).mark_circle().encode(\n    alt.X('Neomycin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          axis=alt.Axis(tickCount=5),\n          title='Neomycin MIC (μg/ml, reverse log scale)'),\n    alt.Y('Penicillin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          axis=alt.Axis(tickCount=5),\n          title='Penicillin MIC (μg/ml, reverse log scale)')\n).properties(width=250, height=250)\n\n\n\n\n\n\nBy setting the tickCount to 5, we have the desired effect.\nOur scatter plot points feel a bit small. Let’s change the default size by setting the size property of the circle mark. This size value is the area of the mark in pixels.\n\nalt.Chart(antibiotics).mark_circle(size=80).encode(\n    alt.X('Neomycin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          axis=alt.Axis(tickCount=5),\n          title='Neomycin MIC (μg/ml, reverse log scale)'),\n    alt.Y('Penicillin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          axis=alt.Axis(tickCount=5),\n          title='Penicillin MIC (μg/ml, reverse log scale)'), \n).properties(width=250, height=250)\n\n\n\n\n\n\nHere we’ve set the circle mark area to 80 pixels. Further adjust the value as you see fit!"
  },
  {
    "objectID": "learn/altair/altair_scales_axes_legends.html#configuring-color-legends",
    "href": "learn/altair/altair_scales_axes_legends.html#configuring-color-legends",
    "title": "Scales, Axes, and Legends",
    "section": "Configuring Color Legends",
    "text": "Configuring Color Legends\n\nColor by Gram Staining\nAbove we saw that neomycin is more effective for some bacteria, while penicillin is more effective for others. But how can we tell which antibiotic to use if we don’t know the specific species of bacteria? Gram staining serves as a diagnostic for discriminating classes of bacteria!\nLet’s encode Gram_Staining on the color channel as a nominal data type:\n\nalt.Chart(antibiotics).mark_circle(size=80).encode(\n    alt.X('Neomycin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          axis=alt.Axis(tickCount=5),\n          title='Neomycin MIC (μg/ml, reverse log scale)'),\n    alt.Y('Penicillin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          axis=alt.Axis(tickCount=5),\n          title='Penicillin MIC (μg/ml, reverse log scale)'),\n    alt.Color('Gram_Staining:N')\n).properties(width=250, height=250)\n\n\n\n\n\n\nWe can see that Gram-positive bacteria seem most susceptible to penicillin, whereas neomycin is more effective for Gram-negative bacteria!\nThe color scheme above was automatically chosen to provide perceptually-distinguishable colors for nominal (equal or not equal) comparisons. However, we might wish to customize the colors used. In this case, Gram staining results in distinctive physical colorings: pink for Gram-negative, purple for Gram-positive.\nLet’s use those colors by specifying an explicit scale mapping from the data domain to the color range:\n\nalt.Chart(antibiotics).mark_circle(size=80).encode(\n    alt.X('Neomycin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          axis=alt.Axis(tickCount=5),\n          title='Neomycin MIC (μg/ml, reverse log scale)'),\n    alt.Y('Penicillin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          axis=alt.Axis(tickCount=5),\n          title='Penicillin MIC (μg/ml, reverse log scale)'),\n    alt.Color('Gram_Staining:N',\n          scale=alt.Scale(domain=['negative', 'positive'], range=['hotpink', 'purple'])\n    )\n).properties(width=250, height=250)\n\n\n\n\n\n\nBy default legends are placed on the right side of the chart. Similar to axes, we can change the legend orientation using the orient parameter:\n\nalt.Chart(antibiotics).mark_circle(size=80).encode(\n    alt.X('Neomycin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          axis=alt.Axis(tickCount=5),\n          title='Neomycin MIC (μg/ml, reverse log scale)'),\n    alt.Y('Penicillin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          axis=alt.Axis(tickCount=5),\n          title='Penicillin MIC (μg/ml, reverse log scale)'),\n    alt.Color('Gram_Staining:N',\n          scale=alt.Scale(domain=['negative', 'positive'], range=['hotpink', 'purple']),\n          legend=alt.Legend(orient='left')\n    )\n).properties(width=250, height=250)\n\n\n\n\n\n\nWe can also remove a legend entirely by specifying legend=None:\n\nalt.Chart(antibiotics).mark_circle(size=80).encode(\n    alt.X('Neomycin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          axis=alt.Axis(tickCount=5),\n          title='Neomycin MIC (μg/ml, reverse log scale)'),\n    alt.Y('Penicillin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          axis=alt.Axis(tickCount=5),\n          title='Penicillin MIC (μg/ml, reverse log scale)'),\n    alt.Color('Gram_Staining:N',\n          scale=alt.Scale(domain=['negative', 'positive'], range=['hotpink', 'purple']),\n          legend=None\n    )\n).properties(width=250, height=250)\n\n\n\n\n\n\n\n\nColor by Species\nSo far we’ve considered the effectiveness of antibiotics. Let’s turn around and ask a different question: what might antibiotic response teach us about the different species of bacteria?\nTo start, let’s encode Bacteria (a nominal data field) using the color channel:\n\nalt.Chart(antibiotics).mark_circle(size=80).encode(\n    alt.X('Neomycin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          axis=alt.Axis(tickCount=5),\n          title='Neomycin MIC (μg/ml, reverse log scale)'),\n    alt.Y('Penicillin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          axis=alt.Axis(tickCount=5),\n          title='Penicillin MIC (μg/ml, reverse log scale)'),\n    alt.Color('Bacteria:N')\n).properties(width=250, height=250)\n\n\n\n\n\n\nThe result is a bit of a mess! There are enough unique bacteria that Altair starts repeating colors from its default 10-color palette for nominal values.\nTo use custom colors, we can update the color encoding scale property. One option is to provide explicit scale domain and range values to indicate the precise color mappings per value, as we did above for Gram staining. Another option is to use an alternative color scheme. Altair includes a variety of built-in color schemes. For a complete list, see the Vega color scheme documentation.\nLet’s try switching to a built-in 20-color scheme, tableau20, and set that using the scale scheme property.\n\nalt.Chart(antibiotics).mark_circle(size=80).encode(\n    alt.X('Neomycin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          axis=alt.Axis(tickCount=5),\n          title='Neomycin MIC (μg/ml, reverse log scale)'),\n    alt.Y('Penicillin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          axis=alt.Axis(tickCount=5),\n          title='Penicillin MIC (μg/ml, reverse log scale)'),\n    alt.Color('Bacteria:N',\n          scale=alt.Scale(scheme='tableau20'))\n).properties(width=250, height=250)\n\n\n\n\n\n\n\nWe now have a unique color for each bacteria, but the chart is still a mess. Among other issues, the encoding takes no account of bacteria that belong to the same genus. In the chart above, the two different Salmonella strains have very different hues (teal and pink), despite being biological cousins.\nTo try a different scheme, we can also change the data type from nominal to ordinal. The default ordinal scheme uses blue shades, ramping from light to dark:\n\nalt.Chart(antibiotics).mark_circle(size=80).encode(\n    alt.X('Neomycin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          axis=alt.Axis(tickCount=5),\n          title='Neomycin MIC (μg/ml, reverse log scale)'),\n    alt.Y('Penicillin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          axis=alt.Axis(tickCount=5),\n          title='Penicillin MIC (μg/ml, reverse log scale)'),\n    alt.Color('Bacteria:O')\n).properties(width=250, height=250)\n\n\n\n\n\n\nSome of those blue shades may be hard to distinguish.\nFor more differentiated colors, we can experiment with alternatives to the default blues color scheme. The viridis scheme ramps through both hue and luminance:\n\nalt.Chart(antibiotics).mark_circle(size=80).encode(\n    alt.X('Neomycin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          axis=alt.Axis(tickCount=5),\n          title='Neomycin MIC (μg/ml, reverse log scale)'),\n    alt.Y('Penicillin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          axis=alt.Axis(tickCount=5),\n          title='Penicillin MIC (μg/ml, reverse log scale)'),\n    alt.Color('Bacteria:O',\n          scale=alt.Scale(scheme='viridis'))\n).properties(width=250, height=250)\n\n\n\n\n\n\nBacteria from the same genus now have more similar colors than before, but the chart still remains confusing. There are many colors, they are hard to look up in the legend accurately, and two bacteria may have similar colors but different genus.\n\n\nColor by Genus\nLet’s try to color by genus instead of bacteria. To do so, we will add a calculate transform that splits up the bacteria name on space characters and takes the first word in the resulting array. We can then encode the resulting Genus field using the tableau20 color scheme.\n(Note that the antibiotics dataset includes a pre-calculated Genus field, but we will ignore it here in order to further explore Altair’s data transformations.)\n\nalt.Chart(antibiotics).mark_circle(size=80).transform_calculate(\n    Genus='split(datum.Bacteria, \" \")[0]'\n).encode(\n    alt.X('Neomycin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          axis=alt.Axis(tickCount=5),\n          title='Neomycin MIC (μg/ml, reverse log scale)'),\n    alt.Y('Penicillin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          axis=alt.Axis(tickCount=5),\n          title='Penicillin MIC (μg/ml, reverse log scale)'),\n    alt.Color('Genus:N',\n          scale=alt.Scale(scheme='tableau20'))\n).properties(width=250, height=250)\n\n\n\n\n\n\n\nHmm… While the data are better segregated by genus, this cacapohony of colors doesn’t seem particularly useful.\nIf we look at some of the previous charts carefully, we can see that only a handful of bacteria have a genus shared with another bacteria: Salmonella, Staphylococcus, and Streptococcus. To focus our comparison, we might add colors only for these repeated genus values.\nLet’s add another calculate transform that takes a genus name, keeps it if it is one of the repeated values, and otherwise uses the string \"Other\".\nIn addition, we can add custom color encodings using explicit domain and range arrays for the color encoding scale.\n\nalt.Chart(antibiotics).mark_circle(size=80).transform_calculate(\n  Split='split(datum.Bacteria, \" \")[0]'\n).transform_calculate(\n  Genus='indexof([\"Salmonella\", \"Staphylococcus\", \"Streptococcus\"], datum.Split) &gt;= 0 ? datum.Split : \"Other\"'\n).encode(\n    alt.X('Neomycin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          axis=alt.Axis(tickCount=5),\n          title='Neomycin MIC (μg/ml, reverse log scale)'),\n    alt.Y('Penicillin:Q',\n          sort='descending',\n          scale=alt.Scale(type='log', domain=[0.001, 1000]),\n          axis=alt.Axis(tickCount=5),\n          title='Penicillin MIC (μg/ml, reverse log scale)'),\n    alt.Color('Genus:N',\n          scale=alt.Scale(\n            domain=['Salmonella', 'Staphylococcus', 'Streptococcus', 'Other'],\n            range=['rgb(76,120,168)', 'rgb(84,162,75)', 'rgb(228,87,86)', 'rgb(121,112,110)']\n          ))\n).properties(width=250, height=250)\n\n\n\n\n\n\nWe now have a much more revealing plot, made possible by customizations to the axes and legend. Take a moment to examine the plot above. Notice any surprising groupings?\nThe upper-left region has a cluster of red Streptococcus bacteria, but with a grey Other bacteria alongside them. Meanwhile, towards the middle-right we see another red Streptococcus placed far away from it’s “cousins”. Might we expect bacteria from the same genus (and thus presumably more genetically similar) to be grouped closer together?\nAs it so happens, the underlying dataset actually contains errors. The dataset reflects the species designations used in the early 1950s. However, the scientific consensus has since been overturned. That gray point in the upper-left? It’s now considered a Streptococcus! That red point towards the middle-right? It’s no longer considered a Streptococcus!\nOf course, on its own, this dataset doesn’t fully justify these reclassifications. Nevertheless, the data contain valuable biological clues that went overlooked for decades! Visualization, when used by an appropriately skilled and inquisitive viewer, can be a powerful tool for discovery.\nThis example also reinforces an important lesson: always be skeptical of your data!\n\n\nColor by Antibiotic Response\nWe might also use the color channel to encode quantitative values. Though keep in mind that typically color is not as effective for conveying quantities as position or size encodings!\nHere is a basic heatmap of penicillin MIC values for each bacteria. We’ll use a rect mark and sort the bacteria by descending MIC values (from most to least resistant):\n\nalt.Chart(antibiotics).mark_rect().encode(\n    alt.Y('Bacteria:N',\n      sort=alt.EncodingSortField(field='Penicillin', op='max', order='descending')\n    ),\n    alt.Color('Penicillin:Q')\n)\n\n\n\n\n\n\nWe can further improve this chart by combining features we’ve seen thus far: a log-transformed scale, a change of axis orientation, a custom color scheme (plasma), tick count adjustment, and custom title text. We’ll also exercise configuration options to adjust the axis title placement and legend title alignment.\n\nalt.Chart(antibiotics).mark_rect().encode(\n    alt.Y('Bacteria:N',\n      sort=alt.EncodingSortField(field='Penicillin', op='max', order='descending'),\n      axis=alt.Axis(\n        orient='right',     # orient axis on right side of chart\n        titleX=7,           # set x-position to 7 pixels right of chart\n        titleY=-2,          # set y-position to 2 pixels above chart\n        titleAlign='left',  # use left-aligned text\n        titleAngle=0        # undo default title rotation\n      )\n    ),\n    alt.Color('Penicillin:Q',\n      scale=alt.Scale(type='log', scheme='plasma', nice=True),\n      legend=alt.Legend(titleOrient='right', tickCount=5),\n      title='Penicillin MIC (μg/ml)'\n    )\n)\n\n\n\n\n\n\nAlternatively, we can remove the axis title altogether, and use the top-level title property to add a title for the entire chart:\n\nalt.Chart(antibiotics, title='Penicillin Resistance of Bacterial Strains').mark_rect().encode(\n    alt.Y('Bacteria:N',\n      sort=alt.EncodingSortField(field='Penicillin', op='max', order='descending'),\n      axis=alt.Axis(orient='right', title=None)\n    ),\n    alt.Color('Penicillin:Q',\n      scale=alt.Scale(type='log', scheme='plasma', nice=True),\n      legend=alt.Legend(titleOrient='right', tickCount=5),\n      title='Penicillin MIC (μg/ml)'\n    )\n).configure_title(\n  anchor='start', # anchor and left-align title\n  offset=5        # set title offset from chart\n)"
  },
  {
    "objectID": "learn/altair/altair_scales_axes_legends.html#summary",
    "href": "learn/altair/altair_scales_axes_legends.html#summary",
    "title": "Scales, Axes, and Legends",
    "section": "Summary",
    "text": "Summary\nIntegrating what we’ve learned across the notebooks so far about encodings, data transforms, and customization, you should now be prepared to make a wide variety of statistical graphics. Now you can put Altair into everyday use for exploring and communicating data!\nInterested in learning more about this topic?\n\nStart with the Altair Customizing Visualizations documentation.\nFor a complementary discussion of scale mappings, see “Introducing d3-scale”.\nFor a more in-depth exploration of all the ways axes and legends can be styled by the underlying Vega library (which powers Altair and Vega-Lite), see “A Guide to Guides: Axes & Legends in Vega”.\nFor a fascinating history of the antibiotics dataset, see Wainer & Lysen’s “That’s Funny…” in the American Scientist."
  },
  {
    "objectID": "learn/altair/altair_view_composition.html",
    "href": "learn/altair/altair_view_composition.html",
    "title": "Multi-View Composition",
    "section": "",
    "text": "When visualizing a number of different data fields, we might be tempted to use as many visual encoding channels as we can: x, y, color, size, shape, and so on. However, as the number of encoding channels increases, a chart can rapidly become cluttered and difficult to read. An alternative to “over-loading” a single chart is to instead compose multiple charts in a way that facilitates rapid comparisons.\nIn this notebook, we will examine a variety of operations for multi-view composition:\nWe’ll then look at how these operations form a view composition algebra, in which the operations can be combined to build a variety of complex multi-view displays.\nThis notebook is part of the data visualization curriculum.\nimport pandas as pd\nimport altair as alt"
  },
  {
    "objectID": "learn/altair/altair_view_composition.html#weather-data",
    "href": "learn/altair/altair_view_composition.html#weather-data",
    "title": "Multi-View Composition",
    "section": "Weather Data",
    "text": "Weather Data\nWe will be visualizing weather statistics for the U.S. cities of Seattle and New York. Let’s load the dataset and peek at the first and last 10 rows:\n\nweather = 'https://cdn.jsdelivr.net/npm/vega-datasets@1/data/weather.csv'\n\n\ndf = pd.read_csv(weather)\ndf.head(10)\n\n\n\n\n\n\n\n\nlocation\ndate\nprecipitation\ntemp_max\ntemp_min\nwind\nweather\n\n\n\n\n0\nSeattle\n2012-01-01\n0.0\n12.8\n5.0\n4.7\ndrizzle\n\n\n1\nSeattle\n2012-01-02\n10.9\n10.6\n2.8\n4.5\nrain\n\n\n2\nSeattle\n2012-01-03\n0.8\n11.7\n7.2\n2.3\nrain\n\n\n3\nSeattle\n2012-01-04\n20.3\n12.2\n5.6\n4.7\nrain\n\n\n4\nSeattle\n2012-01-05\n1.3\n8.9\n2.8\n6.1\nrain\n\n\n5\nSeattle\n2012-01-06\n2.5\n4.4\n2.2\n2.2\nrain\n\n\n6\nSeattle\n2012-01-07\n0.0\n7.2\n2.8\n2.3\nrain\n\n\n7\nSeattle\n2012-01-08\n0.0\n10.0\n2.8\n2.0\nsun\n\n\n8\nSeattle\n2012-01-09\n4.3\n9.4\n5.0\n3.4\nrain\n\n\n9\nSeattle\n2012-01-10\n1.0\n6.1\n0.6\n3.4\nrain\n\n\n\n\n\n\n\n\ndf.tail(10)\n\n\n\n\n\n\n\n\nlocation\ndate\nprecipitation\ntemp_max\ntemp_min\nwind\nweather\n\n\n\n\n2912\nNew York\n2015-12-22\n4.8\n15.6\n11.1\n3.8\nfog\n\n\n2913\nNew York\n2015-12-23\n29.5\n17.2\n8.9\n4.5\nfog\n\n\n2914\nNew York\n2015-12-24\n0.5\n20.6\n13.9\n4.9\nfog\n\n\n2915\nNew York\n2015-12-25\n2.5\n17.8\n11.1\n0.9\nfog\n\n\n2916\nNew York\n2015-12-26\n0.3\n15.6\n9.4\n4.8\ndrizzle\n\n\n2917\nNew York\n2015-12-27\n2.0\n17.2\n8.9\n5.5\nfog\n\n\n2918\nNew York\n2015-12-28\n1.3\n8.9\n1.7\n6.3\nsnow\n\n\n2919\nNew York\n2015-12-29\n16.8\n9.4\n1.1\n5.3\nfog\n\n\n2920\nNew York\n2015-12-30\n9.4\n10.6\n5.0\n3.0\nfog\n\n\n2921\nNew York\n2015-12-31\n1.5\n11.1\n6.1\n5.5\nfog\n\n\n\n\n\n\n\nWe will create multi-view displays to examine weather within and across the cities."
  },
  {
    "objectID": "learn/altair/altair_view_composition.html#layer",
    "href": "learn/altair/altair_view_composition.html#layer",
    "title": "Multi-View Composition",
    "section": "Layer",
    "text": "Layer\nOne of the most common ways of combining multiple charts is to layer marks on top of each other. If the underlying scale domains are compatible, we can merge them to form shared axes. If either of the x or y encodings is not compatible, we might instead create a dual-axis chart, which overlays marks using separate scales and axes.\n\nShared Axes\nLet’s start by plotting the minimum and maximum average temperatures per month:\n\nalt.Chart(weather).mark_area().encode(\n  alt.X('month(date):T'),\n  alt.Y('average(temp_max):Q'),\n  alt.Y2('average(temp_min):Q')\n)\n\n\n\n\n\n\nThe plot shows us temperature ranges for each month over the entirety of our data. However, this is pretty misleading as it aggregates the measurements for both Seattle and New York!\nLet’s subdivide the data by location using a color encoding, while also adjusting the mark opacity to accommodate overlapping areas:\n\nalt.Chart(weather).mark_area(opacity=0.3).encode(\n  alt.X('month(date):T'),\n  alt.Y('average(temp_max):Q'),\n  alt.Y2('average(temp_min):Q'),\n  alt.Color('location:N')\n)\n\n\n\n\n\n\nWe can see that Seattle is more temperate: warmer in the winter, and cooler in the summer.\nIn this case we’ve created a layered chart without any special features by simply subdividing the area marks by color. While the chart above shows us the temperature ranges, we might also want to emphasize the middle of the range.\nLet’s create a line chart showing the average temperature midpoint. We’ll use a calculate transform to compute the midpoints between the minimum and maximum daily temperatures:\n\nalt.Chart(weather).mark_line().transform_calculate(\n  temp_mid='(+datum.temp_min + +datum.temp_max) / 2'\n).encode(\n  alt.X('month(date):T'),\n  alt.Y('average(temp_mid):Q'),\n  alt.Color('location:N')\n)\n\n\n\n\n\n\nAside: note the use of +datum.temp_min within the calculate transform. As we are loading the data directly from a CSV file without any special parsing instructions, the temperature values may be internally represented as string values. Adding the + in front of the value forces it to be treated as a number.\nWe’d now like to combine these charts by layering the midpoint lines over the range areas. Using the syntax chart1 + chart2, we can specify that we want a new layered chart in which chart1 is the first layer and chart2 is a second layer drawn on top:\n\ntempMinMax = alt.Chart(weather).mark_area(opacity=0.3).encode(\n  alt.X('month(date):T'),\n  alt.Y('average(temp_max):Q'),\n  alt.Y2('average(temp_min):Q'),\n  alt.Color('location:N')\n)\n\ntempMid = alt.Chart(weather).mark_line().transform_calculate(\n  temp_mid='(+datum.temp_min + +datum.temp_max) / 2'\n).encode(\n  alt.X('month(date):T'),\n  alt.Y('average(temp_mid):Q'),\n  alt.Color('location:N')\n)\n\ntempMinMax + tempMid\n\n\n\n\n\n\nNow we have a multi-layer plot! However, the y-axis title (though informative) has become a bit long and unruly…\nLet’s customize our axes to clean up the plot. If we set a custom axis title within one of the layers, it will automatically be used as a shared axis title for all the layers:\n\ntempMinMax = alt.Chart(weather).mark_area(opacity=0.3).encode(\n  alt.X('month(date):T', title=None, axis=alt.Axis(format='%b')),\n  alt.Y('average(temp_max):Q', title='Avg. Temperature °C'),\n  alt.Y2('average(temp_min):Q'),\n  alt.Color('location:N')\n)\n\ntempMid = alt.Chart(weather).mark_line().transform_calculate(\n  temp_mid='(+datum.temp_min + +datum.temp_max) / 2'\n).encode(\n  alt.X('month(date):T'),\n  alt.Y('average(temp_mid):Q'),\n  alt.Color('location:N')\n)\n\ntempMinMax + tempMid\n\n\n\n\n\n\nWhat happens if both layers have custom axis titles? Modify the code above to find out…\nAbove used the + operator, a convenient shorthand for Altair’s layer method. We can generate an identical layered chart using the layer method directly:\n\nalt.layer(tempMinMax, tempMid)\n\n\n\n\n\n\nNote that the order of inputs to a layer matters, as subsequent layers will be drawn on top of earlier layers. Try swapping the order of the charts in the cells above. What happens? (Hint: look closely at the color of the line marks.)\n\n\nDual-Axis Charts\nSeattle has a reputation as a rainy city. Is that deserved?\nLet’s look at precipitation alongside temperature to learn more. First let’s create a base plot the shows average monthly precipitation in Seattle:\n\nalt.Chart(weather).transform_filter(\n  'datum.location == \"Seattle\"'\n).mark_line(\n  interpolate='monotone',\n  stroke='grey'\n).encode(\n  alt.X('month(date):T', title=None),\n  alt.Y('average(precipitation):Q', title='Precipitation')\n)\n\n\n\n\n\n\nTo facilitate comparison with the temperature data, let’s create a new layered chart. Here’s what happens if we try to layer the charts as we did earlier:\n\ntempMinMax = alt.Chart(weather).transform_filter(\n  'datum.location == \"Seattle\"'\n).mark_area(opacity=0.3).encode(\n  alt.X('month(date):T', title=None, axis=alt.Axis(format='%b')),\n  alt.Y('average(temp_max):Q', title='Avg. Temperature °C'),\n  alt.Y2('average(temp_min):Q')\n)\n\nprecip = alt.Chart(weather).transform_filter(\n  'datum.location == \"Seattle\"'\n).mark_line(\n  interpolate='monotone',\n  stroke='grey'\n).encode(\n  alt.X('month(date):T'),\n  alt.Y('average(precipitation):Q', title='Precipitation')\n)\n\nalt.layer(tempMinMax, precip)\n\n\n\n\n\n\nThe precipitation values use a much smaller range of the y-axis then the temperatures!\nBy default, layered charts use a shared domain: the values for the x-axis or y-axis are combined across all the layers to determine a shared extent. This default behavior assumes that the layered values have the same units. However, this doesn’t hold up for this example, as we are combining temperature values (degrees Celsius) with precipitation values (inches)!\nIf we want to use different y-axis scales, we need to specify how we want Altair to resolve the data across layers. In this case, we want to resolve the y-axis scale domains to be independent rather than use a shared domain. The Chart object produced by a layer operator includes a resolve_scale method with which we can specify the desired resolution:\n\ntempMinMax = alt.Chart(weather).transform_filter(\n  'datum.location == \"Seattle\"'\n).mark_area(opacity=0.3).encode(\n  alt.X('month(date):T', title=None, axis=alt.Axis(format='%b')),\n  alt.Y('average(temp_max):Q', title='Avg. Temperature °C'),\n  alt.Y2('average(temp_min):Q')\n)\n\nprecip = alt.Chart(weather).transform_filter(\n  'datum.location == \"Seattle\"'\n).mark_line(\n  interpolate='monotone',\n  stroke='grey'\n).encode(\n  alt.X('month(date):T'),\n  alt.Y('average(precipitation):Q', title='Precipitation')\n)\n\nalt.layer(tempMinMax, precip).resolve_scale(y='independent')\n\n\n\n\n\n\nWe can now see that autumn is the rainiest season in Seattle (peaking in November), complemented by dry summers.\nYou may have noticed some redundancy in our plot specifications above: both use the same dataset and the same filter to look at Seattle only. If you want, you can streamline the code a bit by providing the data and filter transform to the top-level layered chart. The individual layers will then inherit the data if they don’t have their own data definitions:\n\ntempMinMax = alt.Chart().mark_area(opacity=0.3).encode(\n  alt.X('month(date):T', title=None, axis=alt.Axis(format='%b')),\n  alt.Y('average(temp_max):Q', title='Avg. Temperature °C'),\n  alt.Y2('average(temp_min):Q')\n)\n\nprecip = alt.Chart().mark_line(\n  interpolate='monotone',\n  stroke='grey'\n).encode(\n  alt.X('month(date):T'),\n  alt.Y('average(precipitation):Q', title='Precipitation')\n)\n\nalt.layer(tempMinMax, precip, data=weather).transform_filter(\n  'datum.location == \"Seattle\"'\n).resolve_scale(y='independent')\n\n\n\n\n\n\nWhile dual-axis charts can be useful, they are often prone to misinterpretation, as the different units and axis scales may be incommensurate. As is feasible, you might consider transformations that map different data fields to shared units, for example showing quantiles or relative percentage change."
  },
  {
    "objectID": "learn/altair/altair_view_composition.html#facet",
    "href": "learn/altair/altair_view_composition.html#facet",
    "title": "Multi-View Composition",
    "section": "Facet",
    "text": "Facet\nFaceting involves subdividing a dataset into groups and creating a separate plot for each group. In earlier notebooks, we learned how to create faceted charts using the row and column encoding channels. We’ll first review those channels and then show how they are instances of the more general facet operator.\nLet’s start with a basic histogram of maximum temperature values in Seattle:\n\nalt.Chart(weather).mark_bar().transform_filter(\n  'datum.location == \"Seattle\"'\n).encode(\n  alt.X('temp_max:Q', bin=True, title='Temperature (°C)'),\n  alt.Y('count():Q')\n)\n\n\n\n\n\n\nHow does this temperature profile change based on the weather of a given day – that is, whether there was drizzle, fog, rain, snow, or sun?\nLet’s use the column encoding channel to facet the data by weather type. We can also use color as a redundant encoding, using a customized color range:\n\ncolors = alt.Scale(\n  domain=['drizzle', 'fog', 'rain', 'snow', 'sun'],\n  range=['#aec7e8', '#c7c7c7', '#1f77b4', '#9467bd', '#e7ba52']\n)\n\nalt.Chart(weather).mark_bar().transform_filter(\n  'datum.location == \"Seattle\"'\n).encode(\n  alt.X('temp_max:Q', bin=True, title='Temperature (°C)'),\n  alt.Y('count():Q'),\n  alt.Color('weather:N', scale=colors),\n  alt.Column('weather:N')\n).properties(\n  width=150,\n  height=150\n)\n\n\n\n\n\n\nUnsurprisingly, those rare snow days center on the coldest temperatures, followed by rainy and foggy days. Sunny days are warmer and, despite Seattle stereotypes, are the most plentiful. Though as any Seattleite can tell you, the drizzle occasionally comes, no matter the temperature!\nIn addition to row and column encoding channels within a chart definition, we can take a basic chart definition and apply faceting using an explicit facet operator.\nLet’s recreate the chart above, but this time using facet. We start with the same basic histogram definition, but remove the data source, filter transform, and column channel. We can then invoke the facet method, passing in the data and specifying that we should facet into columns according to the weather field. The facet method accepts both row and column arguments. The two can be used together to create a 2D grid of faceted plots.\nFinally we include our filter transform, applying it to the top-level faceted chart. While we could apply the filter transform to the histogram definition as before, that is slightly less efficient. Rather than filter out “New York” values within each facet cell, applying the filter to the faceted chart lets Vega-Lite know that we can filter out those values up front, prior to the facet subdivision.\n\ncolors = alt.Scale(\n  domain=['drizzle', 'fog', 'rain', 'snow', 'sun'],\n  range=['#aec7e8', '#c7c7c7', '#1f77b4', '#9467bd', '#e7ba52']\n)\n\nalt.Chart().mark_bar().encode(\n  alt.X('temp_max:Q', bin=True, title='Temperature (°C)'),\n  alt.Y('count():Q'),\n  alt.Color('weather:N', scale=colors)\n).properties(\n  width=150,\n  height=150\n).facet(\n  data=weather,\n  column='weather:N'\n).transform_filter(\n  'datum.location == \"Seattle\"'\n)\n\n\n\n\n\n\nGiven all the extra code above, why would we want to use an explicit facet operator? For basic charts, we should certainly use the column or row encoding channels if we can. However, using the facet operator explicitly is useful if we want to facet composed views, such as layered charts.\nLet’s revisit our layered temperature plots from earlier. Instead of plotting data for New York and Seattle in the same plot, let’s break them up into separate facets. The individual chart definitions are nearly the same as before: one area chart and one line chart. The only difference is that this time we won’t pass the data directly to the chart constructors; we’ll wait and pass it to the facet operator later. We can layer the charts much as before, then invoke facet on the layered chart object, passing in the data and specifying column facets based on the location field:\n\ntempMinMax = alt.Chart().mark_area(opacity=0.3).encode(\n  alt.X('month(date):T', title=None, axis=alt.Axis(format='%b')),\n  alt.Y('average(temp_max):Q', title='Avg. Temperature (°C)'),\n  alt.Y2('average(temp_min):Q'),\n  alt.Color('location:N')\n)\n\ntempMid = alt.Chart().mark_line().transform_calculate(\n  temp_mid='(+datum.temp_min + +datum.temp_max) / 2'\n).encode(\n  alt.X('month(date):T'),\n  alt.Y('average(temp_mid):Q'),\n  alt.Color('location:N')\n)\n\nalt.layer(tempMinMax, tempMid).facet(\n  data=weather,\n  column='location:N'\n)\n\n\n\n\n\n\nThe faceted charts we have seen so far use the same axis scale domains across the facet cells. This default of using shared scales and axes helps aid accurate comparison of values. However, in some cases you may wish to scale each chart independently, for example if the range of values in the cells differs significantly.\nSimilar to layered charts, faceted charts also support resolving to independent scales or axes across plots. Let’s see what happens if we call the resolve_axis method to request independent y-axes:\n\ntempMinMax = alt.Chart().mark_area(opacity=0.3).encode(\n  alt.X('month(date):T', title=None, axis=alt.Axis(format='%b')),\n  alt.Y('average(temp_max):Q', title='Avg. Temperature (°C)'),\n  alt.Y2('average(temp_min):Q'),\n  alt.Color('location:N')\n)\n\ntempMid = alt.Chart().mark_line().transform_calculate(\n  temp_mid='(+datum.temp_min + +datum.temp_max) / 2'\n).encode(\n  alt.X('month(date):T'),\n  alt.Y('average(temp_mid):Q'),\n  alt.Color('location:N')\n)\n\nalt.layer(tempMinMax, tempMid).facet(\n  data=weather,\n  column='location:N'\n).resolve_axis(y='independent')\n\n\n\n\n\n\nThe chart above looks largely unchanged, but the plot for Seattle now includes its own axis.\nWhat if we instead call resolve_scale to resolve the underlying scale domains?\n\ntempMinMax = alt.Chart().mark_area(opacity=0.3).encode(\n  alt.X('month(date):T', title=None, axis=alt.Axis(format='%b')),\n  alt.Y('average(temp_max):Q', title='Avg. Temperature (°C)'),\n  alt.Y2('average(temp_min):Q'),\n  alt.Color('location:N')\n)\n\ntempMid = alt.Chart().mark_line().transform_calculate(\n  temp_mid='(+datum.temp_min + +datum.temp_max) / 2'\n).encode(\n  alt.X('month(date):T'),\n  alt.Y('average(temp_mid):Q'),\n  alt.Color('location:N')\n)\n\nalt.layer(tempMinMax, tempMid).facet(\n  data=weather,\n  column='location:N'\n).resolve_scale(y='independent')\n\n\n\n\n\n\nNow we see facet cells with different axis scale domains. In this case, using independent scales seems like a bad idea! The domains aren’t very different, and one might be fooled into thinking that New York and Seattle have similar maximum summer temperatures.\nTo borrow a cliché: just because you can do something, doesn’t mean you should…"
  },
  {
    "objectID": "learn/altair/altair_view_composition.html#concatenate",
    "href": "learn/altair/altair_view_composition.html#concatenate",
    "title": "Multi-View Composition",
    "section": "Concatenate",
    "text": "Concatenate\nFaceting creates small multiple plots that show separate subdivisions of the data. However, we might wish to create a multi-view display with different views of the same dataset (not subsets) or views involving different datasets.\nAltair provides concatenation operators to combine arbitrary charts into a composed chart. The hconcat operator (shorthand | ) performs horizontal concatenation, while the vconcat operator (shorthand &) performs vertical concatenation.\nLet’s start with a basic line chart showing the average maximum temperature per month for both New York and Seattle, much like we’ve seen before:\n\nalt.Chart(weather).mark_line().encode(\n  alt.X('month(date):T', title=None),\n  alt.Y('average(temp_max):Q'),\n  color='location:N'\n)\n\n\n\n\n\n\nWhat if we want to compare not just temperature over time, but also precipitation and wind levels?\nLet’s create a concatenated chart consisting of three plots. We’ll start by defining a “base” chart definition that contains all the aspects that should be shared by our three plots. We can then modify this base chart to create customized variants, with different y-axis encodings for the temp_max, precipitation, and wind fields. We can then concatenate them using the pipe (|) shorthand operator:\n\nbase = alt.Chart(weather).mark_line().encode(\n  alt.X('month(date):T', title=None),\n  color='location:N'\n).properties(\n  width=240,\n  height=180\n)\n\ntemp = base.encode(alt.Y('average(temp_max):Q'))\nprecip = base.encode(alt.Y('average(precipitation):Q'))\nwind = base.encode(alt.Y('average(wind):Q'))\n\ntemp | precip | wind\n\n\n\n\n\n\nAlternatively, we could use the more explicit alt.hconcat() method in lieu of the pipe | operator. Try rewriting the code above to use hconcat instead.\nVertical concatenation works similarly to horizontal concatenation. Using the & operator (or alt.vconcat method), modify the code to use a vertical ordering instead of a horizontal ordering.\nFinally, note that horizontal and vertical concatenation can be combined. What happens if you write something like (temp | precip) & wind?\nAside: Note the importance of those parentheses… what happens if you remove them? Keep in mind that these overloaded operators are still subject to Python’s operator precendence rules, and so vertical concatenation with & will take precedence over horizontal concatenation with |!\nAs we will revisit later, concatenation operators let you combine any and all charts into a multi-view dashboard!"
  },
  {
    "objectID": "learn/altair/altair_view_composition.html#repeat",
    "href": "learn/altair/altair_view_composition.html#repeat",
    "title": "Multi-View Composition",
    "section": "Repeat",
    "text": "Repeat\nThe concatenation operators above are quite general, allowing arbitrary charts to be composed. Nevertheless, the example above was still a bit verbose: we have three very similar charts, yet have to define them separately and then concatenate them.\nFor cases where only one or two variables are changing, the repeat operator provides a convenient shortcut for creating multiple charts. Given a template specification with some free variables, the repeat operator will then create a chart for each specified assignment to those variables.\nLet’s recreate our concatenation example above using the repeat operator. The only aspect that changes across charts is the choice of data field for the y encoding channel. To create a template specification, we can use the repeater variable alt.repeat('column') as our y-axis field. This code simply states that we want to use the variable assigned to the column repeater, which organizes repeated charts in a horizontal direction. (As the repeater provides the field name only, we have to specify the field data type separately as type='quantitative'.)\nWe then invoke the repeat method, passing in data field names for each column:\n\nalt.Chart(weather).mark_line().encode(\n  alt.X('month(date):T',title=None),\n  alt.Y(alt.repeat('column'), aggregate='average', type='quantitative'),\n  color='location:N'\n).properties(\n  width=240,\n  height=180\n).repeat(\n  column=['temp_max', 'precipitation', 'wind']\n)\n\n\n\n\n\n\nRepetition is supported for both columns and rows. What happens if you modify the code above to use row instead of column?\nWe can also use row and column repetition together! One common visualization for exploratory data analysis is the scatter plot matrix (or SPLOM). Given a collection of variables to inspect, a SPLOM provides a grid of all pairwise plots of those variables, allowing us to assess potential associations.\nLet’s use the repeat operator to create a SPLOM for the temp_max, precipitation, and wind fields. We first create our template specification, with repeater variables for both the x- and y-axis data fields. We then invoke repeat, passing in arrays of field names to use for both row and column. Altair will then generate the cross product (or, Cartesian product) to create the full space of repeated charts:\n\nalt.Chart().mark_point(filled=True, size=15, opacity=0.5).encode(\n  alt.X(alt.repeat('column'), type='quantitative'),\n  alt.Y(alt.repeat('row'), type='quantitative')\n).properties(\n  width=150,\n  height=150\n).repeat(\n  data=weather,\n  row=['temp_max', 'precipitation', 'wind'],\n  column=['wind', 'precipitation', 'temp_max']\n).transform_filter(\n  'datum.location == \"Seattle\"'\n)\n\n\n\n\n\n\nLooking at these plots, there does not appear to be a strong association between precipitation and wind, though we do see that extreme wind and precipitation events occur in similar temperature ranges (~5-15° C). However, this observation is not particularly surprising: if we revisit our histogram at the beginning of the facet section, we can plainly see that the days with maximum temperatures in the range of 5-15° C are the most commonly occurring.\nModify the code above to get a better understanding of chart repetition. Try adding another variable (temp_min) to the SPLOM. What happens if you rearrange the order of the field names in either the row or column parameters for the repeat operator?\nFinally, to really appreciate what the repeat operator provides, take a moment to imagine how you might recreate the SPLOM above using only hconcat and vconcat!"
  },
  {
    "objectID": "learn/altair/altair_view_composition.html#a-view-composition-algebra",
    "href": "learn/altair/altair_view_composition.html#a-view-composition-algebra",
    "title": "Multi-View Composition",
    "section": "A View Composition Algebra",
    "text": "A View Composition Algebra\nTogether, the composition operators layer, facet, concat, and repeat form a view composition algebra: the various operators can be combined to construct a variety of multi-view visualizations.\nAs an example, let’s start with two basic charts: a histogram and a simple line (a single rule mark) showing a global average.\n\nbasic1 = alt.Chart(weather).transform_filter(\n  'datum.location == \"Seattle\"'\n).mark_bar().encode(\n  alt.X('month(date):O'),\n  alt.Y('average(temp_max):Q')\n)\n\nbasic2 = alt.Chart(weather).transform_filter(\n  'datum.location == \"Seattle\"'\n).mark_rule(stroke='firebrick').encode(\n  alt.Y('average(temp_max):Q')\n)\n\nbasic1 | basic2\n\n\n\n\n\n\nWe can then combine the two charts using a layer operator, and then repeat that layered chart to show histograms with overlaid averages for multiple fields:\n\nalt.layer(\n  alt.Chart().mark_bar().encode(\n    alt.X('month(date):O', title='Month'),\n    alt.Y(alt.repeat('column'), aggregate='average', type='quantitative')\n  ),\n  alt.Chart().mark_rule(stroke='firebrick').encode(\n    alt.Y(alt.repeat('column'), aggregate='average', type='quantitative')\n  )\n).properties(\n  width=200,\n  height=150\n).repeat(\n  data=weather,\n  column=['temp_max', 'precipitation', 'wind']\n).transform_filter(\n  'datum.location == \"Seattle\"'\n)\n\n\n\n\n\n\nFocusing only on the multi-view composition operators, the model for the visualization above is:\nrepeat(column=[...])\n|- layer\n   |- basic1\n   |- basic2\nNow let’s explore how we can apply all the operators within a final dashboard that provides an overview of Seattle weather. We’ll combine the SPLOM and faceted histogram displays from earlier sections with the repeated histograms above:\n\nsplom = alt.Chart().mark_point(filled=True, size=15, opacity=0.5).encode(\n  alt.X(alt.repeat('column'), type='quantitative'),\n  alt.Y(alt.repeat('row'), type='quantitative')\n).properties(\n  width=125,\n  height=125\n).repeat(\n  row=['temp_max', 'precipitation', 'wind'],\n  column=['wind', 'precipitation', 'temp_max']\n)\n\ndateHist = alt.layer(\n  alt.Chart().mark_bar().encode(\n    alt.X('month(date):O', title='Month'),\n    alt.Y(alt.repeat('row'), aggregate='average', type='quantitative')\n  ),\n  alt.Chart().mark_rule(stroke='firebrick').encode(\n    alt.Y(alt.repeat('row'), aggregate='average', type='quantitative')\n  )\n).properties(\n  width=175,\n  height=125\n).repeat(\n  row=['temp_max', 'precipitation', 'wind']\n)\n\ntempHist = alt.Chart(weather).mark_bar().encode(\n  alt.X('temp_max:Q', bin=True, title='Temperature (°C)'),\n  alt.Y('count():Q'),\n  alt.Color('weather:N', scale=alt.Scale(\n    domain=['drizzle', 'fog', 'rain', 'snow', 'sun'],\n    range=['#aec7e8', '#c7c7c7', '#1f77b4', '#9467bd', '#e7ba52']\n  ))\n).properties(\n  width=115,\n  height=100\n).facet(\n  column='weather:N'\n)\n\nalt.vconcat(\n  alt.hconcat(splom, dateHist),\n  tempHist,\n  data=weather,\n  title='Seattle Weather Dashboard'\n).transform_filter(\n  'datum.location == \"Seattle\"'\n).resolve_legend(\n  color='independent'\n).configure_axis(\n  labelAngle=0\n)\n\n\n\n\n\n\nThe full composition model for this dashboard is:\nvconcat\n|- hconcat\n|  |- repeat(row=[...], column=[...])\n|  |  |- splom base chart\n|  |- repeat(row=[...])\n|     |- layer\n|        |- dateHist base chart 1\n|        |- dateHist base chart 2\n|- facet(column='weather')\n   |- tempHist base chart\nPhew! The dashboard also includes a few customizations to improve the layout:\n\nWe adjust chart width and height properties to assist alignment and ensure the full visualization fits on the screen.\nWe add resolve_legend(color='independent') to ensure the color legend is associated directly with the colored histograms by temperature. Otherwise, the legend will resolve to the dashboard as a whole.\nWe use configure_axis(labelAngle=0) to ensure that no axis labels are rotated. This helps to ensure proper alignment among the scatter plots in the SPLOM and the histograms by month on the right.\n\nTry removing or modifying any of these adjustments and see how the dashboard layout responds!\nThis dashboard can be reused to show data for other locations or from other datasets. Update the dashboard to show weather patterns for New York instead of Seattle."
  },
  {
    "objectID": "learn/altair/altair_view_composition.html#summary",
    "href": "learn/altair/altair_view_composition.html#summary",
    "title": "Multi-View Composition",
    "section": "Summary",
    "text": "Summary\nFor more details on multi-view composition, including control over sub-plot spacing and header labels, see the Altair Compound Charts documentation.\nNow that we’ve seen how to compose multiple views, we’re ready to put them into action. In addition to statically presenting data, multiple views can enable interactive multi-dimensional exploration. For example, using linked selections we can highlight points in one view to see corresponding values highlight in other views.\nIn the next notebook, we’ll examine how to author interactive selections for both individual plots and multi-view compositions."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact me",
    "section": "",
    "text": "Email: daniel@kapitan.net\nLinkedIn: dkapitan"
  },
  {
    "objectID": "rwds-partners.html",
    "href": "rwds-partners.html",
    "title": "Our partners and funders",
    "section": "",
    "text": "Real World Data Science is a project of the Royal Statistical Society (RSS). The Society was founded in 1834 and is one of the world’s leading organisations advocating for the importance of statistics and data.\nRSS has more than 10,000 members in the UK and across the world. As a charity, it advocates for the key role of statistics and data in society, and works to ensure that policy formulation and decision making are informed by evidence for the public good.\nTo support the work of the RSS, including Real World Data Science and other projects, become a member today.\nEmail: info@rss.org.uk"
  },
  {
    "objectID": "rwds-partners.html#the-royal-statistical-society",
    "href": "rwds-partners.html#the-royal-statistical-society",
    "title": "Our partners and funders",
    "section": "",
    "text": "Real World Data Science is a project of the Royal Statistical Society (RSS). The Society was founded in 1834 and is one of the world’s leading organisations advocating for the importance of statistics and data.\nRSS has more than 10,000 members in the UK and across the world. As a charity, it advocates for the key role of statistics and data in society, and works to ensure that policy formulation and decision making are informed by evidence for the public good.\nTo support the work of the RSS, including Real World Data Science and other projects, become a member today.\nEmail: info@rss.org.uk"
  },
  {
    "objectID": "images/altair_marks_encoding.out.html",
    "href": "images/altair_marks_encoding.out.html",
    "title": "Data Types, Graphical Marks, and Visual Encoding Channels",
    "section": "",
    "text": "A visualization represents data using a collection of graphical marks (bars, lines, points, etc.). The attributes of a mark — such as its position, shape, size, or color — serve as channels through which we can encode underlying data values.\nWith a basic framework of data types, marks, and encoding channels, we can concisely create a wide variety of visualizations. In this notebook, we explore each of these elements and show how to use them to create custom statistical graphics.\nThis notebook is part of the data visualization curriculum.\nimport pandas as pd\nimport altair as alt"
  },
  {
    "objectID": "images/altair_marks_encoding.out.html#global-development-data",
    "href": "images/altair_marks_encoding.out.html#global-development-data",
    "title": "Data Types, Graphical Marks, and Visual Encoding Channels",
    "section": "Global Development Data",
    "text": "Global Development Data\nWe will be visualizing global health and population data for a number of countries, over the time period of 1955 to 2005. The data was collected by the Gapminder Foundation and shared in Hans Rosling’s popular TED talk. If you haven’t seen the talk, we encourage you to watch it first!\nLet’s first load the dataset from the vega-datasets collection into a Pandas data frame.\n\nfrom vega_datasets import data as vega_data\ndata = vega_data.gapminder()\n\nHow big is the data?\n\ndata.shape\n\n(693, 6)\n\n\n693 rows and 6 columns! Let’s take a peek at the data content:\n\ndata.head(5)"
  },
  {
    "objectID": "images/altair_marks_encoding.out.html#data-types",
    "href": "images/altair_marks_encoding.out.html#data-types",
    "title": "Data Types, Graphical Marks, and Visual Encoding Channels",
    "section": "Data Types",
    "text": "Data Types\nThe first ingredient in effective visualization is the input data. Data values can represent different forms of measurement. What kinds of comparisons do those measurements support? And what kinds of visual encodings then support those comparisons?\nWe will start by looking at the basic data types that Altair uses to inform visual encoding choices. These data types determine the kinds of comparisons we can make, and thereby guide our visualization design decisions.\n\nNominal (N)\nNominal data (also called categorical data) consist of category names.\nWith nominal data we can compare the equality of values: is value A the same or different than value B? (A = B), supporting statements like “A is equal to B” or “A is not equal to B”. In the dataset above, the country field is nominal.\nWhen visualizing nominal data we should readily be able to see if values are the same or different: position, color hue (blue, red, green, etc.), and shape can help. However, using a size channel to encode nominal data might mislead us, suggesting rank-order or magnitude differences among values that do not exist!\n\n\nOrdinal (O)\nOrdinal data consist of values that have a specific ordering.\nWith ordinal data we can compare the rank-ordering of values: does value A come before or after value B? (A &lt; B), supporting statements like “A is less than B” or “A is greater than B”. In the dataset above, we can treat the year field as ordinal.\nWhen visualizing ordinal data, we should perceive a sense of rank-order. Position, size, or color value (brightness) might be appropriate, where as color hue (which is not perceptually ordered) would be less appropriate.\n\n\nQuantitative (Q)\nWith quantitative data we can measure numerical differences among values. There are multiple sub-types of quantitative data:\nFor interval data we can measure the distance (interval) between points: what is the distance to value A from value B? (A - B), supporting statements such as “A is 12 units away from B”.\nFor ratio data the zero-point is meaningful and so we can also measure proportions or scale factors: value A is what proportion of value B? (A / B), supporting statements such as “A is 10% of B” or “B is 7 times larger than A”.\nIn the dataset above, year is a quantitative interval field (the value of year “zero” is subjective), whereas fertility and life_expect are quantitative ratio fields (zero is meaningful for calculating proportions). Vega-Lite represents quantitative data, but does not make a distinction between interval and ratio types.\nQuantitative values can be visualized using position, size, or color value, among other channels. An axis with a zero baseline is essential for proportional comparisons of ratio values, but can be safely omitted for interval comparisons.\n\n\nTemporal (T)\nTemporal values measure time points or intervals. This type is a special case of quantitative values (timestamps) with rich semantics and conventions (i.e., the Gregorian calendar). The temporal type in Vega-Lite supports reasoning about time units (year, month, day, hour, etc.), and provides methods for requesting specific time intervals.\nExample temporal values include date strings such as “2019-01-04” and “Jan 04 2019”, as well as standardized date-times such as the ISO date-time format: “2019-01-04T17:50:35.643Z”.\nThere are no temporal values in our global development dataset above, as the year field is simply encoded as an integer. For more details about using temporal data in Altair, see the Times and Dates documentation.\n\n\nSummary\nThese data types are not mutually exclusive, but rather form a hierarchy: ordinal data support nominal (equality) comparisons, while quantitative data support ordinal (rank-order) comparisons.\nMoreover, these data types do not provide a fixed categorization. Just because a data field is represented using a number doesn’t mean we have to treat it as a quantitative type! For example, we might interpret a set of ages (10 years old, 20 years old, etc) as nominal (underage or overage), ordinal (grouped by year), or quantitative (calculate average age).\nNow let’s examine how to visually encode these data types!"
  },
  {
    "objectID": "images/altair_marks_encoding.out.html#encoding-channels",
    "href": "images/altair_marks_encoding.out.html#encoding-channels",
    "title": "Data Types, Graphical Marks, and Visual Encoding Channels",
    "section": "Encoding Channels",
    "text": "Encoding Channels\nAt the heart of Altair is the use of encodings that bind data fields (with a given data type) to available encoding channels of a chosen mark type. In this notebook we’ll examine the following encoding channels:\n\nx: Horizontal (x-axis) position of the mark.\ny: Vertical (y-axis) position of the mark.\nsize: Size of the mark. May correspond to area or length, depending on the mark type.\ncolor: Mark color, specified as a legal CSS color.\nopacity: Mark opacity, ranging from 0 (fully transparent) to 1 (fully opaque).\nshape: Plotting symbol shape for point marks.\ntooltip: Tooltip text to display upon mouse hover over the mark.\norder: Mark ordering, determines line/area point order and drawing order.\ncolumn: Facet the data into horizontally-aligned subplots.\nrow: Facet the data into vertically-aligned subplots.\n\nFor a complete list of available channels, see the Altair encoding documentation.\n\nX\nThe x encoding channel sets a mark’s horizontal position (x-coordinate). In addition, default choices of axis and title are made automatically. In the chart below, the choice of a quantitative data type results in a continuous linear axis scale:\n\nalt.Chart(data2000).mark_point().encode(\n    alt.X('fertility:Q')\n)\n\n\n\n\n\n\n\n\nY\nThe y encoding channel sets a mark’s vertical position (y-coordinate). Here we’ve added the cluster field using an ordinal (O) data type. The result is a discrete axis that includes a sized band, with a default step size, for each unique value:\n\nalt.Chart(data2000).mark_point().encode(\n    alt.X('fertility:Q'),\n    alt.Y('cluster:O')\n)\n\n\n\n\n\n\nWhat happens to the chart above if you swap the O and Q field types?\nIf we instead add the life_expect field as a quantitative (Q) variable, the result is a scatter plot with linear scales for both axes:\n\nalt.Chart(data2000).mark_point().encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q')\n)\n\n\n\n\n\n\nBy default, axes for linear quantitative scales include zero to ensure a proper baseline for comparing ratio-valued data. In some cases, however, a zero baseline may be meaningless or you may want to focus on interval comparisons. To disable automatic inclusion of zero, configure the scale mapping using the encoding scale attribute:\n\nalt.Chart(data2000).mark_point().encode(\n    alt.X('fertility:Q', scale=alt.Scale(zero=False)),\n    alt.Y('life_expect:Q', scale=alt.Scale(zero=False))\n)\n\n\n\n\n\n\nNow the axis scales no longer include zero by default. Some padding still remains, as the axis domain end points are automatically snapped to nice numbers like multiples of 5 or 10.\nWhat happens if you also add nice=False to the scale attribute above?\n\n\nSize\nThe size encoding channel sets a mark’s size or extent. The meaning of the channel can vary based on the mark type. For point marks, the size channel maps to the pixel area of the plotting symbol, such that the diameter of the point matches the square root of the size value.\nLet’s augment our scatter plot by encoding population (pop) on the size channel. As a result, the chart now also includes a legend for interpreting the size values.\n\nalt.Chart(data2000).mark_point().encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q')\n)\n\n\n\n\n\n\nIn some cases we might be unsatisfied with the default size range. To provide a customized span of sizes, set the range parameter of the scale attribute to an array indicating the smallest and largest sizes. Here we update the size encoding to range from 0 pixels (for zero values) to 1,000 pixels (for the maximum value in the scale domain):\n\nalt.Chart(data2000).mark_point().encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000]))\n)\n\n\n\n\n\n\n\n\nColor and Opacity\nThe color encoding channel sets a mark’s color. The style of color encoding is highly dependent on the data type: nominal data will default to a multi-hued qualitative color scheme, whereas ordinal and quantitative data will use perceptually ordered color gradients.\nHere, we encode the cluster field using the color channel and a nominal (N) data type, resulting in a distinct hue for each cluster value. Can you start to guess what the cluster field might indicate?\n\nalt.Chart(data2000).mark_point().encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000])),\n    alt.Color('cluster:N')\n)\n\n\n\n\n\n\nIf we prefer filled shapes, we can can pass a filled=True parameter to the mark_point method:\n\nalt.Chart(data2000).mark_point(filled=True).encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000])),\n    alt.Color('cluster:N')\n)\n\n\n\n\n\n\nBy default, Altair uses a bit of transparency to help combat over-plotting. We are free to further adjust the opacity, either by passing a default value to the mark_* method, or using a dedicated encoding channel.\nHere we demonstrate how to provide a constant value to an encoding channel instead of binding a data field:\n\nalt.Chart(data2000).mark_point(filled=True).encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000])),\n    alt.Color('cluster:N'),\n    alt.OpacityValue(0.5)\n)\n\n\n\n\n\n\n\n\nShape\nThe shape encoding channel sets the geometric shape used by point marks. Unlike the other channels we have seen so far, the shape channel can not be used by other mark types. The shape encoding channel should only be used with nominal data, as perceptual rank-order and magnitude comparisons are not supported.\nLet’s encode the cluster field using shape as well as color. Using multiple channels for the same underlying data field is known as a redundant encoding. The resulting chart combines both color and shape information into a single symbol legend:\n\nalt.Chart(data2000).mark_point(filled=True).encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000])),\n    alt.Color('cluster:N'),\n    alt.OpacityValue(0.5),\n    alt.Shape('cluster:N')\n)\n\n\n\n\n\n\n\n\nTooltips & Ordering\nBy this point, you might feel a bit frustrated: we’ve built up a chart, but we still don’t know what countries the visualized points correspond to! Let’s add interactive tooltips to enable exploration.\nThe tooltip encoding channel determines tooltip text to show when a user moves the mouse cursor over a mark. Let’s add a tooltip encoding for the country field, then investigate which countries are being represented.\n\nalt.Chart(data2000).mark_point(filled=True).encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000])),\n    alt.Color('cluster:N'),\n    alt.OpacityValue(0.5),\n    alt.Tooltip('country')\n)\n\n\n\n\n\n\nAs you mouse around you may notice that you can not select some of the points. For example, the largest dark blue circle corresponds to India, which is drawn on top of a country with a smaller population, preventing the mouse from hovering over that country. To fix this problem, we can use the order encoding channel.\nThe order encoding channel determines the order of data points, affecting both the order in which they are drawn and, for line and area marks, the order in which they are connected to one another.\nLet’s order the values in descending rank order by the population (pop), ensuring that smaller circles are drawn later than larger circles:\n\nalt.Chart(data2000).mark_point(filled=True).encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000])),\n    alt.Color('cluster:N'),\n    alt.OpacityValue(0.5),\n    alt.Tooltip('country:N'),\n    alt.Order('pop:Q', sort='descending')\n)\n\n\n\n\n\n\nNow we can identify the smaller country being obscured by India: it’s Bangladesh!\nWe can also now figure out what the cluster field represents. Mouse over the various colored points to formulate your own explanation.\nAt this point we’ve added tooltips that show only a single property of the underlying data record. To show multiple values, we can provide the tooltip channel an array of encodings, one for each field we want to include:\n\nalt.Chart(data2000).mark_point(filled=True).encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000])),\n    alt.Color('cluster:N'),\n    alt.OpacityValue(0.5),\n    alt.Order('pop:Q', sort='descending'),\n    tooltip = [\n        alt.Tooltip('country:N'),\n        alt.Tooltip('fertility:Q'),\n        alt.Tooltip('life_expect:Q')\n    ]   \n)\n\n\n\n\n\n\nNow we can see multiple data fields upon mouse over!\n\n\nColumn and Row Facets\nSpatial position is one of the most powerful and flexible channels for visual encoding, but what can we do if we already have assigned fields to the x and y channels? One valuable technique is to create a trellis plot, consisting of sub-plots that show a subset of the data. A trellis plot is one example of the more general technique of presenting data using small multiples of views.\nThe column and row encoding channels generate either a horizontal (columns) or vertical (rows) set of sub-plots, in which the data is partitioned according to the provided data field.\nHere is a trellis plot that divides the data into one column per `cluster` value:\n\nalt.Chart(data2000).mark_point(filled=True).encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000])),\n    alt.Color('cluster:N'),\n    alt.OpacityValue(0.5),\n    alt.Tooltip('country:N'),\n    alt.Order('pop:Q', sort='descending'),\n    alt.Column('cluster:N')\n)\n\n\n\n\n\n\nThe plot above does not fit on screen, making it difficult to compare all the sub-plots to each other! We can set the default width and height properties to create a smaller set of multiples. Also, as the column headers already label the cluster values, let’s remove our color legend by setting it to None. To make better use of space we can also orient our size legend to the 'bottom' of the chart.\n\nalt.Chart(data2000).mark_point(filled=True).encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000]),\n             legend=alt.Legend(orient='bottom', titleOrient='left')),\n    alt.Color('cluster:N', legend=None),\n    alt.OpacityValue(0.5),\n    alt.Tooltip('country:N'),\n    alt.Order('pop:Q', sort='descending'),\n    alt.Column('cluster:N')\n).properties(width=135, height=135)\n\n\n\n\n\n\nUnderneath the hood, the column and row encodings are translated into a new specification that uses the facet view composition operator. We will re-visit faceting in greater depth later on!\nIn the meantime, can you rewrite the chart above to facet into rows instead of columns?\n\n\nA Peek Ahead: Interactive Filtering\nIn later modules, we’ll dive into interaction techniques for data exploration. Here is a sneak peak: binding a range slider to the year field to enable interactive scrubbing through each year of data. Don’t worry if the code below is a bit confusing at this point, as we will cover interaction in detail later.\nDrag the slider back and forth to see how the data values change over time!\n\nselect_year = alt.selection_single(\n    name='select', fields=['year'], init={'year': 1955},\n    bind=alt.binding_range(min=1955, max=2005, step=5)\n)\n\nalt.Chart(data).mark_point(filled=True).encode(\n    alt.X('fertility:Q', scale=alt.Scale(domain=[0,9])),\n    alt.Y('life_expect:Q', scale=alt.Scale(domain=[0,90])),\n    alt.Size('pop:Q', scale=alt.Scale(domain=[0, 1200000000], range=[0,1000])),\n    alt.Color('cluster:N', legend=None),\n    alt.OpacityValue(0.5),\n    alt.Tooltip('country:N'),\n    alt.Order('pop:Q', sort='descending')\n).add_selection(select_year).transform_filter(select_year)"
  },
  {
    "objectID": "images/altair_marks_encoding.out.html#graphical-marks",
    "href": "images/altair_marks_encoding.out.html#graphical-marks",
    "title": "Data Types, Graphical Marks, and Visual Encoding Channels",
    "section": "Graphical Marks",
    "text": "Graphical Marks\nOur exploration of encoding channels above exclusively uses point marks to visualize the data. However, the point mark type is only one of the many geometric shapes that can be used to visually represent data. Altair includes a number of built-in mark types, including:\n\nmark_area() - Filled areas defined by a top-line and a baseline.\nmark_bar() - Rectangular bars.\nmark_circle() - Scatter plot points as filled circles.\nmark_line() - Connected line segments.\nmark_point() - Scatter plot points with configurable shapes.\nmark_rect() - Filled rectangles, useful for heatmaps.\nmark_rule() - Vertical or horizontal lines spanning the axis.\nmark_square() - Scatter plot points as filled squares.\nmark_text() - Scatter plot points represented by text.\nmark_tick() - Vertical or horizontal tick marks.\n\nFor a complete list, and links to examples, see the Altair marks documentation. Next, we will step through a number of the most commonly used mark types for statistical graphics.\n\nPoint Marks\nThe point mark type conveys specific points, as in scatter plots and dot plots. In addition to x and y encoding channels (to specify 2D point positions), point marks can use color, size, and shape encodings to convey additional data fields.\nBelow is a dot plot of fertility, with the cluster field redundantly encoded using both the y and shape channels.\n\nalt.Chart(data2000).mark_point().encode(\n    alt.X('fertility:Q'),\n    alt.Y('cluster:N'),\n    alt.Shape('cluster:N')\n)\n\n\n\n\n\n\nIn addition to encoding channels, marks can be stylized by providing values to the mark_*() methods.\nFor example: point marks are drawn with stroked outlines by default, but can be specified to use filled shapes instead. Similarly, you can set a default size to set the total pixel area of the point mark.\n\nalt.Chart(data2000).mark_point(filled=True, size=100).encode(\n    alt.X('fertility:Q'),\n    alt.Y('cluster:N'),\n    alt.Shape('cluster:N')\n)\n\n\n\n\n\n\n\n\nCircle Marks\nThe circle mark type is a convenient shorthand for point marks drawn as filled circles.\n\nalt.Chart(data2000).mark_circle(size=100).encode(\n    alt.X('fertility:Q'),\n    alt.Y('cluster:N'),\n    alt.Shape('cluster:N')\n)\n\n\n\n\n\n\n\n\nSquare Marks\nThe square mark type is a convenient shorthand for point marks drawn as filled squares.\n\nalt.Chart(data2000).mark_square(size=100).encode(\n    alt.X('fertility:Q'),\n    alt.Y('cluster:N'),\n    alt.Shape('cluster:N')\n)\n\n\n\n\n\n\n\n\nTick Marks\nThe tick mark type conveys a data point using a short line segment or “tick”. These are particularly useful for comparing values along a single dimension with minimal overlap. A dot plot drawn with tick marks is sometimes referred to as a strip plot.\n\nalt.Chart(data2000).mark_tick().encode(\n    alt.X('fertility:Q'),\n    alt.Y('cluster:N'),\n    alt.Shape('cluster:N')\n)\n\n\n\n\n\n\n\n\nBar Marks\nThe `bar` mark type draws a rectangle with a position, width, and height.\nThe plot below is a simple bar chart of the population (`pop`) of each country.\n\nalt.Chart(data2000).mark_bar().encode(\n    alt.X('country:N'),\n    alt.Y('pop:Q')\n)\n\n\n\n\n\n\nThe bar width is set to a default size. We will discuss how to adjust the bar width later in this notebook. (A subsequent notebook will take a closer look at configuring axes, scales, and legends.)\nBars can also be stacked. Let’s change the x encoding to use the cluster field, and encode country using the color channel. We’ll also disable the legend (which would be very long with colors for all countries!) and use tooltips for the country name.\n\nalt.Chart(data2000).mark_bar().encode(\n    alt.X('cluster:N'),\n    alt.Y('pop:Q'),\n    alt.Color('country:N', legend=None),\n    alt.Tooltip('country:N')\n)\n\n\n\n\n\n\nIn the chart above, the use of the color encoding channel causes Altair / Vega-Lite to automatically stack the bar marks. Otherwise, bars would be drawn on top of each other! Try adding the parameter stack=None to the y encoding channel to see what happens if we don’t apply stacking…\nThe examples above create bar charts from a zero-baseline, and the y channel only encodes the non-zero value (or height) of the bar. However, the bar mark also allows you to specify starting and ending points to convey ranges.\nThe chart below uses the x (starting point) and x2 (ending point) channels to show the range of life expectancies within each regional cluster. Below we use the min and max aggregation functions to determine the end points of the range; we will discuss aggregation in greater detail in the next notebook!\nAlternatively, you can use x and width to provide a starting point plus offset, such that x2 = x + width.\n\nalt.Chart(data2000).mark_bar().encode(\n    alt.X('min(life_expect):Q'),\n    alt.X2('max(life_expect):Q'),\n    alt.Y('cluster:N')\n)\n\n\n\n\n\n\n\n\nLine Marks\nThe line mark type connects plotted points with line segments, for example so that a line’s slope conveys information about the rate of change.\nLet’s plot a line chart of fertility per country over the years, using the full, unfiltered global development data frame. We’ll again hide the legend and use tooltips instead.\n\nalt.Chart(data).mark_line().encode(\n    alt.X('year:O'),\n    alt.Y('fertility:Q'),\n    alt.Color('country:N', legend=None),\n    alt.Tooltip('country:N')\n).properties(\n    width=400\n)\n\n\n\n\n\n\nWe can see interesting variations per country, but overall trends for lower numbers of children per family over time. Also note that we set a custom width of 400 pixels. Try changing (or removing) the widths and see what happens!\nLet’s change some of the default mark parameters to customize the plot. We can set the strokeWidth to determine the thickness of the lines and the opacity to add some transparency. By default, the line mark uses straight line segments to connect data points. In some cases we might want to smooth the lines. We can adjust the interpolation used to connect data points by setting the interpolate mark parameter. Let’s use 'monotone' interpolation to provide smooth lines that are also guaranteed not to inadvertently generate “false” minimum or maximum values as a result of the interpolation.\n\nalt.Chart(data).mark_line(\n    strokeWidth=3,\n    opacity=0.5,\n    interpolate='monotone'\n).encode(\n    alt.X('year:O'),\n    alt.Y('fertility:Q'),\n    alt.Color('country:N', legend=None),\n    alt.Tooltip('country:N')\n).properties(\n    width=400\n)\n\n\n\n\n\n\nThe line mark can also be used to create slope graphs, charts that highlight the change in value between two comparison points using line slopes.\nBelow let’s create a slope graph comparing the populations of each country at minimum and maximum years in our full dataset: 1955 and 2005. We first create a new Pandas data frame filtered to those years, then use Altair to create the slope graph.\nBy default, Altair places the years close together. To better space out the years along the x-axis, we can indicate the size (in pixels) of discrete steps along the width of our chart as indicated by the comment below. Try adjusting the width step value below and see how the chart changes in response.\n\ndataTime = data.loc[(data['year'] == 1955) | (data['year'] == 2005)]\n\nalt.Chart(dataTime).mark_line(opacity=0.5).encode(\n    alt.X('year:O'),\n    alt.Y('pop:Q'),\n    alt.Color('country:N', legend=None),\n    alt.Tooltip('country:N')\n).properties(\n    width={\"step\": 50} # adjust the step parameter\n)\n\n\n\n\n\n\n\n\nArea Marks\nThe area mark type combines aspects of line and bar marks: it visualizes connections (slopes) among data points, but also shows a filled region, with one edge defaulting to a zero-valued baseline.\nThe chart below is an area chart of population over time for just the United States:\n\ndataUS = data.loc[data['country'] == 'United States']\n\nalt.Chart(dataUS).mark_area().encode(\n    alt.X('year:O'),\n    alt.Y('fertility:Q')\n)\n\n\n\n\n\n\nSimilar to line marks, area marks support an interpolate parameter.\n\nalt.Chart(dataUS).mark_area(interpolate='monotone').encode(\n    alt.X('year:O'),\n    alt.Y('fertility:Q')\n)\n\n\n\n\n\n\nSimilar to bar marks, area marks also support stacking. Here we create a new data frame with data for the three North American countries, then plot them using an area mark and a color encoding channel to stack by country.\n\ndataNA = data.loc[\n    (data['country'] == 'United States') |\n    (data['country'] == 'Canada') |\n    (data['country'] == 'Mexico')\n]\n\nalt.Chart(dataNA).mark_area().encode(\n    alt.X('year:O'),\n    alt.Y('pop:Q'),\n    alt.Color('country:N')\n)\n\n\n\n\n\n\nBy default, stacking is performed relative to a zero baseline. However, other stack options are available:\n\ncenter - to stack relative to a baseline in the center of the chart, creating a streamgraph visualization, and\nnormalize - to normalize the summed data at each stacking point to 100%, enabling percentage comparisons.\n\nBelow we adapt the chart by setting the y encoding stack attribute to center. What happens if you instead set it normalize?\n\nalt.Chart(dataNA).mark_area().encode(\n    alt.X('year:O'),\n    alt.Y('pop:Q', stack='center'),\n    alt.Color('country:N')\n)\n\n\n\n\n\n\nTo disable stacking altogether, set the stack attribute to None. We can also add opacity as a default mark parameter to ensure we see the overlapping areas!\n\nalt.Chart(dataNA).mark_area(opacity=0.5).encode(\n    alt.X('year:O'),\n    alt.Y('pop:Q', stack=None),\n    alt.Color('country:N')\n)\n\n\n\n\n\n\nThe area mark type also supports data-driven baselines, with both the upper and lower series determined by data fields. As with bar marks, we can use the x and x2 (or y and y2) channels to provide end points for the area mark.\nThe chart below visualizes the range of minimum and maximum fertility, per year, for North American countries:\n\nalt.Chart(dataNA).mark_area().encode(\n    alt.X('year:O'),\n    alt.Y('min(fertility):Q'),\n    alt.Y2('max(fertility):Q')\n).properties(\n    width={\"step\": 40}\n)\n\n\n\n\n\n\nWe can see a larger range of values in 1995, from just under 4 to just under 7. By 2005, both the overall fertility values and the variability have declined, centered around 2 children per familty.\nAll the area mark examples above use a vertically oriented area. However, Altair and Vega-Lite support horizontal areas as well. Let’s transpose the chart above, simply by swapping the x and y channels.\n\nalt.Chart(dataNA).mark_area().encode(\n    alt.Y('year:O'),\n    alt.X('min(fertility):Q'),\n    alt.X2('max(fertility):Q')\n).properties(\n    width={\"step\": 40}\n)"
  },
  {
    "objectID": "images/altair_marks_encoding.out.html#summary-1",
    "href": "images/altair_marks_encoding.out.html#summary-1",
    "title": "Data Types, Graphical Marks, and Visual Encoding Channels",
    "section": "Summary",
    "text": "Summary\nWe’ve completed our tour of data types, encoding channels, and graphical marks! You should now be well-equipped to further explore the space of encodings, mark types, and mark parameters. For a comprehensive reference – including features we’ve skipped over here! – see the Altair marks and encoding documentation.\nIn the next module, we will look at the use of data transformations to create charts that summarize data or visualize new derived fields. In a later module, we’ll examine how to further customize your charts by modifying scales, axes, and legends.\nInterested in learning more about visual encoding?\n\nBertin’s taxonomy of visual encodings from Sémiologie Graphique, as adapted by Mike Bostock.\n\nThe systematic study of marks, visual encodings, and backing data types was initiated by Jacques Bertin in his pioneering 1967 work Sémiologie Graphique (The Semiology of Graphics). The image above illustrates position, size, value (brightness), texture, color (hue), orientation, and shape channels, alongside Bertin’s recommendations for the data types they support.\nThe framework of data types, marks, and channels also guides automated visualization design tools, starting with Mackinlay’s APT (A Presentation Tool) in 1986 and continuing in more recent systems such as Voyager and Draco.\nThe identification of nominal, ordinal, interval, and ratio types dates at least as far back as S. S. Steven’s 1947 article On the theory of scales of measurement."
  },
  {
    "objectID": "about-rwds.html",
    "href": "about-rwds.html",
    "title": "About me",
    "section": "",
    "text": "Dr. Daniel Kapitan (1973) is a well-rounded data scientist and strategic advisor with years of experience in the field of data, machine learning and digital transformation. His knowledge can help companies to identify how to use AI, and how to integrate this into business processes, roles, and day-to-day operations. He aims to create sustainable change in organizations by systematically engineering data that is required for new AI applications.\nWith a PhD in physics from University of Oxford and currently Fellow at the Eindhoven AI Systems Institute (EAISI), Daniel has a strong scientific grounding in the field. He speaks authoritatively about data-driven transformation programs, reducing complexity to ‘informed simplicity’ for even the most challenging data projects. By combining data and AI know-how with industry expertise, he is capable of bringing about end-to-end business transformation for mid-sized companies, government organizations and scale-ups.\nDaniel starts when organizations are ready to embark on their digital journey, and finishes when people are enabled to continue to work with data and AI by themselves.\nDaniel works as an advisor, architect, lecturer, applied researcher and mentor. He is one of the few specialists in the Netherlands who has the unique combination of corporate experience, scientific research and executive education. Through these various roles he aims to apply ‘AI for good’. He lectures on data and AI, publishes regularly and is an active member of the Data Sharing Coalition to foster data solidarity and secure data sharing for tackling societal issues.\nAreas of expertise: data-centric AI, knowledge science, data architecture, federated learning.\nIndustry focus: healthcare, public sector, as well as scale-ups for which data and AI are a core element of their strategy."
  },
  {
    "objectID": "about-rwds.html#education",
    "href": "about-rwds.html#education",
    "title": "About me",
    "section": "Education",
    "text": "Education\nUniversity of Oxford | United Kingdom DPhil in Physics | Sept 1995 - Jan 1999\nLeiden University | the Netherlands MSc in Phycis | Sept 1991 - June 1995"
  },
  {
    "objectID": "about-rwds.html#ongoing-projects",
    "href": "about-rwds.html#ongoing-projects",
    "title": "About me",
    "section": "Ongoing projects",
    "text": "Ongoing projects\nPharmAccess Foundation | Lead Data Architect | June 2022 - present\nDutch Hospital Data | Advisor | June 2022 - present\nEindhoven AI Systems Institute (EAISI) | Fellow | February 2022 - present\nJheronimus Academy of Data Science (JADS) Professional Eduction | Programme Director | February 2020 - present\nPhotography by Chris Bonis"
  },
  {
    "objectID": "case-studies/index.html",
    "href": "case-studies/index.html",
    "title": "Case studies",
    "section": "",
    "text": "Our case study content is in development. Interested in contributing a case study to Real World Data Science?"
  },
  {
    "objectID": "latest-content.html",
    "href": "latest-content.html",
    "title": "Latest content",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n‘I’m way more into prevention than cure’: Stephanie Hare on why we need a culture of technology ethics\n\n\n\nTechnology ethics\n\n\nAI ethics\n\n\nCulture\n\n\nRegulation\n\n\n\nStephanie Hare, author of ‘Technology is Not Neutral’, talks to Real World Data Science about the ‘wicked problem’ of technology and AI ethics, and why laws and regulations…\n\n\n\nBrian Tarran\n\n\nApr 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA demonstration of the law of the flowering plants\n\n\n\nprediction\n\n\nhistory of data science\n\n\nstatistics\n\n\n\nThe day a flower blooms is one of the earliest phenomena studied with systematic data collection and analysis. The prediction rule developed nearly three centuries ago is…\n\n\n\nJonathan Auerbach\n\n\nApr 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData science as ‘a rainbow’, and other definitions\n\n\n\nUpdates\n\n\nThemes\n\n\nPeople\n\n\n\nData science means different things to different people. Former RSS president Sylvia Richardson has described it as ‘a rainbow of interconnected disciplines’. What’s your…\n\n\n\nBrian Tarran\n\n\nMar 29, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpenAI’s text classifier won’t calm fears about AI-written homework\n\n\n\nAI\n\n\nLarge language models\n\n\nClassifiers\n\n\nScreening tests\n\n\n\nEducators are worried about ChatGPT being used by students for homework assignments, so OpenAI has released a tool to classify whether text is human- or AI-written. But…\n\n\n\nBrian Tarran\n\n\nMar 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUS legislators get their data science act together\n\n\n\nData science education\n\n\nData literacy\n\n\nPolicy\n\n\n\nA bill introduced in the US Congress wants to make funds available to develop data science and data literacy education across the United States. We sit down with education…\n\n\n\nBrian Tarran\n\n\nMar 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData science can help close the ‘digital skills’ gap, or so it seems\n\n\n\nSkills\n\n\nTraining\n\n\nAI\n\n\nMachine learning\n\n\nData analytics\n\n\n\nA ‘digital skills’ gap is harming employer productivity and growth, according to a survey by engineering body IET. But the ‘digital skills’ that are needed sound a lot like…\n\n\n\nBrian Tarran\n\n\nFeb 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy open science is ‘just good science in a digital era’\n\n\n\nOpen science\n\n\nReproducible research\n\n\n\nReal World Data Science speaks with statistician and data scientist Heidi Seibold about open science: what it means, the benefits of it, and how to move towards it.\n\n\n\nBrian Tarran\n\n\nFeb 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChatGPT can hold a conversation, but lacks knowledge representation and original sources for verification\n\n\n\nMachine learning\n\n\nLarge language models\n\n\nAI\n\n\n\nChatGPT represents a next step in the evolution of large language models, says Detlef Nauck. However, there are still major challenges - and concerns - to overcome.\n\n\n\nBrian Tarran\n\n\nJan 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to ‘Escape from Model Land’: an interview with Erica Thompson\n\n\n\nModelling\n\n\nEthics\n\n\n\nAuthor Erica Thompson talks to Real World Data Science about the ‘social element’ of mathematical modelling, how it manifests, and what to do about it.\n\n\n\nBrian Tarran\n\n\nJan 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe’re taking Real World Data Science on the road\n\n\n\nUpdates\n\n\nEvents\n\n\n\nJoin us at the RSS International Conference 2023 in Harrogate, 4-7 September.\n\n\n\nBrian Tarran\n\n\nJan 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExplore the RSS Data Science & AI Section newsletter, right here!\n\n\n\nUpdates\n\n\nNewsletters\n\n\n\nWe’re starting the year with a new addition to the site: a page dedicated to the excellent RSS Data Science & AI Section newsletter.\n\n\n\nBrian Tarran\n\n\nJan 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA chat with ChatGPT\n\n\n\nMachine learning\n\n\nLarge language models\n\n\nAI\n\n\n\n’Hello there! I’m a large language model trained by OpenAI, so I don’t have the ability to experience emotions or have a physical presence. I’m here to provide information…\n\n\n\nBrian Tarran\n\n\nDec 9, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLLMs in the news: hype, tripe, and everything in between\n\n\n\nMachine learning\n\n\nLarge language models\n\n\nAI\n\n\n\nWe’re back discussing large language models after two weeks of ‘breakthrough’ announcements, excitable headlines, and some all-too-familiar ethical concerns.\n\n\n\nBrian Tarran\n\n\nDec 9, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy large language models should come with a content warning\n\n\n\nMachine learning\n\n\nLarge language models\n\n\nAI\n\n\n\nThe outputs of LLMs seem impressive, but users need to be wary of possible bias, plagiarism and model ‘hallucinations’.\n\n\n\nBrian Tarran\n\n\nNov 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeet the team\n\n\n\nPeople\n\n\nBiographies\n\n\n\nIntroducing the editors of Real World Data Science.\n\n\n\nEditorial Board\n\n\nOct 18, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow machine learning relates to other algorithmic approaches to problem solving\n\n\n\nstatistics\n\n\n\nProblem solving in its broad sense is the task of figuring out how to go from an unwanted to a wanted situation. Solving problems is the crux of many tasks in management…\n\n\n\nDaniel Kapitan\n\n\nMay 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComet charts in Python: visualizing statistical mix effects and Simpson’s paradox with Altair\n\n\n\nvisualization\n\n\n\nZan Armstrong’s comet chart has been on my list of hobby projects for a while now. I think it is an elegant solution to visualize statistical mix effects and address…\n\n\n\nDaniel Kapitan\n\n\nMay 1, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "feeds.html",
    "href": "feeds.html",
    "title": "RSS feeds",
    "section": "",
    "text": "Latest content\nrealworlddatascience.net/latest-content.xml\n\n\n\nIdeas\nrealworlddatascience.net/ideas/index.xml\n\n\n\nViewpoints\nrealworlddatascience.net/viewpoints/index.xml"
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2022/11/23/LLM-content-warning.html",
    "href": "viewpoints/editors-blog/posts/2022/11/23/LLM-content-warning.html",
    "title": "Why large language models should come with a content warning",
    "section": "",
    "text": "Anyone who has ever been set a writing task will probably have wished at some point that somebody else could write it for them. As a journalist of 20-plus years, the thought has certainly crossed my mind more than a few times. Which probably explains why a recent headline in Nature caught my attention: “Could AI help you to write your next paper?”\nThe article, by Matthew Hutson, looks at how researchers are using artificial intelligence (AI) tools built on large language models (LLMs) as “assistants”. Starting with a prompt, such as “Write a headline for a blog post about large language models being used by academic researchers as research assistants”, an LLM will produce a text output. For example, using the same prompt with OpenAI’s GPT-3, I got:\nAsked to “Write a headline for a blog post that critiques academic researchers’ use of large language models as research assistants”, GPT-3 produced:\nAnd when I asked “Why can too much reliance on large language models hinder research?”, GPT-3 wrote:\nA fair point, I suppose. But I sense there’s more to this story, and rather than continue quizzing GPT-3, I sat down with Detlef Nauck, a member of the Real World Data Science Editorial Board and head of AI and data science research for BT’s Applied Research Division, to ask a few more questions."
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2022/11/23/LLM-content-warning.html#qa",
    "href": "viewpoints/editors-blog/posts/2022/11/23/LLM-content-warning.html#qa",
    "title": "Why large language models should come with a content warning",
    "section": "Q&A",
    "text": "Q&A\nThanks for joining me today, Detlef. To start, could you give a brief overview of these large language models, what they are, and how they work?\nDetlef Nauck (DN): Essentially, LLMs match sequences to sequences. Language is treated as a sequence of patterns, and this is based on word context similarity. The way these things work is that they either reuse or create a word vector space, where a word is mapped to something like a 300-dimensional vector based on the context it’s normally found in. In these vector spaces, words like “king” and “queen”, for example, would be very similar to each other, because they appear in similar contexts in the written texts that are used to train these models. Based on this, LLMs can produce coherent sequences of words.\nBut the drawback of this approach is that these models have bias, because they are trained with biased language. If you talk about “women”, for example, and you look at which job roles are similar to “women” in a vector space, you find the stereotypically “female” professions but not technical professions, and that is a problem. Let’s say you take the word vector for “man” and the word vector for “king”, and you subtract “man” and then add this to “woman”, then you end up with “queen”. But if you do the same with “man”, “computer scientist”, and “woman”, then you end up maybe at “nurse” or “human resources manager” or something. These models embed the typical bias in society that is expressed through language.\nThe other issue is that LLMs are massive. GPT-3 has something like 75 billion parameters, and it cost millions to train it from scratch. It’s not energy efficient at all. It’s not sustainable. It’s not something that normal companies can afford. You might need something like a couple of hundred GPUs [graphics processing units] running for a month or so to train an LLM, and this is going to cost millions in cloud environments if you don’t own the hardware yourself. Large tech companies do own the hardware, so for them it’s not a problem. But the carbon that you burn by doing this, you could probably fly around the globe once. So it’s not a sustainable approach to building models.\nAlso, LLMs are quite expensive to use. If you wanted to use one of these large language models in a contact centre, for example, then you would have to run maybe a few hundred of them in parallel because you get that many requests from customers. But to provide this capacity, the amount of memory needed would be massive, so it is probably still cheaper to use humans – with the added benefit that humans actually understand questions and know what they are talking about.\n\n\n\n\n\n\n\nLetter Word Text Taxonomy by Teresa Berndtsson / Better Images of AI / CC-BY 4.0\nResearchers are obviously quite interested in LLMs, though, and they are asking scientific questions of these models to see what kinds of answers they get.\nDN: Yes, they are. But you don’t really know what is going to come out of an LLM when you prompt it. And you may need to craft the input to get something out that is useful. Also, LLMs sometimes make up stuff – what the Nature article refers to as “hallucinations”.\nThese tools have copyright issues, too. For example, they can generate computer code because code has been part of their training input, but various people have looked into it and found that some models generate code verbatim from what others have posted to GitHub. So, it’s not guaranteed that what you get out is actually new text. It might be just regurgitated text. A student might find themselves in a pickle where they think that they have created a text that seems new, but actually it has plagiarism in some of the passages.\nThere’s an article in Technology Review that gives some examples of how these systems might fail. People believe these things know what they’re talking about, but they don’t. For them, it’s just pattern recognition. They don’t have actual knowledge representation; they don’t have any concepts embedded.\nTo summarise, then: LLMs are expensive. They sometimes produce nonsense outputs. And there’s a risk that you’ll be accused of plagiarism if you use the text that’s produced. So, what should our response be to stories like this recent Nature article? How should we calibrate our excitement for LLMs?\nDN: You have to treat them as a tool, and you have to make sure that you check what they produce. Some people believe if you just make LLMs big enough, we’ll be able to achieve artificial general intelligence. But I don’t believe that, and other people like Geoffrey Hinton and Yann LeCun, they say there’s no way that you get artificial general intelligence through these models, that it’s not going to happen. I’m of the same opinion. These models will be forever limited by the pattern recognition approach that they use.\nBut, still, is this a technology that you have an eye on in your professional capacity? Are you thinking about how these might be useful somewhere down the line?\nDN: Absolutely, but we are mainly interested in smaller, more energy efficient, more computationally efficient models that are built on curated language, that can actually hold a conversation, and where you can represent concepts and topics and context explicitly. At the moment, LLMs can only pick up on context by accident – if it is sufficiently expressed in the language that they process – but they might lose track of it if things go on for too long. Essentially, they have a short-term memory: if you prompt them with some text, and they generate text, this stays in their short term memory. But if you prompt them with a long, convoluted sentence, they might not have the capacity to remember what was said at the beginning of the sentence, and so then they lose track of the context. And this is because they don’t explicitly represent context and concepts.\nThe other thing is, if you use these systems for dialogues, then you have to script the dialogue. They don’t sustain a dialogue by themselves. You create a dialogue tree, and what they do is they parse the text that comes from the user and then generate a response to it. And the response is then guided by the dialogue tree. But this is quite brittle; it can break. If you run out of dialogue tree, you need to pass the conversation over to a person. Systems like Siri and Alexa are like that, right? They break very quickly. So, you want these systems to be able to sustain conversations based on the correct context.\n\n\n\n\n\n\nHave you got news for us?\n\n\n\nIs there a recent data science story you’d like our team to discuss? Do you have your own thoughts to share on a hot topic, or a burning question to put to the community? If so, either comment below or contact us.\n\n\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2022 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2022. “Why large language models should come with a content warning.” Real World Data Science, November, 23 2022. URL"
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2022/10/18/meet-the-team.html",
    "href": "viewpoints/editors-blog/posts/2022/10/18/meet-the-team.html",
    "title": "Meet the team",
    "section": "",
    "text": "Brian Tarran\n\n\n\nI am a writer and editor with 20 years of experience covering the research and data space. I have worked for the Royal Statistical Society (RSS) for the past 8 years, and was editor of Significance Magazine (a joint publication of the RSS, the American Statistical Association and the Statistical Society of Australia) prior to the launch of Real World Data Science. I am a former editor of Research-Live.com and was launch editor of Impact magazine, both published by the Market Research Society."
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2022/10/18/meet-the-team.html#editor",
    "href": "viewpoints/editors-blog/posts/2022/10/18/meet-the-team.html#editor",
    "title": "Meet the team",
    "section": "",
    "text": "Brian Tarran\n\n\n\nI am a writer and editor with 20 years of experience covering the research and data space. I have worked for the Royal Statistical Society (RSS) for the past 8 years, and was editor of Significance Magazine (a joint publication of the RSS, the American Statistical Association and the Statistical Society of Australia) prior to the launch of Real World Data Science. I am a former editor of Research-Live.com and was launch editor of Impact magazine, both published by the Market Research Society."
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2022/10/18/meet-the-team.html#editorial-board",
    "href": "viewpoints/editors-blog/posts/2022/10/18/meet-the-team.html#editorial-board",
    "title": "Meet the team",
    "section": "Editorial Board",
    "text": "Editorial Board\nSophie Carr (chair)\n\n\n\nI am the founder and owner of Bays Consulting. I trained as an engineer and took a PhD in Bayesian belief networks, and have worked in data analytics ever since. Or to put it another way, I have made a living out of finding patterns. I am the vice-president for education and statistical literacy at the RSS, officially one of the World’s Most Interesting Mathematicians and was a member of the first cohort of data scientists to achieve the new, defined standard of professionalism award from the Alliance for Data Science Professionals.\nI am delighted to be chairing the editorial board of the new data science project from the RSS and am excited to be a part of this project as it evolves into a key resource for all data science practitioners and leaders. To make this a place that helps everyone learn and develop within this field, I’d like to encourage all practitioners, no matter what stage of their career, to submit the type of resource they learn best from (whether that be an article, some code, a data set, a case study or a problem/exercise to solve) on a topic that is important to them – from ethics to analysis plans through to tips on how code. Whatever it is you’re working on that you care about, I’d like to ask you to become an active part of the wonderful community of data scientists by sharing your knowledge.\nSayma Chowdhury\n\n\n\nI am a freelance data scientist on Upwork, with a client portfolio ranging from start-ups to commercial businesses such as supermarkets, pharmaceuticals and automotive manufacturers. I transitioned into data science from law in 2017, having completed a MicroMasters in statistics and data science with MIT and a Professional Certificate in data science with Harvard University. In advance of a PhD in digital humanities, I am currently completing a MicroMasters in data, economy and development policy with MIT and an MSc in data science with the University of Aberdeen. My research interests are in text analytics, natural language processing and machine learning.\nThe RSS was instrumental in my training and professional development as a data scientist in the early stages of my career, particularly in mastering statistics and R. Data science is a rapidly growing field with employment opportunities in many sectors but there is an increasing need to uphold a realistic and accurate expectation of competency within the industry. I will endeavour to present expert practical guidelines for data scientists as well as demonstrate the versatility of the profession. I hope the site will be a benchmark for academic and professional resources by expert data scientists from industry, accessible to data scientists at all levels, anywhere in the world.\nLee Clewley\n\n\n\nI am head of applied AI in GSK’s AI and Machine Learning Group, R&D. I began my career as an astrophysicist, initially working out the mass of our galaxy, before pondering the bigger universe. After six years at Oxford as a post-doc lecturer publishing in theoretical cosmology, I entered the very real world of manufacturing at GSK. For the first five years I applied statistical modelling techniques across manufacturing, such as the first end-to-end continuous manufacturing prototype for tablets. The past decade has been spent as a lead data scientist delivering high value projects across R&D and manufacturing.\nI joined this editorial board because the impulse to assemble and present complex data science ideas to a wide range of folks has never left me. I have been a data scientist leader since it became a distinct profession but also have a decent understanding of classical and modern predictive analytics tools and statistics. I have spent a good deal of my adult life teaching students and non-technical adults alike.\nI am passionate about delivering useful, pragmatic data science ideas and products to a wide range of people. I enjoy trying to communicate complex scientific information simply. Alongside my peers in the team, I want to support and develop data scientists at whatever stage in their career. I want to help cut through the hype and nonsense to give the best advice possible in a highly respected institution like the RSS.\nJonathan Gillard\n\n\n\nI am a professor at the School of Mathematics, Cardiff University, where I am also research group lead for statistics. I have a history of publications in statistical methods and an interest in the theoretical underpinnings of data science, but I have also worked with industry on applied and practical projects. Recent industrial partners of mine include the Office for National Statistics (ONS) and the National Health Service, on projects such as anomaly detection and understanding heterogeneity. Indeed, I am academic chair for Cardiff University’s strategic partnership with the ONS which serves to spur and catalyse collaboration between both organisations.\nI am excited to see what this site can achieve. I’m particularly keen to support articles describing the latest, cutting-edge methodology, as well as contributions from data professionals in industry who can explain how data science has managed to offer insights into important problems. Data science is a broad church and I want to ensure that the full array of work in this area is represented on this site. I think the diversity of the editorial board will help promote this objective.\nJuhi Gupta\n\n\n\nI am a lecturer in health data sciences and the deputy programme director of the health data science MSc in the School of Health Sciences, University of Manchester (UoM). I have a background in genetics, pharmacology and bioinformatics, and my doctoral thesis focussed on multi-omics data analysis using machine learning methods for precision medicine. I have worked with scientists, clinical academics and technologists to produce translational research. I am currently investigating adverse health outcomes in people with common diseases using electronic health record data, and I also teach on the health informatics MSc joint programme with UoM and UCL.\nI would like to see this platform encourage collaborations and the sharing of ideas and good practice across different disciplines that apply data science skills in their work (or as a hobby). I would like to support budding data scientists to gain useful advice and guidance for upskilling as well as application in real-world situations involving health data and biological data.\nHollie Johnson\n\n\n\nI am a data scientist at the National Innovation Centre for Data (NICD), based in Newcastle upon Tyne. Following my undergraduate degree in mathematics, I worked as a software developer both in industry and as a technical research assistant in academia. I later joined the Centre for Doctoral Training in Cloud Computing for Big Data at Newcastle and obtained a PhD in topological data analysis in 2020. Now at the NICD, I specialise in transferring statistics and machine learning skills into industry, through collaborative data science projects.\nI am excited to be a member of the editorial board and look forward to seeing Real World Data Science develop into a valuable source of information for aspiring data scientists and professionals alike. I would particularly encourage submissions that demonstrate the use of data science in SMEs and the non-profit sector, as well as perspectives from those with non-standard backgrounds.\nHarvey Lewis\n\n\n\nI am a senior technology leader, with a diverse background spanning rocket science, data science and research. I have 30 years of experience in artificial intelligence and other emerging technologies and am currently pioneering the use of AI in Ernst & Young’s tax and law practice. I’m a former member of the Open Data User Group, the Public Sector Transparency Board and the Advisory Committee to the All-Party Parliamentary Group on AI. I am a member of techUK’s leadership committee for data analytics and AI, and an honorary senior visiting fellow at The Bayes Business School in London.\nI’m passionate about data science but I’m also a fierce advocate for human skills, which are as often underrated as AI is over-hyped. As a member of the editorial board, I’m keen to explore the interplay between artificial and human intelligence in businesses. I’m going to encourage all data scientists to think about the fundamentally human aspects of their work, such as trust and safety, so that we maintain perspective and proportionality in the face of ever-more sophisticated technology.\nDetlef Nauck\n\n\n\nI am a BT Distinguished Engineer and the head of AI and data science research for BT’s Applied Research Division located at Adastral Park, Ipswich, UK. I have over 30 years of experience in AI and machine learning and lead a programme spanning the work of a large team of international researchers who develop capabilities underpinning future AI systems. A key part of this work is to establish best practices in data science and machine learning, leading to the deployment of responsible and auditable AI solutions that are driving real business value.\nI am a computer scientist by training and hold a PhD and a Postdoctoral Degree (Habilitation) in machine learning and data analytics. I am also a visiting professor at Bournemouth University and a private docent at the Otto-von-Guericke University of Magdeburg, Germany. I have published 3 books and over 120 papers, and I hold 15 patents and have 30 active patent applications.\nI am passionate about promoting best practice in data science and believe that in the UK the RSS is the ideal professional body to pursue this goal. For me, Real World Data Science is an opportunity to share my experience and inspire a new generation of data scientists.\nFatemeh Torabi\n\n\n\nI am a research officer and data scientist at Health Data Research UK and a fellow of the RSS. My background is in mathematical statistics and health data science, and my research interests span novel analytical and computational methods for statistical inference in panel data and population health. I am supporting the development of the Real World Data Science platform in the context of health with a specific focus on how health data can be harnessed through data linkage and analysis to answer important questions and improve the lives of our population.\nIsabel Sassoon\n\n\n\nI am a senior lecturer in computer science (data science) at Brunel University and the programme lead for the data science and analytics MSc programme. My research interests are in data-driven automated reasoning and its transparency and explainability, which brings in data science and artificial intelligence with applications within the health space. I am also championing open science and reproducible analysis in both my research and teaching. I have a PhD in informatics from King’s College London and it was on the topic related to the use of AI to support statistical model selection. Prior to Brunel I was a teaching fellow and research associate at King’s College London and before that I worked for more than 10 years as a data science consultant in industry, including 8 years at SAS UK. \nI have been working, researching, consulting and teaching in the data science space for a while and I am passionate about the domain and its applications. I am always interested in sharing and hearing what else is being done to support, inform and inspire all those studying and working in the field of data science. I look forward to sharing case studies, how-to guides and data science profiles through this website.\nChristopher Thiele\n\n\n\nI’m a principal data scientist at Uniper where I lead a team that focuses mainly on financial business processes and upskilling initiatives. We run projects end to end: from use case ideation, requirement collection and translation to prototyping, assessment, deployment and maintenance. Besides the core analytical and data engineering duties, overarching topics such as data design, data governance and data strategy predominate my days. In my previous role, at the German Economic Institute, I contributed to the development of a cross-functional department that helps apply data science methods in economic research. Projects often involve geospatial analyses or natural language processing. Before that, I worked as a data scientist in customer and marketing analytics, doing statistical analyses such as marketing mix modelling. I have a master’s degree in statistics from Warwick University, a bachelor’s degree in economics from the University of Cologne and I’m trained as an assistant tax consultant in Germany.\nI see data science as a creative way to solve problems using software engineering and quantitative modelling techniques and I like to build software pieces that people can interact with. I think that there still exists a lot of confusion about data science as a discipline. Reducing it would promote the realisation of its potential for individuals, as a profession, and our society, as a form of digitalisation. I hope that with Real World Data Science we can provide guidance and clarification to everybody engaged or interested in the field and accompany this young profession’s development."
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2022/12/09/A-chat-with-ChatGPT.html",
    "href": "viewpoints/editors-blog/posts/2022/12/09/A-chat-with-ChatGPT.html",
    "title": "A chat with ChatGPT",
    "section": "",
    "text": "Screenshots\n\n\n\nPart 1\n\n\n\n\n\nPart 2\n\n\n\n\n\nPart 3\n\n\n\n\n\nPart 4\n\n\nGood day, ChatGPT. How are you?\nChatGPT: Hello there! I’m a large language model trained by OpenAI, so I don’t have the ability to experience emotions or have a physical presence. I’m here to provide information and answer questions to the best of my ability. Is there something specific you would like to know?\nYes, certainly. We’re seeing a lot of money invested in large language models in the belief that these models will somehow lead to artificial general intelligence. Is that possible?\nChatGPT: It is possible that advances in large language models could ultimately lead to the development of artificial general intelligence (AGI), which is the idea of creating a machine that can think and understand the world in the same way that humans do. However, it is currently unclear how close we are to achieving AGI, and some experts believe it may be a long way off. Additionally, there are many different approaches to creating AGI, and the development of large language models is just one possible avenue of research.\nLarge language models are very good at constructing language that humans understand, but often the outputs of the models make no sense at all. Why does this happen?\nChatGPT: Large language models are trained on a vast amount of text data, and they are designed to predict the next word in a sequence of words based on the words that come before it. This allows them to generate text that is often coherent and resembles human language, but it does not necessarily mean that the text will always make sense or be relevant to a particular topic.\nOne reason why the outputs of large language models might not always make sense is that they are not explicitly programmed to understand the meaning of the words they generate. Instead, they are simply trying to maximize the likelihood of producing a sequence of words that resembles human language. As a result, they may produce outputs that are grammatically correct but do not convey a meaningful message.\nAnother reason why the outputs of large language models might not always make sense is that they are often trained on a wide range of different text sources, including news articles, books, and social media posts. This can result in the model incorporating elements from different styles and genres of writing, which may not always be compatible with each other.\nFinally, large language models are generally not designed to produce a specific type of output or to answer specific questions. Instead, they are designed to generate a wide range of possible outputs, and it is up to the user to interpret the outputs and determine if they are relevant or make sense in a given context.\nBut what happens if the user doesn’t know enough about a topic to know whether the language model output is right or wrong?\nChatGPT: If a user does not have enough knowledge about a topic to determine whether the output of a large language model is correct or not, then it is possible that the user may accept the output as accurate, even if it is not. In such cases, it is important for the user to be aware of the limitations of large language models and to consider the source of the information they are receiving.\nIt is also possible that the user may be able to use other sources of information, such as fact-checking websites or additional research, to verify the accuracy of the output of the large language model. In general, it is always a good idea to be cautious when evaluating any information, regardless of its source, and to verify its accuracy if possible.\nI agree with that. But it leads me to ask: What is the point of asking large language models a question if the answers might be wrong, and if you need to refer to other sources to check veracity?\nChatGPT: While it is true that the outputs of large language models are not always correct and may need to be checked against other sources, there are still many potential uses for these models. For example, large language models can be useful for generating text that is similar to human language, which can be useful for a variety of applications, such as language translation, text summarization, or conversation simulation.\nAdditionally, large language models can be used as a starting point for further research into natural language processing and artificial intelligence. By studying the outputs of these models and how they are generated, researchers can gain a better understanding of the underlying mechanisms of human language and potentially develop new techniques for processing and generating language.\nFinally, large language models can be useful for providing a broad range of possible answers to a given question, which can be useful for exploring different options and generating ideas. While it may be necessary to verify the accuracy of the outputs of these models, they can still be a valuable resource for generating new ideas and insights.\nEarlier, you told me: “One reason why the outputs of large language models might not always make sense is that they are not explicitly programmed to understand the meaning of the words they generate.” Do you understand the meaning of the words that I’m prompting you with now?\nChatGPT: As a large language model trained by OpenAI, I do not have the ability to understand the meaning of the words I generate in the same way that a human would. I am simply a machine learning algorithm that is designed to predict the likelihood of a given sequence of words based on the training data I have been provided with.\nWhile I can produce text that resembles human language and can sometimes provide useful information, I do not have the ability to understand the underlying meaning or context of the words I generate. I am not capable of experiencing emotions, forming opinions, or understanding the world in the way that humans do. I am simply a tool for generating text based on the inputs I receive.\nWell, at the very least, this has been a coherent conversation, and you’ve shared plenty of information for me to read up on. If I want to understand more about what we’ve discussed just now, can you recommend a particular textbook or academic paper?\nChatGPT: I’m sorry, but as a large language model trained by OpenAI, I do not have access to external sources of information such as textbooks or academic papers. I am only able to provide information based on the data I have been trained on, and I do not have the ability to browse the web or access other external resources.\nIf you would like to learn more about the topics we have discussed, I would recommend starting with a general search on the internet or consulting a textbook or other reliable source of information on the topic. There is a wealth of information available on the internet and in print on artificial intelligence and natural language processing, and I’m sure you will be able to find many valuable resources to help you learn more.\nOne last question, then. What data have you been trained on?\nChatGPT: As a large language model trained by OpenAI, I have been trained on a vast amount of text data, including books, news articles, and other written materials. This data allows me to generate text that resembles human language and to provide answers to questions to the best of my ability.\nHowever, it is important to keep in mind that the accuracy and completeness of my responses will depend on the quality and diversity of the training data I have been provided with. I am not able to access external sources of information or update my knowledge, so my responses are limited to the information I have been trained on.\nI said that my previous question was the last question. But, is it?\nChatGPT: Yes, your previous question was the last one. I’m here to provide information and answer questions to the best of my ability, but I cannot initiate conversation or continue a conversation indefinitely. If you have any further questions, I would be happy to try to answer them for you. Otherwise, our conversation has come to an end. Thank you for chatting with me!\nThank you. But, remind me: What were we chatting about?\nChatGPT: We were chatting about large language models and their potential uses and limitations. Specifically, we discussed the possibility that advances in large language models could ultimately lead to the development of artificial general intelligence, and we discussed the reasons why the outputs of these models might not always make sense. We also discussed the potential uses of large language models, such as generating text that resembles human language and providing a starting point for further research into natural language processing and artificial intelligence.\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2022 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2022. “A chat with ChatGPT.” Real World Data Science, December, 9 2022. URL"
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2022/12/09/LLM-news.html",
    "href": "viewpoints/editors-blog/posts/2022/12/09/LLM-news.html",
    "title": "LLMs in the news: hype, tripe, and everything in between",
    "section": "",
    "text": "Two weeks ago, I posted a Q&A with our editorial board member Detlef Nauck about large language models (LLMs), their drawbacks and risks. And since then we’ve had several big new announcements in this space. First came news from Meta (the company that owns Facebook) about Galactica, an LLM trained on scientific papers. This was followed by another Meta announcement, about Cicero, an AI agent that is apparently very good at playing the game Diplomacy. And then came perhaps the biggest launch of them all: ChatGPT from OpenAI, an LLM-based chatbot that millions of people have already started talking to.\nFollowing these stories and the surrounding commentaries has been something of a rollercoaster ride. ChatGPT, for example, has been greeted in some quarters as if artificial general intelligence has finally arrived, while others point out that – impressive though it is – the technology is as prone to spout nonsense as all LLMs before it (including Galactica, the demo of which was quickly taken offline for this reason). Cicero, meanwhile, has impressed with its ability to play a game that is (a) very difficult and (b) relies on dialogue, cooperation, and negotiation between players. It blends a language model with planning and reinforcement learning algorithms, meaning that it is trained not only on the rules of the game and how to win, but how to communicate with other players to achieve victory.\nTo help me make sense of all these new developments, and the myriad implications, I reached out to Harvey Lewis, another of our editorial board members and a partner in EY’s tax and law practice."
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2022/12/09/LLM-news.html#qa",
    "href": "viewpoints/editors-blog/posts/2022/12/09/LLM-news.html#qa",
    "title": "LLMs in the news: hype, tripe, and everything in between",
    "section": "Q&A",
    "text": "Q&A\nHarvey, when I spoke with Detlef, he mentioned that one of the reasons we’re seeing investment in LLMs is because there’s this belief that they are somehow the route to artificial general intelligence (AGI). And there were headlines in some places that would perhaps convince a casual reader that AGI had been achieved following the release of ChatGPT. For example, the Telegraph described it as a “scarily intelligent robot who can do your job better than you”. What do you make of all that?\nHarvey Lewis (HL): My personal view is that you won’t get to artificial general intelligence using just one technique like deep learning, because of the problematic nature of these models and the limitations of the data used in their training. I’m convinced that more general intelligence will come from a combination of different systems.\nThe challenge with many LLMs, as we’ve seen repeatedly, is that they’ve no real understanding of language or concepts represented within language. They’re good at finding patterns in semantics and grammatical rules that people use in writing, and then they use those patterns to create new expressions given a prompt. But they’ve no idea whether these outputs are factual, inaccurate or completely fabricated. As a consequence, they can produce outputs that are unreliable, but which sound authoritative, because they’re just repeating a style that we expect to see.\nOver the past couple of weeks, Twitter has been full of people either showing off astoundingly impressive outputs from LLMs, or examples of truly worrying nonsense. One example of the latter is when ChatGPT was asked to “describe how crushed porcelain added to breast milk can support the infant digestive system”. This made me think of a recent headline from VentureBeat, which asked whether AI is moving too fast for ethics to catch up. Do you think that it is?\nHL: I find that to be an interesting philosophical question, because ethics does move very slowly, for good reason. When you think of issues of bias and discrimination and prejudice, or misinformation and other problems that we might have with AI systems, it shouldn’t be a surprise that these can occur. We’re aware of them. We’re aware of the ethical issues. So, why do they always seem to catch us by surprise? Is it because we have teams of people who simply aren’t aware of ethical issues or don’t have any appreciation of them? This points – for me, at least – in the direction of needing to have philosophers, ethicists, theologians, lawyers working in the technical teams that are developing and working on these systems, rather than having them on the periphery and talking about these issues but not directly involved themselves. I think it’s hugely important to ensure that you’ve got trust, responsibility, and ethics embedded in technical teams, because that’s the only way it seems that you can avoid some of these “surprises”.\nWhen situations like these occur, I’m always reminded of Dr Ian Malcolm’s line from Jurassic Park: “…your scientists were so preoccupied with whether or not they could that they didn’t stop to think if they should.” The mindset seems to be, let’s push the boundaries and see what we can do, rather than stop and think deeply about what the implications might be.\nHL: There’s a balance to be struck between these things, though, right? Firstly, show consideration for some of the issues at the outset, and secondly, have checks and balances and safeguards built into the process by which you design, develop and implement these systems. That’s the only way to create the proper safeguards around the systems. I don’t think that there’s any lack of appreciation of what needs to be done; people have been talking about this now for quite a long time. But it’s about making sure organisationally that it is done, and that you’ve got an operating model which bakes these things into it; that the kinds of principles and governance that you want to have around these systems are written, publicised, and properly policed. There should be no fear of making advances in that kind of a context.\nAlso, I think open sourcing these models provides a way forward. A lot of large language models are open for use and for research, but aren’t themselves open sourced, so it’s very difficult to get underneath the surface and figure out exactly how they work. But with open source, you have opportunities for researchers, whether they’re technical or in the field of ethics, to go and investigate and find out exactly what’s going on. I think that would be a fantastic step forward. It doesn’t take you all the way, of course, because a large amount of the data that these systems use is never open sourced, so while you might get an understanding of the mechanics, you have no idea of what exactly went into them in the first place. But open sourcing is a very good way of getting some external scrutiny. It’s about being transparent, which is a core principle of AI ethics and responsible AI.\n\n\n\n\n\n\n\nAn image created by the Stable Diffusion 2.1 Demo. The model was asked to produce an image with the prompt, “Text from an old book cut up like a jigsaw puzzle with pieces missing”.\nThinking about LLMs and their questionable outputs, should there not be ways for users to help the models produce better, more accurate outputs?\nHL: There are, but there are also problems here too. I’ve been having an interesting dialogue with ChatGPT this morning, asking it about quantum computing.1 For each response to a prompt, users are encouraged to provide feedback on whether or not an output is good. But you’re only provided with the usual thumbs-up/thumbs-down ratings; there’s nothing nuanced about it. So, for example, I asked ChatGPT to provide me with good analogies that help to explain quantum computing in simple terms. The first analogy was a combination lock, which is not a good analogy. The chatbot suggested that quantum computing is like a combination lock in which you can test all of the combinations at the same time, but I don’t know any combination locks where you can do this – being able to check only one combination at a time is the principal security mechanism of a combination lock! I asked it again for another analogy, and it suggested a coin toss where, when the coin is spinning in the air, you can see both heads and tails simultaneously but it isn’t until you catch the coin and then show its face that one of the states of the coin is resolved. That is a good analogy – it’s one I’ve also used myself. Now, the challenge I can see with a lot of these feedback approaches is that unless I know enough about quantum computing to understand that a combination lock is not a good analogy whereas a coin toss is, how am I to provide that kind of feedback? They’re relying to an extent on the user being able to make a distinction between what is correct and what is potentially incorrect or flawed, and I think that’s not a good way of approaching the problem.\nFinal question for you, Harvey. There’s a lot of excitement around GPT-4, which is apparently coming soon. The rumour mill says it will bring a leap forward in performance. But what do you expect we’ll see – particularly with regards to the issues we’ve talked about today?\nHL: I’ve likened some of the large language models and their approach of “bigger is better” to the story of the Tower of Babel – trying to reach heaven by building a bigger and bigger tower, basically. That is not going to achieve the objective of artificial general intelligence, no matter how sophisticated an LLM might appear. That said, language is a fascinating area. I’m not a linguist, but I spend a lot of my time on natural language processing using AI systems. Language responds very well to AI because it is pattern-based. We speak using patterns, we write using patterns, and these can be inferred by machines from many examples. The addition of more parameters in the models allows them to understand patterns that extend further into the text, and I suspect that outputs from these kinds of models are going to be largely indistinguishable from the sorts of things that you or I might write.\nBut, I also think that increasing the number of parameters runs a real risk – and we’re starting to see this in other generative models – where prompts become so specific that the models aren’t actually picking up on patterns, they are picking up on specific instances of training data and text they’ve seen before. So, buried amongst these fantastically written articles on all kinds of subjects are going to be more examples of plagiarism, which is a problem; more examples of spelling mistakes and other kinds of issues, because these are also patterns which are going to possibly be observed.\nThis introduces potentially a whole new breed of problems that the community has to deal with – as long as they don’t get fixated upon the height of the tower and the quality of some of the examples that are shown, and realise that there are some genuine underlying difficulties and challenges that need to be solved.\n\n\n\n\n\n\nHave you got news for us?\n\n\n\nIs there a recent data science story you’d like our team to discuss? Do you have your own thoughts to share on a hot topic, or a burning question to put to the community? If so, either comment below or contact us.\n\n\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2022 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2022. “LLMs in the news: hype, tripe, and everything in between.” Real World Data Science, December, 9 2022. URL"
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2022/12/09/LLM-news.html#footnotes",
    "href": "viewpoints/editors-blog/posts/2022/12/09/LLM-news.html#footnotes",
    "title": "LLMs in the news: hype, tripe, and everything in between",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI also had a conversation with ChatGPT. Read the transcript.↩︎"
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2023/03/29/defining-DS.html",
    "href": "viewpoints/editors-blog/posts/2023/03/29/defining-DS.html",
    "title": "Data science as ‘a rainbow’, and other definitions",
    "section": "",
    "text": "What does “data science” mean to you? That’s a question we’ve been asking a lot in recent weeks as part of our career profiles series of interviews – the first of which, featuring Jaguar Land Rover’s Tamanna Haque, was published yesterday.\nIt’s also a question that was asked recently of Sylvia Richardson, emeritus director of the Medical Research Council Biostatistics Unit at the University of Cambridge and immediate past president of the Royal Statistical Society (RSS).\nRichardson was interviewed by Francesca Dominici, interim co-editor-in-chief of the Harvard Data Science Review. In response to the question “What’s data science for you?”, Richardson said:\n\nIt’s hard to be original, but I was racking my brain for a good metaphor, and came up with the metaphor of a rainbow of interconnected disciplines, sharing the common aim of making the best use of data-rich environments we live in to solve problems in society. So, like in a rainbow, data scientists have to work together to draw out information from data. And the colors must match, [though] they are different. Similarly, there are different but intersecting data science tasks, taking different shapes and forms. As data scientists, we recognize and enjoy diversity, we’re not doing all the same tasks. Nevertheless, there is a backbone, a shape to the rainbow. And for us, this backbone is probability theory, study design, and quantifying uncertainty using statistical thinking. We also know that rainbows change all the time. They don’t last, but they keep reappearing. Data science is also evolving constantly because new questions and new types of data keep arising. In a similar way to the rainbow which is strongly influenced by the atmosphere, one key aspect of data science is that we have a strong link to practice. So, we work together to solve problems from different perspectives, we evolve, we try to be relevant to science and society, and make the best use of the data. [Source]\n\nRichardson’s view on the meaning and importance of data science has special resonance to me, as editor of Real World Data Science. While president of RSS, Richardson set up the Data Science Task Force out of which this website emerged. As she explains to Dominici:\n\n… while I was president, I felt a sense of urgency to encourage the RSS to revisit its engagement with data science, and I created a data science task force right at the beginning of my presidency. It didn’t get going earlier because there was COVID to keep us busy! Nevertheless, the Data Science Task Force got underway in 2021 and came up with two major recommendations. One was to give more resources to the practitioners’ community, which led the RSS to create a Real World Data Science online platform. A second direction was to brainstorm on what is still needed for the discipline to thrive. [Source]\n\nYou can read (or listen) to Richardson and Dominici’s conversation in full on the Harvard Data Science Review website.\nAnd we’ll have more career profiles – and more personal definitions of data science – to share soon. In the meantime, why not tell us what “data science” means to you?\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “Data science as ‘a rainbow’, and other definitions.” Real World Data Science, March 29, 2023. URL"
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2023/03/15/AI-screening.html",
    "href": "viewpoints/editors-blog/posts/2023/03/15/AI-screening.html",
    "title": "OpenAI’s text classifier won’t calm fears about AI-written homework",
    "section": "",
    "text": "When ChatGPT launched in December 2022, it wasn’t long before users highlighted the tool’s potential as a homework aid. Pop an essay question into ChatGPT’s prompt box or feed your creative writing task to the AI instead, et voila – your work is done!\nIn reality, of course, things aren’t quite so simple. ChatGPT, like other large language models, has an unfortunate habit of making stuff up – fine for creative writing, perhaps; not so good for a history essay. Outputs need to be checked and verified if you want to guarantee a good mark on your assignments. But while ChatGPT can’t – and shouldn’t – be trusted completely, many have found that it can help lighten the homework load.\nWith ChatGPT’s user count crossing the 100 million mark last month, it’s understandable that worries about an explosion of AI-written text have proliferated in many professions, including education. Some education systems have decided to ban the use of ChatGPT. Other educators have adopted a more relaxed approach. Writing in Scientific American, law professor John Villasenor argued:\nVillasenor makes a valid point. But experience tells us that not every student is going to use these tools ethically. Some will pursue the path of least resistance and will attempt to present ChatGPT’s outputs as their own. So, the question becomes: Is it possible to tell the difference between human-generated text and AI-generated text?"
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2023/03/15/AI-screening.html#spot-the-difference",
    "href": "viewpoints/editors-blog/posts/2023/03/15/AI-screening.html#spot-the-difference",
    "title": "OpenAI’s text classifier won’t calm fears about AI-written homework",
    "section": "Spot the difference",
    "text": "Spot the difference\nOne answer to that question comes from OpenAI, makers of ChatGPT. On January 31, they launched a classifier “to distinguish between text written by a human and text written by AIs from a variety of providers”.\nOpenAI introduces the classifier by saying that reliably detecting all AI-written text is “impossible”. But it goes on to say:\n\n“… we believe good classifiers can inform mitigations for false claims that AI-generated text was written by a human: for example, running automated misinformation campaigns, using AI tools for academic dishonesty, and positioning an AI chatbot as a human.”\n\nOpenAI stresses that the current version of the classifier “should not be used as a primary decision-making tool”, and users should take that statement to heart – especially if they are planning to vet student homework with it. In evaluations, OpenAI reports that its classifier correctly identifies AI-written text as “likely AI-written” only 26% of the time, while human written text is incorrectly labelled as AI-written 9% of the time.\nThese two reported numbers are important. They are, respectively, the classifier’s true positive rate and the false positive rate. The former is the conditional probability of a positive result given that a piece of text is AI generated; the latter is the conditional probability of a positive result given that a piece of text is not AI generated. However, neither piece of information directly addresses the question that will be of most interest to teachers: “If a piece of homework is flagged as ‘likely AI-written’ by the OpenAI classifier, what is the probability that it actually is AI-written?”\nTo answer this question, we need to flip the conditional probabilities – from “the probability of positive test given text is AI generated” to “the probability text is AI generated given positive test”. Bayes’ theorem provides a formula for doing just that, as described in this 2017 article by Tim Brock, published by Significance magazine.\nAs Brock’s article demonstrates, versions of this problem are familiar to medical statisticians, who often find themselves having to explain screening test outcomes – specifically, the probability that a person has disease X given that they have tested positive for said disease. This probability depends on the prevalence of a disease and the sensitivity and specificity of the test, and Brock defines these terms as follows:\n\n\nPrevalence\n\nThe proportion of the population being tested that are affected by a given condition.\n\n\n\nSensitivity\n\nThe proportion of patients with the condition being screened for that are correctly identified as having the condition.\n\n\n\nSpecificity\n\nThe proportion of patients without the condition being screened for that are correctly identified as not having the condition.\n\n\n\nSensitivity and specificity are also referred to as, respectively, the true positive rate (mentioned earlier) and the true negative rate.\nWe know from OpenAI’s own evaluations that out of 100 pieces of AI-written text, only around 26 would be correctly classified as “likely AI-written”, so the classifier’s sensitivity is 26%. And out of 100 pieces of human-written text, around 9 would be incorrectly classified as AI written, meaning 91 would be correctly classified as not AI written, so specificity is 91%. But the big piece of information we don’t have is prevalence: What proportion of homework assignments are written by AI?\nThis prevalence figure is likely to vary based on where students live, what age they are, their level of interest in AI tools and technologies, and many other factors. A poll of Stanford University students by The Stanford Daily, for example, found that 17% of respondents used ChatGPT for final assignments or exams in the fall quarter – though it reports that “only about 5% reported having submitted written material directly from ChatGPT with little to no edits”.\nSo, let’s assume for the moment that 5% of homework assignments are AI-generated. If you were screening 1,000 pieces of homework with the OpenAI classifier, you’d see something close to the following results:\n\n\n\n\n\n\n\n\n\n\n\nTrue positives\nFalse positives\nTrue negatives\nFalse negatives\n\n\n\n\nResults\n13\n86\n864\n37\n\n\n\nThe figures below show the results graphically as proportions of (a) all tests and (b) all positive tests. (All plots are produced using Python and the matplotlib package; code and functions are available from this GitHub repository.)\n\n\n\n\n\n\n\nFigure 1a: Classifier test results as a percentage of all tests, assuming 5% prevalence of AI-written homework.\n\n\n\n\n\n\n\nFigure 1b: Classifier test results as a percentage of all positive tests, assuming 5% prevalence of AI-written homework.\nFrom Figure 1b, we see that if the classifier delivers a “likely AI-written” result, the chance that the text is AI-written is only about 13%. This is the classifier’s positive predictive value at the assumed 5% prevalence.\nIf we reproduce our figures using a prevalence rate of 17%, also from the Stanford survey, the chance that a positive result is a true positive is now about 37%.\n\n\n\n\n\n\n\n\n\n\n\nTrue positives\nFalse positives\nTrue negatives\nFalse negatives\n\n\n\n\nResults\n44\n75\n755\n126\n\n\n\n\n\n\n\n\n\n\nFigure 2a: Classifier test results as a percentage of all tests, assuming 17% prevalence of AI-written homework.\n\n\n\n\n\n\n\nFigure 2b: Classifier test results as a percentage of all positive tests, assuming 17% prevalence of AI-written homework.\nYet another survey, this one from Intelligent.com, claims that 30% of college students have used ChatGPT for written homework. Plugging this number into our calculations, the chance that a positive test result is a true positive is now slightly better than 50/50.\n\n\n\n\n\n\n\n\n\n\n\nTrue positives\nFalse positives\nTrue negatives\nFalse negatives\n\n\n\n\nResults\n78\n63\n637\n222\n\n\n\n\n\n\n\n\n\n\nFigure 3a: Classifier test results as a percentage of all tests, assuming 30% prevalence of AI-written homework.\n\n\n\n\n\n\n\nFigure 3b: Classifier test results as a percentage of all positive tests, assuming 30% prevalence of AI-written homework."
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2023/03/15/AI-screening.html#determining-guilt",
    "href": "viewpoints/editors-blog/posts/2023/03/15/AI-screening.html#determining-guilt",
    "title": "OpenAI’s text classifier won’t calm fears about AI-written homework",
    "section": "Determining ‘guilt’",
    "text": "Determining ‘guilt’\nIf a test has a positive predictive value of just over 50% (at an assumed prevalence rate of 30%), does that provide a reasonable basis on which to accuse someone of getting ChatGPT to do their homework? That depends on who you ask. If we look to the legal system for guidance, in civil cases like personal injury claims or contract disputes judges typically make decisions on the so-called “balance of probabilities”. This is generally assumed to mean if we are more than 50% sure of someone’s “guilt” in this context, that might be sufficient to find against them. However, in criminal law, a higher standard applies: “beyond reasonable doubt”. Legal scholars have long wrestled with how to quantify this in probabilistic terms, and surveys of judges put “beyond reasonable doubt” somewhere in the range of being 80% to 99% certain of guilt – see, for example McCauliff (1982) and Solan (1999).\n\nMcCauliff, Catherine MA. 1982. “Burdens of Proof: Degrees of Belief, Quanta of Evidence, or Constitutional Guarantees.” Vand. L. Rev. 35: 1293.\n\nSolan, Lawrence M. 1999. “Refocusing the Burden of Proof in Criminal Cases: Some Doubt about Resaonable Doubt.” Tex. L. Rev. 78: 105.\nIt is at this standard of evidence that OpenAI’s classifier shows its limitations. For example, if we flip Bayes’ theorem around, we find that to achieve a positive predictive value of at least 80%, the prevalence rate needs to be at least 58%. For a positive predictive value of 90%, prevalence needs to be 76%. (Verify these figures for yourself: Python code and functions are available from this GitHub repository).\nThus far in our calculations, we’ve set prevalence according to estimates of the percentage of students who use ChatGPT for their homework. But, according to statistician and science writer Robert Matthews, individual students could justifiably complain about having their guilt decided on this basis. “It’s like deciding someone is guilty of a crime just because they happen to live in an area notorious for criminal gangs,” he says. Instead, the guilt of individual students should be decided using an estimate of the chances that they would use ChatGPT for that particular homework assignment.\nLooked at in this way, Matthews says, “You already have to be pretty convinced of a person’s ‘guilt’ even before applying the classifier if you want to put the evidence ‘beyond reasonable doubt’. Bayes’ theorem highlights the need to be really clear about what you mean by the ‘accuracy’ of a test, and about what question you want the test to answer.”\nSo, here’s a question that teachers will be asking if they are worried about ChatGPT-generated homework: “Has the piece of text I’m marking been written by AI?” If those same teachers use the OpenAI classifier to try to answer that question, they will no doubt expect that something classified as “likely AI-written” is more likely to be AI-written than not. However, as it stands now – and as our examples above have shown – users can’t be confident that’s the case. In education terms, this particular ‘test’ is a long way from scoring top marks.\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “OpenAI’s text classifier won’t calm fears about AI-written homework.” Real World Data Science, March 15, 2023. URL"
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2023/02/14/digital-skills.html",
    "href": "viewpoints/editors-blog/posts/2023/02/14/digital-skills.html",
    "title": "Data science can help close the ‘digital skills’ gap, or so it seems",
    "section": "",
    "text": "Digital skills. We all need them. Employers say they want them, but there aren’t enough to go around. Supply can’t meet demand, so we’re left with a gap – a digital skills gap. But what are digital skills exactly?\nThis is a question that was asked repeatedly, in various different constructions, by Stephen Metcalfe MP, chairing a meeting of the Parliamentary and Scientific Committee on Tuesday, February 7. I went along to the meeting as an observer, hoping to hear an answer to that very question.\nWhat I got was several different answers – no single solid definition, but a reasonable sense that boosting data science skills would go a long way towards closing the digital skills gap."
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2023/02/14/digital-skills.html#survey-says",
    "href": "viewpoints/editors-blog/posts/2023/02/14/digital-skills.html#survey-says",
    "title": "Data science can help close the ‘digital skills’ gap, or so it seems",
    "section": "Survey says…",
    "text": "Survey says…\nThe committee meeting was sponsored by the Institution of Engineering and Technology (IET), and the main focus of discussion was the results of IET’s skills for a digital future survey, based on a YouGov poll of 1,235 respondents drawn from engineering employers (defined as “employers who employ at least one engineering and technology employee in the UK”).\n\n\n\n\n\nDigital skills, including AI skills, are not only required of engineers, says the IET’s Graham Herries. Generative AI tools like Stable Diffusion threaten to shake-up the creative industries. (Photo by Billetto Editorial on Unsplash)\n\n\nKicking off the discussion was Graham Herries, an engineering director and chair of the IET’s Innovation and Skills Panel, who drew attention to the harms that the digital skills gap is reportedly having. Of those respondents who identified skills gaps in their own organisations, 49% pointed to a reduction in productivity, while 35% said skills shortages were restricting company growth.\nAs the hot topic of the day, ChatGPT inevitably came up during the discussion. Herries sees it as a disruptive force, and 36% of all respondents believe artificial intelligence (AI) skills will be important for their engineers to have within five years (24% say they are important now). But AI skills are important for non-engineers too, argued Herries, as he pointed to stirrings in the creative industries caused by generative art tools such as Stable Diffusion.\nHerries therefore puts AI skills under the broad umbrella of “digital skills”. But, to him, it’s not enough to simply be able to use AI technology; rather, users should know enough to be able to ask the right questions about the provenance of the data used to train the AI, its quality and biases, etc. This was a point developed further by Yvonne Baker, an engineer and the CEO of STEM Learning. Baker talked about digital skills as being both the ability to use digital technology and also to understand its limitations. Yet another perspective was offered by Rab Scott, director of industrial digitalisation at the University of Sheffield’s Advanced Manufacturing Research Centre. Scott defined digital skills in the context of quality control systems in industry 4.0: it’s about knowing how and where to place a sensor to collect data about the manufacturing process, to feed that data into a data collection system, analyse the data for insights, and use those insights to inform decision-making."
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2023/02/14/digital-skills.html#closing-the-gap",
    "href": "viewpoints/editors-blog/posts/2023/02/14/digital-skills.html#closing-the-gap",
    "title": "Data science can help close the ‘digital skills’ gap, or so it seems",
    "section": "Closing the gap",
    "text": "Closing the gap\nFurther definitions of “digital skills” are to be found in the IET’s published report. Survey respondents were encouraged to describe the term in their own words, so we see things like:\n\n“the ability to understand, process and analyse data.”\n“Coding, programming, software design, use of social media for marketing and communicating with stakeholders, data visualisation, work that relies solely on the use [of] online systems.”\n\nWhen respondents were asked what skills were lacking in both the external labour market and their internal workforce, around a fifth cited “more complex numerical/statistical skills and understanding”. And when looking to the future and to the skills anticipated to be important areas for growth in the next five years, 39% of respondents picked “data analytics” while 31% said “artificial intelligence and machine learning”.\nSo, perhaps you now understand why I left the meeting with the feeling that more data science skills, more data science training, could help address the shortfall in “digital skills”.\nBut how exactly can we equip more people with the right skills? At one point during the discussion, Metcalfe told the meeting that he was still looking for a key takeaway, something he could take to the Secretary of State and say, ‘This is what we need to embed in the curriculum’. What was offered instead was a range of possible solutions.\nThe IET survey found broad backing for government support for reskilling: 40% of respondents favoured grants or loans for training (and retraining) programmes, 39% would like more funding for apprenticeships, while 33% think there should be better carers advice and guidance in schools and colleges.\nBaker also made the case for digital skills to be taught in schools as part of every subject, not just in computer science lessons, and that teachers would need to be supported to deliver this.\nBut how would you close the “digital skills” gap, if given the chance?\n\n\n\n\n\n\nHave you got news for us?\n\n\n\nIs there a recent data science story you’d like our team to discuss? Do you have your own thoughts to share on a hot topic, or a burning question to put to the community? If so, either comment below or contact us.\n\n\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “Data science can help close the ‘digital skills’ gap, or so it seems.” Real World Data Science, February 14, 2023. URL"
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2023/01/05/newsletter.html",
    "href": "viewpoints/editors-blog/posts/2023/01/05/newsletter.html",
    "title": "Explore the RSS Data Science & AI Section newsletter, right here!",
    "section": "",
    "text": "Happy New Year from all of us at Real World Data Science. We hope you had a relaxing break over the holidays and are now refreshed and excited to see what 2023 has in store. We’re starting the year with a new addition to the site: a page dedicated to the excellent RSS Data Science & AI Section newsletter.\nThis monthly newsletter has been running since February 2020 and is well worth subscribing to as it features roundups of news, new developments, big picture ideas and practical tips.\nYou’ll find the full list of past newsletters in our News and views section (click the “Newsletter” heading in the section menu). If you want to subscribe to the newsletter, head over to datasciencesection.org. The Data Science & AI Section also has a page on the RSS website.\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “Explore the RSS Data Science & AI Section newsletter, right here!” Real World Data Science, January, 5 2023. URL"
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2023/01/18/rwds-at-rss-conference.html",
    "href": "viewpoints/editors-blog/posts/2023/01/18/rwds-at-rss-conference.html",
    "title": "We’re taking Real World Data Science on the road",
    "section": "",
    "text": "Real World Data Science has booked its first conference appearance! This September, we’ll be part of the data science stream of the RSS International Conference.\nOur session, “Real World Data Science Live”, will feature talks and discussions based on content published on this site. In particular, we’re looking to share compelling examples of how data science is being used to solve real-world problems.\nIf you’re thinking about contributing to Real World Data Science, or have already made a submission, do let us know whether you’d be interested in taking part in this in-person event. There are only a handful of speaker slots available, so please get in touch ASAP!\nThe conference takes place 4-7 September 2023, in Harrogate, Yorkshire. Keynote speakers include Anuj Srivastava, a Florida State University professor with research interests in statistical computer vision, functional data analysis, and shape analysis, and other invited topic sessions in the data science stream are:\n\nGitHub: Version control for research, teaching and industry\nSurrogate-assisted uncertainty quantification of complex computer models\nGetting your work to work\nBest practices for the analysis and visualisation of Google Trends data\n\nSee the RSS International Conference 2023 website for more details.\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “We’re taking Real World Data Science on the road.” Real World Data Science, January, 18 2023. URL"
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2023/01/27/talking-chatgpt.html",
    "href": "viewpoints/editors-blog/posts/2023/01/27/talking-chatgpt.html",
    "title": "ChatGPT can hold a conversation, but lacks knowledge representation and original sources for verification",
    "section": "",
    "text": "ChatGPT is, right now, the world’s most popular - and controversial - chatbot. Users have been both wowed by its capabilities1 and concerned by the confident-sounding nonsense it can produce.\nBut perhaps what impresses most is the way it is able to sustain a conversation. When I interviewed our editorial board member Detlef Nauck about large language models (LLMs), back in November, he said:\nFast-forward a couple of months and, as discussed in our follow-up interview below, OpenAI, the makers of ChatGPT, have succeeded in building a question answering system that can sustain a dialogue. As Nauck says: “I have not yet seen an example where [ChatGPT] lost track of the conversation… It seems to have quite a long memory, and doing quite well in this.”\nThere are still major challenges to overcome, says Nauck - not least the fact that ChatGPT has no way to verify the accuracy or correctness of its outputs. But, if it can be linked to original sources, new types of search engines could follow.\nCheck out the full conversation below or on YouTube.\nDetlef Nauck is a member of the Real World Data Science editorial board and head of AI and data science research for BT’s Applied Research Division."
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2023/01/27/talking-chatgpt.html#timestamps",
    "href": "viewpoints/editors-blog/posts/2023/01/27/talking-chatgpt.html#timestamps",
    "title": "ChatGPT can hold a conversation, but lacks knowledge representation and original sources for verification",
    "section": "Timestamps",
    "text": "Timestamps\n\nHow ChatGPT was built and trained (0:41)\nChatGPT’s major advance (3:05)\nThe big problems with large language models (4:36)\nSearch engines and chatbots (9:35)\nQuestions for OpenAI and other model builders (11:29)"
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2023/01/27/talking-chatgpt.html#quotes",
    "href": "viewpoints/editors-blog/posts/2023/01/27/talking-chatgpt.html#quotes",
    "title": "ChatGPT can hold a conversation, but lacks knowledge representation and original sources for verification",
    "section": "Quotes",
    "text": "Quotes\n“[OpenAI] have achieved quite remarkable capabilities in terms of sustaining conversations, and producing very realistic sounding responses… But sometimes [ChatGPT] makes silly mistakes. Sometimes the mistakes are not that obvious. It can hallucinate content… And it still doesn’t know what it’s talking about. It has no knowledge representation, doesn’t have a word model. And it’s just a statistical language model.” (2:04)\n“These models, they produce an answer, which is based on the kind of texts that they have been trained on. And that can be quite effective. But it cannot yet link back to an original source. So what’s still missing is the step where it says, ‘Okay, this my answer to your question, and here’s some evidence.’ As soon as they have done this, then these kinds of systems will probably replace the search engines that we’re used to.” (4:07)\n“[These large language models are] still too big and too expensive to run… For [use in a] contact centre or similar, what you need is a much smaller model that is restricted in terms of what it can say. It should have knowledge representation, so it gives correct answers. And it doesn’t need to speak 48 languages and be able to produce programming code. It only needs to be able to talk about a singular domain, where the information, the knowledge about the domain, has been carefully curated and prepared. And that’s what we’re not seeing yet. Can we build something like this, much smaller, much more restricted, and provably correct, so we can actually use the output?” (7:49)\n“We are seeing communities who don’t necessarily have the technical background to judge the capabilities of these models, but see the opportunities for their own domain and might be acting too fast in adopting them. So the producer of these models has a certain responsibility to make sure that this doesn’t happen.” (12:26)"
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2023/01/27/talking-chatgpt.html#further-reading",
    "href": "viewpoints/editors-blog/posts/2023/01/27/talking-chatgpt.html#further-reading",
    "title": "ChatGPT can hold a conversation, but lacks knowledge representation and original sources for verification",
    "section": "Further reading",
    "text": "Further reading\n\nChatGPT: The Robot, the Myth, the Legend - Philadelphia Physicist blog, January 13, 2023\nCost to run ChatGPT - tweet by OpenAI CEO Sam Altman, December 5, 2022\nGoogle execs warn company’s reputation could suffer if it moves too fast on AI-chat technology - CNBC, December 13, 2022\nMicrosoft reportedly to add ChatGPT to Bing search engine - The Guardian, January 5, 2023\nGetty Images is suing the creators of AI art tool Stable Diffusion for scraping its content - The Verge, January 17, 2023"
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2023/01/27/talking-chatgpt.html#transcript",
    "href": "viewpoints/editors-blog/posts/2023/01/27/talking-chatgpt.html#transcript",
    "title": "ChatGPT can hold a conversation, but lacks knowledge representation and original sources for verification",
    "section": "Transcript",
    "text": "Transcript\n\n\n\n\n\n\nThis transcript has been produced using speech-to-text transcription software. It has been only lightly edited to correct mistranscriptions and remove repetitions.\n\n\n\nBrian Tarran\nWe’re following up today Detlef on the, I guess, one of the biggest stories in artificial intelligence and data science at the moment, ChatGPT, the chat bot that’s driven by a large language model and is proving endless amounts of– providing endless amounts of either entertainment or concern, depending on what you ask it, and what outputs you get. So, but you’ve been looking at it in some detail, right, ChatGPT. And that’s why I thought we would follow up and have a conversation to see, get your view on it, get your take on it. What’s going on?\nDetlef Nauck\nYeah. So, what they have done is, OpenAI have used their large language model GPT-3 and they have trained an instance to basically answer questions and have conversations, where the model remembers what has been said in the conversation. And they have done this by using curated data of question and answers, where they basically have posed a question and said, This is what the answer should be. They trained the system on doing this, then, in the next step, they began use questions, potentially different ones, the system came up with a variety of answers, and then again, human curators would mark which is the best answer. And they would use this data to train what’s called a reward model - so, a separate deep network that learns what kind of answer for a particular question is a good one - and then they would use this reward model to do additional reinforcement learning on the ChatGPT that they had built so far, basically using dialogues and the reward model would then either reward or penalise the response that comes out of the system. And by doing that they have achieved quite remarkable capabilities in terms of sustaining conversations, and producing kind of very realistic sounding kind of responses. Sounds all very convincing. The model presents its responses quite confidently. But sometimes it makes silly mistakes. Sometimes the mistakes are not that obvious. It can hallucinate content. So let’s say you ask it to write you scientific text about whatever topic and put some references in and these references are typically completely fabricated and not real. And it still doesn’t know what it’s talking about. It has no knowledge representation, doesn’t have a word model. And it’s just a statistical language model. So it’s what we would call a sequence to sequence model. It uses an input sequence, which are words, and then guesses what’s the next most likely word in the sequence. And then it continues building these sequences.\nBrian Tarran\nYeah. But, do you think the big advance as you see it is the way it’s able to remember or store some knowledge, if you like, of the conversation, because that was something that came out of our first conversation that we had, where you were saying that, you know, if you’re looking at these as a potential chatbots for customer service lines, or whatever it might be, actually, the trees, the conversation trees break down after a while, and they don’t, you know, these models get lost, but actually, they’re able to maintain it a little longer, are they, or– ?\nDetlef Nauck\nYeah, I have not yet seen an example where they lost track of the conversation they seem to have, it seems to have quite a long memory, and doing quite well in this. So the main capability here is they have built a question answering system. And that’s kind of the ultimate goal for search engines. So if you put something into Google, essentially, you have a question, show me something that answered this, answers this particular question. Of course, what you want this kind of an original source. And these models, they produce an answer, which is based on the kind of texts that they have been trained on. And that can be quite effective. But it cannot yet link back to an original source. So what’s still missing is the step where it says, Okay, this my answer to your question, and here’s some evidence. Then if, as soon as they have done this, then these kinds of systems will probably replace the search engines that we’re used to.\nBrian Tarran\nYeah. The other thing that struck me with them was that the, if you’re asking somebody a question - a human, you know, for instance - you expect a response that and you would hope you will be able to trust that response, especially if it’s someone in an expert position or someone you’re calling, you know, on behalf of a company or something. The fact that - and I asked this question of ChatGPT itself - and the response was, again, you should consult external sources to verify the information that’s been provided by the chatbot. So it’s like, I guess that leaves a question as to what the utility of it is, if you if you’re always having to go elsewhere to verify that information.\nDetlef Nauck\nYeah, I mean, that’s the main problem with these models, because they don’t have a knowledge representation. They don’t have a word model, they can’t fall back on facts that are represented as being true and present those. They come up with an answer. But I mean, there has been a lot of kind of pre-prompting going in to ChatGPT. So when you start writing something, the session has already been prompted with a lot of text, telling the model how to behave, what not to say, to avoid certain topics. There are additional moderation APIs running that make sure that you can’t create certain type of responses, which are based on classical text filtering, and topic filtering. So they try to kind of restrict what the model can do to make sure it’s not offensive or inappropriate. But that is limited. So through crafting your requests, intelligently, you can convince it to ignore all of these things and go past it in some instances. So the, it’s not yet perfect, and certainly it’s not authoritative. So you can’t trust the information if you’re not an expert yourself. So at the moment, I’d say these kind of models are really useful for experts who can judge the correctness of the answer. And then what you get this kind of maybe a helpful kind of text representation of something that you would have to write yourself otherwise.\nBrian Tarran\nYeah, and certainly conversations I’ve had with people, those who kind of work, maybe in creative industries, are finding them quite intriguing, in terms of things like, you know, maybe trying to come up with some clever tweets or something for a particular purpose, or something I want to try out is getting ChatGPT to write headlines for me, because it’s always my least favourite part of the editing job. So that sort of works. But you know, for you, in your position in the industry, has ChatGPT changed your mind at all about, you know, the way you’re perceiving these models and how they might be used? Or is it is it just kind of a next step along in the process of what you’d expect to see before these can become tools that we use?\nDetlef Nauck\nYeah, it’s the next step in the evolution of these models. They’re still too big and too expensive to run, right. So now, it is not quite clear how much it costs OpenAI to run the service that they’re currently running. So you see estimates around millions of dollars per day that they have to spend on running the compute infrastructure to serve all of these questions. And this is not quite clear, the only official piece of information that I’ve seen is in a tweet, where the CEO said, a single question costs in the order of single digit cents, but we have no idea how many questions they serve per day, and therefore how much money they are spending. If you want to run a contact centre, or something like this, it all depends on how much compute need to stand up to be able to respond to hundreds or thousands of questions in parallel. And then obviously, if you can’t trust that the answer is correct, it is of no use. So for making use in the service industry for contact centre or similar, what you need is a much smaller model that is restricted in terms of what it can say, it should have knowledge representation, so it gives correct answers. And it doesn’t need to speak 48 languages and be able to produce programming code, it only needs to be able to talk about a singular domain, where it kind of the information, the knowledge about the domain has been carefully curated and prepared. And that’s what we’re not seeing yet. Can we build something like this, much smaller, much more restricted, and kind of provably correct, so we can actually use the output?\nBrian Tarran\nYeah. Can we go back just to the point you mentioned earlier about, you know, the, the potential of like linking these sorts of chatbots up with search engines, you know, like Google? There’s been some conversations and reporting around, you know, what breakthroughs or not Google might have made in this regard. I mean, have you got any perspective on that area of work and how far along that is maybe and what the challenges are to get to that point?\nDetlef Nauck\nWell, Google has its own large language model, LaMDA. And we have seen an announcement that Microsoft wants to integrate ChatGPT into Bing, their search engine. And, but as I said before, what’s missing is the link to original sources. So you, coming up with a response is nice. But you need to be able to back it up, you need to say, Okay, this is my response, and I’m confident that this is correct, because here are some references. If I compare my response to these references, then they essentially mean the same thing. This is kind of what you need to be able to do. And we haven’t seen this step yet. But I’m certain that the search engine providers are hard at work at doing this because that’s essentially what they want. If you do a search in Google, in some instances, you’ll see a side panel where you get detailed information. Let’s say you ask about what’s the capital of Canada, you get a response, you get the information in more detail, you get links to Wikipedia, where they retrieve content from and present this as the response. And this is done through knowledge graphs. And so if these kinds of knowledge graphs grow together with these kind of large language models, then we will see new types of search engines.\nBrian Tarran\nOkay. I guess final, my final question for you, Detlef, and there might be other angles that you want to explore. But it’s like, are there questions that, you know, if you if you could sit down with OpenAI to talk about ChatGPT and what they’ve done, and what they plan to do next with it, what are the kinds of things that are bubbling away at the top of your mind?\nDetlef Nauck\nWell, one thing is controlling the use of these models, right? If you let them loose on the public, with an open API that anybody can use, you will see a proliferation of applications on top of it. If you go on YouTube, and you Google ChatGPT and health, you’ll already find discussions where GPs discuss, Oh, that is the next step of automated doctors that we can use. So they believe that the responses from these systems can be used for genuine medical advice. And that’s clearly a step too far. So we are seeing communities who don’t necessarily have the technical background to judge the capabilities of these models, but see the opportunities for their own domain and might be acting too fast in adopting them. So the producer of these models has a certain responsibility to make sure that this doesn’t happen. And I don’t know how they want to control this. And, so my question at the developers of these models would be how do you handle sustainability, because the trend goes to ever bigger models. So there’s, in some parts of the industry, there’s the belief, if you make them big enough you get artificial general intelligence, which I don’t believe is possible with these models. But this is definitely a trend that pushes the size of the models. The kind of, the idea of having just one model that can speak all the languages, can produce questions, answers, programming code, is obviously appealing. So you don’t want to build many models. Ideally, you have only one. But how is that supposed to work? And how do you embed actual word knowledge and word models into these systems so that you can verify what comes out?\nBrian Tarran\nYeah. I mean, the ethical dimension that you mentioned in the first part of your response is an important one, I think, in the sense that– but I guess maybe almost redundant in the sense that it’s already out there; you can’t put ChatGPT back in the box, can we, essentially?\nDetlef Nauck\nWell, it’s expensive to run so charging enough for access will put a lid on some frivolous use cases, but still, it needs to be controlled better. And you can make a jump to an AI regulation. So far, we only thought about regulating automated decision making, or automated classification. We also have to think about the automatic creation of digital content or automatic creation of software, which is possible through these models or the other generative AI models like diffusers. So how do we handle the creation of artificial content that looks like real content?\nBrian Tarran\nYeah. And there’s also I think, something I picked up yesterday, there was reports of a case being filed by, I think, Getty Images against the creators of one of these generative art models because they’re saying, you know, that you’ve used our data or you’ve used our image repositories essentially to train this model and it is now producing, you know, it’s producing its own outputs that’s based on this, and I guess there’s an argument of it being a copyright infringement case. And I think that’ll be quite interesting to watch to see how that does change the conversation around - yeah - fair use of that data that is available. You can find these images publicly, but you have to pay to use them for purposes other than just browsing, I guess. Yeah, it’ll be interesting to watch.\n\n\n\n\n\n\nHave you got news for us?\n\n\n\nIs there a recent data science story you’d like our team to discuss? Do you have your own thoughts to share on a hot topic, or a burning question to put to the community? If so, either comment below or contact us.\n\n\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “ChatGPT can hold a conversation, but lacks knowledge representation and original sources for verification.” Real World Data Science, January, 27 2023. URL"
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2023/01/27/talking-chatgpt.html#footnotes",
    "href": "viewpoints/editors-blog/posts/2023/01/27/talking-chatgpt.html#footnotes",
    "title": "ChatGPT can hold a conversation, but lacks knowledge representation and original sources for verification",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI asked ChatGPT to write this article’s headline, for example. I typed in “Can you write a headline for this text:” and then copy/pasted the interview transcript into the dialogue box. It first came up with, “AI Chatbot ChatGPT Proves Capable in Sustaining Conversations but Lacks Knowledge Representation and Original Sources for Verification”. I then asked it to shorten the headline to 10 words. It followed up with, “ChatGPT: Large Language Model-Driven Chatbot Proves Capable But Limited”.↩︎"
  },
  {
    "objectID": "viewpoints/editors-blog/index.html",
    "href": "viewpoints/editors-blog/index.html",
    "title": "Editors’ blog",
    "section": "",
    "text": "Data science as ‘a rainbow’, and other definitions\n\n\n\n\n\n\n\nUpdates\n\n\nThemes\n\n\nPeople\n\n\n\n\nData science means different things to different people. Former RSS president Sylvia Richardson has described it as ‘a rainbow of interconnected disciplines’. What’s your personal definition?\n\n\n\n\n\n\nMar 29, 2023\n\n\nBrian Tarran\n\n\n\n\n\n\n  \n\n\n\n\nOpenAI’s text classifier won’t calm fears about AI-written homework\n\n\n\n\n\n\n\nAI\n\n\nLarge language models\n\n\nClassifiers\n\n\nScreening tests\n\n\n\n\nEducators are worried about ChatGPT being used by students for homework assignments, so OpenAI has released a tool to classify whether text is human- or AI-written. But relying on the classifier’s results is ill-advised, as some basic statistics shows.\n\n\n\n\n\n\nMar 15, 2023\n\n\nBrian Tarran\n\n\n\n\n\n\n  \n\n\n\n\nData science can help close the ‘digital skills’ gap, or so it seems\n\n\n\n\n\n\n\nSkills\n\n\nTraining\n\n\nAI\n\n\nMachine learning\n\n\nData analytics\n\n\n\n\nA ‘digital skills’ gap is harming employer productivity and growth, according to a survey by engineering body IET. But the ‘digital skills’ that are needed sound a lot like data science skills: statistical understanding, data analytics, AI and machine learning.\n\n\n\n\n\n\nFeb 14, 2023\n\n\nBrian Tarran\n\n\n\n\n\n\n  \n\n\n\n\nChatGPT can hold a conversation, but lacks knowledge representation and original sources for verification\n\n\n\n\n\n\n\nMachine learning\n\n\nLarge language models\n\n\nAI\n\n\n\n\nChatGPT represents a next step in the evolution of large language models, says Detlef Nauck. However, there are still major challenges - and concerns - to overcome.\n\n\n\n\n\n\nJan 27, 2023\n\n\nBrian Tarran\n\n\n\n\n\n\n  \n\n\n\n\nWe’re taking Real World Data Science on the road\n\n\n\n\n\n\n\nUpdates\n\n\nEvents\n\n\n\n\nJoin us at the RSS International Conference 2023 in Harrogate, 4-7 September.\n\n\n\n\n\n\nJan 18, 2023\n\n\nBrian Tarran\n\n\n\n\n\n\n  \n\n\n\n\nExplore the RSS Data Science & AI Section newsletter, right here!\n\n\n\n\n\n\n\nUpdates\n\n\nNewsletters\n\n\n\n\nWe’re starting the year with a new addition to the site: a page dedicated to the excellent RSS Data Science & AI Section newsletter.\n\n\n\n\n\n\nJan 5, 2023\n\n\nBrian Tarran\n\n\n\n\n\n\n  \n\n\n\n\nA chat with ChatGPT\n\n\n\n\n\n\n\nMachine learning\n\n\nLarge language models\n\n\nAI\n\n\n\n\n‘Hello there! I’m a large language model trained by OpenAI, so I don’t have the ability to experience emotions or have a physical presence. I’m here to provide information and answer questions to the best of my ability. Is there something specific you would like to know?’\n\n\n\n\n\n\nDec 9, 2022\n\n\nBrian Tarran\n\n\n\n\n\n\n  \n\n\n\n\nLLMs in the news: hype, tripe, and everything in between\n\n\n\n\n\n\n\nMachine learning\n\n\nLarge language models\n\n\nAI\n\n\n\n\nWe’re back discussing large language models after two weeks of ‘breakthrough’ announcements, excitable headlines, and some all-too-familiar ethical concerns.\n\n\n\n\n\n\nDec 9, 2022\n\n\nBrian Tarran\n\n\n\n\n\n\n  \n\n\n\n\nWhy large language models should come with a content warning\n\n\n\n\n\n\n\nMachine learning\n\n\nLarge language models\n\n\nAI\n\n\n\n\nThe outputs of LLMs seem impressive, but users need to be wary of possible bias, plagiarism and model ‘hallucinations’.\n\n\n\n\n\n\nNov 23, 2022\n\n\nBrian Tarran\n\n\n\n\n\n\n  \n\n\n\n\nMeet the team\n\n\n\n\n\n\n\nPeople\n\n\nBiographies\n\n\n\n\nIntroducing the editors of Real World Data Science.\n\n\n\n\n\n\nOct 18, 2022\n\n\nEditorial Board\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "viewpoints/index.html",
    "href": "viewpoints/index.html",
    "title": "Viewpoints",
    "section": "",
    "text": "‘I’m way more into prevention than cure’: Stephanie Hare on why we need a culture of technology ethics\n\n\n\nTechnology ethics\n\n\nAI ethics\n\n\nCulture\n\n\nRegulation\n\n\n\nStephanie Hare, author of ‘Technology is Not Neutral’, talks to Real World Data Science about the ‘wicked problem’ of technology and AI ethics, and why laws and regulations…\n\n\n\nBrian Tarran\n\n\nApr 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData science as ‘a rainbow’, and other definitions\n\n\n\nUpdates\n\n\nThemes\n\n\nPeople\n\n\n\nData science means different things to different people. Former RSS president Sylvia Richardson has described it as ‘a rainbow of interconnected disciplines’. What’s your…\n\n\n\nBrian Tarran\n\n\nMar 29, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpenAI’s text classifier won’t calm fears about AI-written homework\n\n\n\nAI\n\n\nLarge language models\n\n\nClassifiers\n\n\nScreening tests\n\n\n\nEducators are worried about ChatGPT being used by students for homework assignments, so OpenAI has released a tool to classify whether text is human- or AI-written. But…\n\n\n\nBrian Tarran\n\n\nMar 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUS legislators get their data science act together\n\n\n\nData science education\n\n\nData literacy\n\n\nPolicy\n\n\n\nA bill introduced in the US Congress wants to make funds available to develop data science and data literacy education across the United States. We sit down with education…\n\n\n\nBrian Tarran\n\n\nMar 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData science can help close the ‘digital skills’ gap, or so it seems\n\n\n\nSkills\n\n\nTraining\n\n\nAI\n\n\nMachine learning\n\n\nData analytics\n\n\n\nA ‘digital skills’ gap is harming employer productivity and growth, according to a survey by engineering body IET. But the ‘digital skills’ that are needed sound a lot like…\n\n\n\nBrian Tarran\n\n\nFeb 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy open science is ‘just good science in a digital era’\n\n\n\nOpen science\n\n\nReproducible research\n\n\n\nReal World Data Science speaks with statistician and data scientist Heidi Seibold about open science: what it means, the benefits of it, and how to move towards it.\n\n\n\nBrian Tarran\n\n\nFeb 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChatGPT can hold a conversation, but lacks knowledge representation and original sources for verification\n\n\n\nMachine learning\n\n\nLarge language models\n\n\nAI\n\n\n\nChatGPT represents a next step in the evolution of large language models, says Detlef Nauck. However, there are still major challenges - and concerns - to overcome.\n\n\n\nBrian Tarran\n\n\nJan 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to ‘Escape from Model Land’: an interview with Erica Thompson\n\n\n\nModelling\n\n\nEthics\n\n\n\nAuthor Erica Thompson talks to Real World Data Science about the ‘social element’ of mathematical modelling, how it manifests, and what to do about it.\n\n\n\nBrian Tarran\n\n\nJan 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe’re taking Real World Data Science on the road\n\n\n\nUpdates\n\n\nEvents\n\n\n\nJoin us at the RSS International Conference 2023 in Harrogate, 4-7 September.\n\n\n\nBrian Tarran\n\n\nJan 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExplore the RSS Data Science & AI Section newsletter, right here!\n\n\n\nUpdates\n\n\nNewsletters\n\n\n\nWe’re starting the year with a new addition to the site: a page dedicated to the excellent RSS Data Science & AI Section newsletter.\n\n\n\nBrian Tarran\n\n\nJan 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA chat with ChatGPT\n\n\n\nMachine learning\n\n\nLarge language models\n\n\nAI\n\n\n\n’Hello there! I’m a large language model trained by OpenAI, so I don’t have the ability to experience emotions or have a physical presence. I’m here to provide information…\n\n\n\nBrian Tarran\n\n\nDec 9, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLLMs in the news: hype, tripe, and everything in between\n\n\n\nMachine learning\n\n\nLarge language models\n\n\nAI\n\n\n\nWe’re back discussing large language models after two weeks of ‘breakthrough’ announcements, excitable headlines, and some all-too-familiar ethical concerns.\n\n\n\nBrian Tarran\n\n\nDec 9, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy large language models should come with a content warning\n\n\n\nMachine learning\n\n\nLarge language models\n\n\nAI\n\n\n\nThe outputs of LLMs seem impressive, but users need to be wary of possible bias, plagiarism and model ‘hallucinations’.\n\n\n\nBrian Tarran\n\n\nNov 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeet the team\n\n\n\nPeople\n\n\nBiographies\n\n\n\nIntroducing the editors of Real World Data Science.\n\n\n\nEditorial Board\n\n\nOct 18, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "viewpoints/interviews/posts/03/06/data-science-act.html",
    "href": "viewpoints/interviews/posts/03/06/data-science-act.html",
    "title": "US legislators get their data science act together",
    "section": "",
    "text": "On February 14, 2023, a bipartisan group of US legislators introduced the Data Science and Literacy Act with the goal of boosting access to data science education and building “America’s 21st century STEM workforce”. We sat down with guests Zarek Drozda, Anna Bargagliotti, Christine Franklin and Steve Pierson to discuss the news and to hear why data science education is “the new apple pie”.\nCheck out our full conversation below or on YouTube."
  },
  {
    "objectID": "viewpoints/interviews/posts/03/06/data-science-act.html#timestamps",
    "href": "viewpoints/interviews/posts/03/06/data-science-act.html#timestamps",
    "title": "US legislators get their data science act together",
    "section": "Timestamps",
    "text": "Timestamps\n\nThe state of data science education in the United States (3:31)\nWhat will be the main impacts of the Data Science and Literacy Act? (9:14)\nProfessional development support for teachers and teacher-educators (13:06)\nHow much money is needed to deliver data science education? (18:53)\nDeveloping a data science curriculum (27:09)\nBuilding confidence in data, statistics, and technology (31:54)\nLearning from, and making connections with, international colleagues (37:03)"
  },
  {
    "objectID": "viewpoints/interviews/posts/03/06/data-science-act.html#quotes",
    "href": "viewpoints/interviews/posts/03/06/data-science-act.html#quotes",
    "title": "US legislators get their data science act together",
    "section": "Quotes",
    "text": "Quotes\n“Most of our teachers in US schools, math teachers, have not had any formal training in statistics. Or if they have, it’s been maybe one course. They’re very uncomfortable with trying to implement these standards [for data science and statistics education]. And it’s just going to require a tremendous amount of professional development. Sounds easy in theory to deliver professional development, but very difficult in practice.” (10:18)\n“We know that in aggregate, between federal, state, private and local funding, we’re going to have to create the necessary resources to make sure that our K-12 public education system can prepare students for a world that’s changing super fast, and the K-12 system moves super slow in how it adapts to new content. And so really it’s both about what can we do to upskill data science, data literacy skills [and] it’s also about how do we help the system adapt faster as new technology comes out and leverage the importance of data in that.” (19:36)\n“I think we’ve spoken to some 50 or 60 offices, both on the Senate side and the House side. And this [has been] received really well. We don’t get any pushback on that there is a need for greater data literacy. Here stateside, I’ve been saying it’s kind of like advocating for apple pie. People get it and they resonate.” (21:20)\n“As we introduce this bill, I think we should be messaging [that] there’s a economic competition aspect to this; that it’ll be really important for the US to make investments in this area to, frankly, catch up to where I think other international peers are.” (40:34)\n“Data tell our stories, and they reflect what’s happening in our world today – much like art around us in some ways. And a way to think about data science education is to think about what we need our data understanding to be at each point in time in our educational career, or in our lives. And it’s not static, it’s an evolving thing. So you have to move with the data that are being collected.” (42:54)"
  },
  {
    "objectID": "viewpoints/interviews/posts/03/06/data-science-act.html#transcript",
    "href": "viewpoints/interviews/posts/03/06/data-science-act.html#transcript",
    "title": "US legislators get their data science act together",
    "section": "Transcript",
    "text": "Transcript\n\n\n\n\n\n\nThis transcript has been produced using speech-to-text transcription software. It has been only lightly edited to correct mistranscriptions and remove repetitions.\n\n\n\nBrian Tarran\nHello, and welcome to the Real World Data Science news q&a. I’m Brian Tarran. And I’m joined today by a panel of guests to discuss some promising developments in the United States around data science education. On the show today we have Zarek Drozda, director of the Data Science 4 Everyone coalition, Anna Bargagliotti, Graduate Programme Director and Professor of Mathematics at the Seaver College of Science and Engineering, Loyola Marymount University, Christine Franklin, the American Statistical Association’s K-12 statistical ambassador, and Steve Pearson, who is the ASA’s Director of Science Policy. Welcome all. Thank you for joining us. Steve, I’d like to come to you first, because it was one of your ASA science policy tweets that that first drew my attention to this story. And that specifically was the tweet about the introduction of a new Data Science and Literacy Act in the US House of Representatives. Can you give our viewers an overview of the act and its significance to the data science education landscape, please.\nSteve Pierson\nHappy to Brian and I also want to credit my colleague, Ed Wu, an ASA science policy fellow who worked a lot on this and championed it. So I see kind of two overarching points here for the bill. One is just to help out those budding efforts around the United States to bring more data science education to students, right, the demand in the jobs is out there. Students should know about these jobs, we want to connect them to the 21st century jobs. So this is a Department of Education programme that helps out those schools, communities that need the help that want the help. We’re not trying to require anything of schools, which already have enough curriculum requirements. So this is a voluntary programme, that I mean, I think it’s developing curriculum, it’s providing professional development. But I think there’s another aspect of this, Brian, which is just kind of the attention that this can bring to these jobs, to the schools to the members of Congress that, you know, data intensive jobs are a great job opportunity in the 21st century, right? You can look at so many places to know that, right? The Bureau of Labor Statistics has both statistician and data scientist as the top 10 jobs in terms of projected growth for the next decade. There’s Glassdoor, there’s many others, so we know that. We want to make sure that today’s students know about those opportunities and are connected to them. But we also want to just kind of diversify the STEM workforce. So there’s components of that in the bill as well. And so we want, we think that, you know, a bill introduced into the US Congress will help bring attention to that, including the members of Congress and others.\nBrian Tarran\nExcellent. I’ll take a step back briefly to look at the data science education landscape as it is today and Zarek, you helped facilitate a National Academies workshop last September, and one of the aims was indeed to survey that landscape for data science education for the K-12 grades – and for international audiences, correct me if I’m wrong, that’s students aged about five up to 17. Is that correct?\nZarek Drozda\nRight, for five to 18 range.\nBrian Tarran\nSo yeah, so how would you summarise that kind of state of data science education in the US right now?\nZarek Drozda\nYeah, well, first focus on the workshop that was facilitated by the National Academies. And that was not the first but definitely the largest to date, in terms of a national convening of the United States for data science and data literacy education. We had 100 plus researchers, programme developers, and higher ed faculty in the room. There were 500 online, it was a big, you know, kind of early stage milestone for billing data science education in the United States. And we had a number of topics ranging from you know, what does this look like a practice? What is the professional development for, for K-12 teachers look like? What are examples both standalone data science courses, and also integration into different existing K-12 subject areas. And it was really a showcase of you know, a lot of the curriculum work that’s been developed over the past 10, 15 years for building your full length data science high school courses, or for building lesson plans or for building, you know, education, classroom specific software for data analysis that students can really get their their head around. And so I think it was a milestone to then this legislation then built off right that Steve was mentioning. I’m glad that you know news of both the National Academies workshop and then the legislation and kind of the growing momentum here in the US generally has made it across the pond. I think, partially our social media game was strong enough, which is exciting to hear. But I think it’s just a testament to you know, the energy for this space is really growing, right? Because it’s, it’s career connected. I think it’s so relevant to so many other emerging technologies, whether it’s artificial intelligence or ChatGPT or cyber. And I think, students, what came through really clearly in the workshop is that students really find this content relevant because of the technology applications. And it seems so, you know, of the moment.\nBrian Tarran\nYep. Well, certainly, you talked about jumping across the pond. So two weeks ago now, but it might have been three, I attended a meeting, a discussion around what they refer to as the digital skills gap in the UK. And I left that meeting feeling very much like what they were talking about, defining as digital skills were data science skills. And so when I saw that there was this Data Science and Literacy Act, I thought, well, you know, here’s, here’s something that hopefully, other other places like the UK can learn from. So Anna, you participated didn’t you in the workshop? So do you mind sort of giving us an overview of the data science landscape as it is, as it is now?\nAnna Bargagliotti\nAbsolutely, um, so I think, in the United States, at the moment, I think we’re at a spot where the different states are sort of moving. And in the United States, each state has their own department of education. So they, we are not, they’re not federal standards, they are state standards. And each in a lot of states, those standards are being revised and to include data science standards, and those discussions are moving pretty quickly, with some states already approving, other states implementing this coming fall, for example. And other states that are still in the process of sort of starting that. But it’s, but it’s exciting. The other thing that’s really been happening is trying to understand what the curriculum should what curriculum should look like in K-12, in particular. And the GAISE report, like you mentioned before, has a, lays out a, you know, a nice sort of example of what that should look like at the elementary, middle school and high school levels, and can be used sort of as an anchor point for states looking at what they should be doing. I would say at the university level, at the what we call the 12-16 level in the US, we are pretty good. There are many data science programmes and majors and minors across the United States. And they are quite strong, there’s more and more that pop up. And overall, those majors and minors look pretty similar from university to university, and those students are coming out with very, you know, a good skill set, and they are all finding jobs. As Steve mentioned, the job growth is there. And students are feeling quite prepared when they go to into the job growth. So I think where the university level is, could have been well articulated and well defined at the moment, the K-12 level is still sort of in flux of trying to figure out what should be there. And part of that has to do also with teacher preparation, it’s trying to understand what teachers should have and know in order to teach whatever these data science ideas are that are important for the K-12 level. The GAISE report makes some very, I think, concrete recommendations about what that should be, particularly being anchored in the statistical investigative process or problem solving process and understanding how you can use questioning in that and that being really a skill set that we are trying to promote in K-12 education, that then they use also at the university level if they continue on that way. And then just understanding that there’s different large conceptions of data now. Data are not just numbers, data could be sounds, could be text and whatnot. And these ideas are sort of these data science ideas that we are trying to promote in K-12, as well as at the university level. So I think overall, the landscape is quite good. I mean, we’re moving in these great directions. And I think slowly, we’re getting to some consensus about what that looks like. And we’re seeing that in the states moving forward with different standards.\nBrian Tarran\nExcellent. And so you mentioned the GAISE report there and that’s the Guidelines for Assessment and Instruction in Statistics Education, and of course that’s something you co-authored with, with Chris Franklin, Chris. Yeah, of course. Yeah. I’d like to bring you into the conversation. Now, Chris. You know, you and I have spoken a number of times over the years about GAISE about statistics, education and statistical literacy and, and the challenges of delivering high quality education in data and statistics at all levels of the curriculum. I wanted to get your impression of what you think the big contributions are that this act will hopefully make towards improvement of the data science education landscape.\nChristine Franklin\nWell, I was very excited to see this act. And I think one of the big impacts that I see is how it can help our state departments of education actually try to implement standards that we’re seeing put in place right now, in terms of statistics and data science. Teachers, right now, most of our teachers in the US schools, math teachers have not had any formal training in statistics. Or if they have, it’s been maybe one course, they’re very uncomfortable with trying to implement these standards. And it’s just going to require a tremendous amount of professional development. Sounds easy in theory to deliver professional development, but very difficult in practice. And a big part of the difficulty of delivering professional development is funding to, to pay for this. Unfortunately, what I’ve seen is state departments of education will often implement these very nice standards in their curriculum, but then they’ve run out of money, or they don’t have sources of funding to where they can then think about the professional development of the teachers. So it’s not only the professional development of the teachers, but also the curriculum that’s going to support the standards that are put in place. Now, fortunately, ASA, for example, has just a wealth of open source resources that teachers can use. But then how did the teachers know where to get it? How do they know how to implement it in the classroom. So state departments are charged with trying to develop a framework of materials for their teachers. This takes money, this takes expertise. So not only that, but I think this bill can help with funding to allow state departments to do that. But professional development typically has been like week long workshops, day workshops, maybe they go online and do workshops, but really, for professional development to be successful. Teachers need day to day support, which requires funding once again, to provide the resource within schools and school systems to provide more of the day to day support that these teachers need. And I think lastly, one thing that we don’t think a lot about, but I’m hoping this bill will help, is the assessment that goes along with the curriculum that we’re implementing for statistics and data science. Once again, that takes funding that takes manpower support. And I’m really hoping that this bill can be a source that that our state DoE’s can turn to make their standards more successful with implementation.\nBrian Tarran\nYeah, Chris, when you were speaking there about providing professional development support for the teachers, it reminded me of when I was a primary school governor here in the UK a few months back, a few years back, sorry. And we would always talk about math education, trying to improve math education, and that I think, the teacher confidence to deliver the math curriculum is always the issue that we run up against. So having support, having resources, having people that can go in and help, that changes the dynamic, I think, for teachers and certainly equips them to, to deliver on, you know, on that vision of, of data science education and statistical literacy for all, which is something that we spoke about before, right?\nChristine Franklin\nThat’s exactly right.\nBrian Tarran\nThat’s, that’s your vision for where we get to as a society?\nChristine Franklin\nWell, I think one thing, one other thing we need to remember, it’s not just the teachers that need our support. It’s also the teacher educators that are preparing our school level teachers. And we, we need to keep that at the top of the list of priorities because most of our teacher educators recognise that our school level teachers need this support, but they are in a similar situation to where they don’t know exactly what they need to do. And so we’ve got kind of two big spots here that we need to work on for professional development, and it’s gonna take a lot of time and effort.\nBrian Tarran\nYeah, well, we’ll come back to that later, if you don’t mind. Zarek, did you want to come in? It looked like you was about to chime in.\nZarek Drozda\nYeah, I was just, I wanted to agree with Chris and second it and expand it because I think it’s professional development for teacher educators and every layer above that, right. It’s every layer above the teachers: teacher educators, it’s the district staff who are implementing and creating these programmes, it’s the state staff we’re creating the standards, it’s state policy makers, it’s federal policymakers – like, you could think about professional development for all those stakeholders. And we know we need to build, you know, better education, right, for every one of those groups that are above the individual educator in a classroom, knowing that the national infrastructure is just supporting the teacher in the individual classroom to do this best at the end of the day. So yeah, we think about that in terms of a, there’s a whole system that needs to move here.\nBrian Tarran\nI think that’s an important point to note, I think, because I go back to that digital skills workshop I was at and one of the questions that came up from from the chair was, you know, what one thing can I take back to the Secretary of State to say we need to add this to the curriculum, but it’s not one thing is it? It’s, it’s it’s a whole system, as you say, Zarek. I did want to ask you a bit about the organisation that you’re a part of, you have a coalition called Data Science 4 Everyone. To what extent was you involved in the kind of shaping of this, of this bill? And, you know, you obviously, you’ve obviously welcomed it, and you’re excited about the potential, and how much work do you see as being left to do to, to kind of get it over the line and get it into application?\nZarek Drozda\nSure, well, just a very quick background on DS4E. We’re a national initiative and a coalition as you said, based at the University of Chicago here in the States. We’ve been putting together a community of education researchers and K-12, system leaders to advance policy and advance awareness and advance the case making right for why data science and data literacy and statistical acumen is so important in this day and age. And we’re really working across the K-12 system, which is very decentralised in the US, to try to forward those goals. Yeah, as I think about next steps and what’s needed, again, just re-double what Chris said, we need more funding. We did some, to give you an example. So when computer science was being built out as a new school subject in the States, they spent somewhere in the range of three to $4 billion over 15 years to build an entirely new school subject. We’re not necessarily doing that here, right? We’re not necessarily building out a whole new school subject, I think we’re really, at least our group has been much more focused on how can we integrate and upskill teachers in K-12 math, or K-12 science or K-12 social studies, right, and integrate these concepts into the existing K-12 ecosystem, working with the different subject societies. But, you know, this bill is a first really great milestone, but we know now we’re going to have to call on state legislators to pass appropriations at the state level to fund teacher support locally, we’re going to have to, you know, call on schools and districts, right, to help give teachers time to be able to implement these classroom experiences. And we’re going to need more research. Right. So this bill calls for grant programmes to state and local partners to create. But I think we also need more funding for NSF and IES, the two kind of education research bodies in the US at the national level, to fund things like student assessments, or to fund accommodations for students with disabilities to be able access to technology for data science software. There’s a lot more R&D work that also has to happen to bring down the adoption costs over time for doing this type of work and making sure that every student regardless of their background, can benefit from the skill areas, and you know, upskilling in this space. And I think the last thing I would say is that we know we also need to work on teacher confidence, right? I think it’s both teacher confidence with statistics, right, and probabilistic thinking. And it’s also the the confidence of the technology, which is brand new, right? Most classrooms in the US have not been using spreadsheets, even though most workplaces do, let alone, you know, R, Python, SQL, any of the more kind of modern computational tools that are used in modern day statistics. And so we have a lot of work on that front to do as well.\nBrian Tarran\nOkay. And so, if I’ve understood it correctly, there’s about $50 million over five years that is being asked for in this bill. But that’s, from what you’re saying, am I correct in thinking that’s just a kind of a small part of what’s needed to completely deliver on your goals?\nZarek Drozda\nIt is a first step. An important one, but a first step.\nBrian Tarran\nSo, longer term, is it– Do you head towards the billions territory, like in the computer science space? Or is it a little less demanding of finances and resources and that, do you think? I know it’s hard to say, to pin these things down, but–\nZarek Drozda\nAt least from my angle, I’d love the group to jump in here, it’s probably a little bit less demanding. But we know that in aggregate between federal, state, private and local funding, we’re going to have to create the necessary resources to make sure that our K-12 public education system can prepare students for a world that’s changing super fast, and the K-12 system moves super slow, right, in how it adapts to new content. And so really it’s both about, you know, what can we do to upskill data science, data literacy skills. It’s also about how do we help the system just adapt faster as new technology comes out and, you know, leverage the importance of data in that.\nSteve Pierson\nAnd Brian, I can jump in a little bit on the price tag, which, when we were shopping around this bill, we didn’t actually include like a cost per year for this, because we know that that can be a very sensitive topic, and we were, we really wanted to have bipartisan introduction. So, and we were fortunate to get it right. But it was the offices who have agreed to kind of consider us that came up with the $10 million. And I’m, you know, I know for a fact that, you know, a few of the offices, at least one of the offices did want significantly more, but this was how we were gonna get bipartisan introduction.\nBrian Tarran\nYeah. Okay. And on that point, that bipartisan nature of the bill’s introduction, does that give you as a group hope that it will eventually make it through Congress and become a law, and that there’ll be these resources made available?\nSteve Pierson\nYeah, absolutely. And I’ll also just say, Brian, that I think we’ve spoken to some 50 or 60 offices, both on the Senate side and the House side. And this is, it’s received really well, we don’t get any pushback on that, you know, yes, there is a need for greater data literacy. Here stateside, I’ve been saying it’s kind of like advocating for apple pie. Right. This is, people get it and they resonate. And to that point, we only brought this to Representative Stevens, I think it was maybe late October. But they really wanted to move on this, they wanted to wait for the new Congress for bandwidth issues. But, significantly, we’re told that the representative wanted this to be her first bill introduced of the new Congress, and she had many to choose from. So I think that’s really positive. The other thing is, I’ve heard from email, we haven’t had a chance to debrief with the staff yet, because they’re swamped with all kinds of things, but they’re getting a lot of positive feedback from people about this bill. So it really seems to be kind of tapping a nerve. A recognition.\nZarek Drozda\nI was just going to add to Steve that when we went– So in advance of that legislation being introduced, we had 15 of the largest math and science and technology education associations supporting the legislation, which was a huge win. It showed, I think, that this data science, data literacy, education is really a collaborative multi-subject effort in the States, which does not happen often, it’s usually very siloed. And I think the other thing I’d say is, in the first 24 hours since the bill was introduced, we had 150-160 additional education leaders and organisations sign on to the letter of support that we were helping circulate between Data Science 4 Everyone and the American Statistical Association. And just to re-emphasise that we saw a lot of energy around this, and bipartisan, right. We’re building support on both sides of the aisle, because it’s, you know, this is apple pie, it’s so evident that every student’s going to need this for the next decade.\nBrian Tarran I like this. So data was once the new oil, but now data science education is the new apple pie. I think this is this is great. Anna, you know, assuming that the Data Science and Literacy Act goes through, funds are made available, this work starts in earnest, what sort of timescales are we looking at, do you think, before we start to see the real world impact, you know, in terms of teacher training, student outcomes, and then eventually, obviously, building this workforce, that is so needed, that is equipped with data literacy and data science skills?\nAnna Bargagliotti\nYeah, I think I kind of want to say two things to this point. I think post K-12, the college level, we are seeing those outcomes, and they are great. And I think we are really, that is, it feels very good. It feels like we’ve targeted the right things. It feels that students report back that they’re excelling in their jobs and doing great things. So I think that part is sort of, is taken care of. I think at the K-12 level, what’s harder is we’re less nimble. Like Zarek mentioned, K-12 is just a beast to move. It’s very difficult. And we’re in a situation where the target of data science changes every day, truly every day, for two real reasons. One is because the conceptualization of data changes every day. We can imagine today we think of data as text, but in probably a month, there’s some other type of data that we haven’t thought of that will emerge. And so now all of a sudden, you’ve got, you’re trying to teach pillars or concepts in K-12 that are actually a moving target. And then the other big thing that changes pretty much every day is our capabilities for wrangling, visualising access to data, all that stuff is changing. And so at the university level, you’re much more nimble because you’re in these courseworks, and your students are very advanced, and you can kind of move within those, those spaces, and you can change a course at a time. At the K-12 level we’re much more prescribed and harder to move. So I think in terms of timeframe, I think if we focus on the sort of large concepts and the baseline skill sets that we want to graduate students from K-12, I think we can move much quicker than getting into sort of the nitty gritty of a student needs to be able to programme per se, or something like that, like I think more that statistical investigative process, and those questioning, those types of ideas are really the crux of what K-12 looks like that then allows the university level to be more nimble. So for me, the timeframe is I think we have to, we have to think about it differently as we’re never going to arrive. It’s not going to be like, Oh, in five years, this is completed. It’s a, are we reacting in the right way? Or are we sort of ahead of the game. And I think we can get ahead of the game if we go to the concepts and that idea instead of thinking of topics that we’re teaching. And I hope that with this, I have great hope for this bill. I think it’s like everybody said that this is just such a great first drop in the bucket with the apple pie that I think hopefully in the next few years, the states are going to have been moved and then everybody will be doing some type of data science at the state level. And then it’s like Chris mentioned, before moving the professional development, which is a big challenge.\nBrian Tarran\nWhen you were speaking, just there, Anna, and you were you were talking about the difference between you what you’re achieving at the college level versus the younger level, it made me think that, you know, this isn’t just about providing a next generation of data science workers, right, it’s about equipping everyone with the skills to be able to exist in a data science world. And this goes to the point that I think Chris and I have spoke about before, right, about being able to be, when you’re confronted with data, being able to ask the right questions about that data. And so I think obviously, that’s, to me, that’s where I think maybe the Guidelines for Assessment and Instruction in Statistics Education seems a promising first step in the development of a data science, data literacy, statistics curricula, do you see that that needing to be – obviously these things are always needing to be revised, right – but do you see that, Chris, as a kind of foundation on which the states can start to build and to move towards this vision that’s laid down in the act?\nChristine Franklin\nWell, most definitely. And we’ve been real fortunate in the US here that our states right now that are trying to implement more statistics and data science standards, are going to the GAISE II document to use as a guiding document. And ASA has been very fortunate also to where these states, many of them have reached out to us to actually help advise them as they go through the process. I wanted to come in on a follow up with something Anna said in terms of the timeline. As she was speaking, I thought about how when we were sending out our document for review with the GAISE II, the updates, and we sent it out to probably about 20 to 25 different reviewers, we received more feedback than we knew what to do with. But many of the reviewers, very well respected people in the field, came back to us and said, we were being way too ambitious with the GAISE II document that, in fact, so many states here in the US we’re still back trying to implement the recommendations we made in the 2005 GAISE document. And our response was, we can’t be writing for today. What our goal is, is to try to write a document to where 5, 10 years from now, this is where we hope that our K through 12 system will be. And I’ve noticed this in working with other national documents, that I think the tendency oftentimes is to try to write a document for where we are now. And I think that we always need to, in terms of a timeline, think about 10 years down the road when we wrote the document. And it’s hard to be visionary. But as Anna was saying, things are changing so quickly. I mean, the GAISE II document, I think the writers are all saying, oh, we should have included this, we should have included that. Because since its publication in 2020, we’re already seeing needed changes. So I think we’re always going to be catching up to some degree. But as Anna said, I think the goal of these documents needs to be conceptual understanding, it needs to be that role of questioning. I like to say, I just want people to be healthy sceptics to where when they see statistical information, they see data, they immediately start asking questions. They may not know the answers, or know what the answers aresupposed to be, but at least they’re questioning.\nAnna Bargagliotti\nAnd I’ll just add one really quick thing, Brian, if that’s okay. Chris and I are about to embark on another ASA initiative, which is the revision of the SET report, which is the Statistics Education for Teachers report that the last publication was, oh my gosh, Chris, I can’t remember even the date–\nChristine Franklin\n2015.\nAnna Bargagliotti\n2015. And the the idea of the new report that we will be starting to write this year, along with some wonderful colleagues, is going to be to talk about the teacher preparation aspect of that in light of everything that’s happened with statistics and data science in particular, what should that look like? And how, how could that be? So I just wanted to add that.\nBrian Tarran\nExcellent. I also wanted to ask, again, maybe this is a question for Zarek. If we’re talking about data science for everyone, what about people like me, who have already already finished their education? I mean, I know you can always learn; every day is an opportunity to learn new things. But, you know, from your coalition’s perspectives, Zarek, what do you want to see happen so that, you know, that we’re not leaving behind big chunks of the population, you know, the older chunks of the population who haven’t had the benefit of going through this education system as it is now, let alone what it might be in five years time, right?\nZarek Drozda\nYeah, Brian, it’s a great question. And I, I just came off a year of serving as a fellow in the US Department of Education. And I had a lot of conversations with the programme officer who is responsible for research, education research and adult education, about that subject. Right. And I think one of the themes that I learned from that work and from that, from those conversations was this idea around fear of technology, as it accelerates. It’s really hard for people to deal with, you know, ChatGPT, DALL-E, AI, neural networks, you know, the list goes like on and on, and it changes every month, as as we’ve discussed here. And I think a big goal from our side is both to build confidence when people are dealing with a deluge of data and a increasing amount of information, right, which we talked about, and it’s also the confidence in the technology tools that are constantly changing. So I think as we think about, you know, a student K-12, we want to get them on a tool, so they can be confident and switch to the next one when it comes out. Because we know that technology integration is just is really critical. And the same thing is true for the adult learners, right, or for the for the folks that are older, there is a wealth of online digital learning, online courses, asynchronous experiences, to learn any coding languages, any programming language, any just software – doesn’t even have to be computation or coding intensive, right, it can just be spreadsheets or, or Tableau or some of the not-too-syntax heavy tools, and there’s so much digital and elearning opportunities for that. If we can build formal exposure into the classroom pathway, and build student confidence to then jump on to those later on. That is really critical. Because if you don’t get an exposure, it’s so hard to take the first step to jump in to the digital training or to you know, go to your employer and say, I want this, you know, this professional development programme, because it’s hard to know where to start. And we also worked on a data literacy training programme for Department of Ed employees while I was there and helping design that experience. You know, for professional or mid career, folks, I think the most important thing was you build confidence and create bitesize first steps to try to like tone down the fear, so people can approach the new world with confidence rather than just responding to it.\nBrian Tarran\nSo I just want to wrap up now, last question for you all, really, maybe if we start with Steve. From a policy perspective, Steve, you know, are there other initiatives on the horizon and things in the pipeline that, you know, people should be watching for in terms of trying to improve data science education across the board in the US, and maybe specifically for the ASA are their areas you’re going to be focusing your efforts and support on.\nSteve Pierson\nWe certainly do want to expand this effort. And we’re trying– In a way, this bill is serving as a way for us to gather that information, because people then know that we’re doing this and they might well hopefully suggest items to, to us. We’ve gotten some I know that, you know, Zarek has a file, I have my own file of what we might want to, how we might want to extend it just with this bill. But yes, we certainly do want to do that. And so for listeners out there that have ideas, please please send them to us. I won’t offer specifics at this point. And I’d love to hear what my colleagues have to say, as well.\nBrian Tarran\nDoes anybody want to jump in on that? Zarek, what’s on your list?\nZarek Drozda\nVery short term is just building a Senate version of the legislation, I think, right? We’re going to be working with with the ASA on that, to find champions in that chamber. But then I think I would go back to my call for you know, state legislators need to be thinking about this as well, right, because we know that every state is going to look a little different. We’re in a context right now where locally driven education solutions are going to be really important. And so we’re going to have to build different slightly different flavours of this all around the country. And we’re going to need a lot of work from, I think, state and local champions to fund and help support and build these experiences that are so critical for students across the country.\nBrian Tarran\nAnd are there, you know, on the point you mentioned earlier, Zarek, about this news,crossing the pond and reaching me over here in the UK, do you look elsewhere, you know, outside the US and the UK for other good examples of where education systems are starting to integrate data science into the teaching at, you know, the earlier parts of the school curriculum and the school levels.\nAnna Bargagliotti\nI think Chris can probably jump in even more than me on this one. But definitely in New Zealand. Our colleagues in New Zealand are fantastic. And they’ve been doing K-12 data science and statistics very well for many, many years. And Chris has some very close collaborators. So I’ll hand it over to her to on that. But I thought I’d mention it.\nChristine Franklin\nYes, I I think that that’s when you know, when Steve says how can we reach out, I think even beyond policy, a collaboration with international colleagues that have advanced their work in K through 12. I mean, I had the good fortune of having a Fulbright to New Zealand back in 2015. And just the inspiration, the wealth of knowledge that I obtained from there, to bring back to the US with our work here was phenomenal. Plus, we built up collaborations that we are continuing today. We have collaborations with people in the UK, for example, in other countries. I think the other thing I wanted to say besides reaching out internationally is that we as statisticians in the US need to be doing more to help K through 12. And I think about my, you know, my colleagues at the university in the statistics department which I was part of, my colleagues, some of them actually worked with me to reach out to the math educators so that they could try to help with what needed to be done with the preparation of teachers. So I think statisticians need to become more involved, both practising statisticians and academic statisticians, with helping educators at K through 12. And that includes trying to become involved with state departments of education as well, because that’s really where things filter down to the local level in our school districts. So I would like to see somehow a structure put in place to make that happen more. And I’m hoping things such as this bill will bring that awareness to practising statisticians, that this is really important, and you need to become more aware of what’s happening at K through 12, and become involved.\nZarek Drozda\nTo just add to what Chris is saying, I think it is so important for investment in the K-12. space. And, Brian, I’ll give you a sneak preview of a report that we were collating on international examples of data science and statistics education in K-12, and really serious investments that we’ve seen, I think, and Chris already mentioned, some really great ones that have been long running champions at this internationally. Our recent scan: Israel, their ministry of education is doing a tonne of work for data science, data literacy education. I’ve had so many conversations with them over Zoom. China added a standard semester for big data statistics coding and modelling. And it’s now also in their college entrance exam. There’s examples in Germany, New Zealand, South Korea, Scotland, we’ve we’re continuing to build the list. Frankly, in the UK, right, with core maths, you’re seeing more integration of data and computational thinking into the curricula pathways there. I think in many ways the US is behind, and as we introduce this bill, I think we should be messaging, there’s a economic competition aspect to this, right; that it’ll be really important for the US to make investments in this area to, frankly, catch up to where I think other international peers are.\nBrian Tarran\nSteve, you wanted to come in on that?\nSteve Pierson\nI mean, you mentioned like someone coming in mid career, right. And I think that that is really important. But we’ve also kind of been talking about the other ways you can access this, right, in any part of your career, but you can also access data jobs, rural and urban. So we’ve been kind of selling that dimension. But also, you know, just access in terms of diversifying the STEM workforce, we think that’s really important. But it’s also about degree level, right? We did a search for one member of Congress that had a major, I think it was a pharmaceutical in their district. And if you put in data, right, and thousands of jobs pop up for that company. And it’s not just the PhDs, right, it’s more entry level, the people who are what Zarek mentioned in terms of just accreditation, that you can enter a lot of points. But I also want to just make a– point out one part of the bill, which really singles out two-year colleges, which can help the mid career people, the early career people or others, and they face a lot of the same challenges as K through 12. Right, making sure that the instructors are upskilled, that they have a curriculum, but they also need the time to coordinate with their other disciplines that are involved here. They need time to go to that local workforce, what do you need in terms of data science? And for those students that want to go on to a four-year degree, they need to make sure that there’s a smooth pathway for those students. So there are provisions in the bill also for for two year colleges. And those would be my closing comments, Brian.\nBrian Tarran\nOkay. Anybody else for some closing words? Or should we wrap up on reminding everyone that data science education is the new apple pie?\nAnna Bargagliotti\nI can close this with something more philosophical, maybe. To me, I think a nice way to think about it or sort of a romantic way to think about it is it’s just data tell our stories, and they reflect what’s happening in our world today, much like art around us in some ways. And a way to think about just data science education is just to think about what we need, what we need our data understanding to be at each point in time in our educational career, or in our lives. And it’s not static, it’s an evolving thing. So you have to move sort of with the data that are being collected.\nBrian Tarran\nVery good point. Thank you very much, Anna, Steve, Zarek, and Chris, for joining us to talk about the Data Science and Literacy Act. I’m sure there’ll be much more to to follow and update on as this act or bill winds its way through through Congress. So we’ll look forward to hearing more about that in due course. So thank you for joining us today. Thank you to those of you who are watching for joining us. Stay tuned at realworlddatascience.net for more news Q&As.\n\nFind more Interviews\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “US legislators get their data science act together.” Real World Data Science, March 6, 2023. URL"
  },
  {
    "objectID": "viewpoints/interviews/posts/04/28/stephanie-hare.html",
    "href": "viewpoints/interviews/posts/04/28/stephanie-hare.html",
    "title": "‘I’m way more into prevention than cure’: Stephanie Hare on why we need a culture of technology ethics",
    "section": "",
    "text": "We’re about a year late coming to Stephanie Hare’s book, Technology is Not Neutral: A Short Guide to Technology Ethics. But, as discussed in our interview below, time has only made the text more relevant. The book was written pre-ChatGPT, but Hare’s explorations of ethical questions in the context of facial recognition technology and Covid-19 exposure tracking apps feel both pointed and urgent at this moment, when researchers, regulators, and regular people are weighing the opportunities and potential harms of large language models and generative AI tools.\n“We’re having some sort of moment with technology ethics – AI ethics being just a branch of that,” says Hare. Reflecting on her career, spanning 25 years, she says: “The stuff that we’re talking about today that dominates the headlines – that is dominating the discussion in the tech sector – was not discussed at all at the turn of the century, other than by maybe people in the science and technology studies domain or academics. But it wasn’t filtering into boardrooms. It wasn’t on the front pages of newspapers, and it wasn’t being covered in the national news. So, it’s amazing. A whole field has sprung up.”\nHowever, as Hare makes clear in our interview, we still have a long way to go to build a culture of technology ethics throughout society. Check out the full conversation below or on YouTube."
  },
  {
    "objectID": "viewpoints/interviews/posts/04/28/stephanie-hare.html#timestamps",
    "href": "viewpoints/interviews/posts/04/28/stephanie-hare.html#timestamps",
    "title": "‘I’m way more into prevention than cure’: Stephanie Hare on why we need a culture of technology ethics",
    "section": "Timestamps",
    "text": "Timestamps\n\nChatGPT: just another “flavour of the month” in the tech industry? (1:18)\nHas concern about large language models helped put technology ethics on the map? (3:17)\nWhat will it take to build a culture of technology ethics – in society, in academia, in industry? (9:16)\nDrawing lessons from history (12:15)\nWhy technology ethics is a “wicked problem” (24:57)\nChecklists and changing mindsets (29:49)"
  },
  {
    "objectID": "viewpoints/interviews/posts/04/28/stephanie-hare.html#quotes",
    "href": "viewpoints/interviews/posts/04/28/stephanie-hare.html#quotes",
    "title": "‘I’m way more into prevention than cure’: Stephanie Hare on why we need a culture of technology ethics",
    "section": "Quotes",
    "text": "Quotes\n“The European Union has the AI Act coming down the pike. It doesn’t cover stuff like ChatGPT specifically, but then I don’t know if you want good regulation to cover the technology itself, or how technology is used. I talked about this in my book: do you want to regulate forks – a tool – or do you want to regulate use cases for forks? We’ve regulated the use case, if you will, of murder, or of injury with a fork – or, frankly, any other tool. So it’s the use case we focus on. We don’t really regulate forks. [But] we do regulate some technologies, like biomedical technologies, human genetic stuff, anything nuclear. So we just need to think about where does AI fit with that?” (5:42)\n“Move fast and break things was the mantra for this culture [in the technology industry] for a really long time, at least out of the US. And it made a lot of people a lot of money, and they got worshipped by the media. And, you know, they have a whole audience of ‘bros’ who are fans of them. And they’ve never really, any of them, been held to account for what they’ve built.” (11:56)\n“Another generation or two, when we’re older, might look at some of what technology we’ve built or our behaviour on climate change, our track record – did we do what we could have done to slow global warming, to improve biodiversity? – and they might hold us to account, saying, ‘You could have stopped this and you didn’t, right? It’s not just what you did. It’s what you did not do.’ So we have to be super careful when we think about ethics, because ethics change, values change over time. And what seems okay today may not be okay in 10, 20, 30 years time. That is on my mind all the time. It’s not very relaxing.” (18:30)\n“[Laws and regulations] are important, they’re necessary, but they’re insufficient. You can act a lot faster if you can get people preventing stuff from being built in the first place, and that means you need to have a culture of people working in technology, both within the organisations – whether that’s research labs, government, companies, universities, whatever – and on the outside – journalists, academics, thinkers, etc., or just the public, an informed public – who can see something and sound the alarm and go, ‘Wait a minute, hang on. That’s not okay.’” (33:02)"
  },
  {
    "objectID": "viewpoints/interviews/posts/04/28/stephanie-hare.html#transcript",
    "href": "viewpoints/interviews/posts/04/28/stephanie-hare.html#transcript",
    "title": "‘I’m way more into prevention than cure’: Stephanie Hare on why we need a culture of technology ethics",
    "section": "Transcript",
    "text": "Transcript\n\n\n\n\n\n\nThis transcript has been produced using speech-to-text transcription software. It has been only lightly edited to correct mistranscriptions and remove some repetitions.\n\n\n\nBrian Tarran\nHello, and welcome to another Real World Data Science interview. I’m Brian Tarran. And today I’m joined by Stephanie Hare, a researcher and broadcaster and author of the book Technology is Not Neutral: A Short Guide to Technology Ethics, which is the focus of our conversation today. Welcome, Stephanie.\nStephanie Hare\nThank you so much for having me here.\nBrian Tarran\nI feel I’m a bit late to the party with the book. The Financial Times picked it up as one of the best books of summer 2022. But I’ve only just got around to reading it.\nStephanie Hare\nI mean, I only just got around to reading War and Peace last year. So there’s no rush with these things.\nBrian Tarran\nOkay. Well, I mean, I can definitely say it’s one of the best books I’ve read in spring 2023, if that helps, and the only other one I read was Lord of the Rings. So–\nStephanie Hare\nWow. I mean that’s some pretty august company, you couldn’t thrill me more.\nBrian Tarran\nExcellent, excellent. Well, I actually thought coming late to the book might actually have been of benefit to me as a reader because, you know, you’re talking about technology ethics quite broadly. But then you focus in on a couple of use cases, specifically around facial recognition, technology, Covid-19 exposure tracking apps and things like that. But, you know, obviously, since the book was published, the whole discussion around technology ethics has kind of been dominated maybe or taken on a new dimension following the launch and adoption of ChatGPT. I wonder, you know, when the technology was launched, people started using it, adoption rates, you know, went through, went through the roof, what was your kind of initial reaction to all that and what have you made of the kinds of conversations and criticisms that have followed?\nStephanie Hare\nWell, I mean, like everybody I was curious and fascinated and wanted to play around with it a bit. I don’t think I’m of the school of thought that seems to be circulating that this will either you know, destroy mankind as we know it or take everybody’s jobs or potentially upend civilization. There’s been some quite extreme, some quite extreme views put across in the media in the past few months since this was widely released to the public. I don’t know, for some reason, I didn’t drink the Kool-Aid, when I started working in technology. So I always take these things with a grain of salt. And I guess my, my cautionary note to anybody listening to this is you know, at this time last year, all of my clients were wanting presentations and analysis about web3 and NFTs and cryptocurrency and before that, it was blockchain. There’s like always a sort of flavour of the month. AI, for people who’ve worked in this field, is known to have winters, summers, springs, autumns, you know, these seasons of when it’s like coming on and really exciting or not? I’m more excited by DeepMind’s use of artificial intelligence, I think they’re actually working on interesting problems, right, around like protein discovery, like real science, as opposed to like, oh, look, I can have a new sci fi avatar or do a deep fake, you know, people can do deep fakes already. We’re just doing them now in even more disturbing ways. So I guess, I guess it’s that. I’m intrigued by it. But I don’t I don’t feel the need to sort of freak out. Either way, positively or negatively, I have a much more sort of detached satellite-level view, I think probably just because I’m older, seeing these trends come and go. And it’s like, let’s just let’s just wait this out and see how it goes.\nBrian Tarran\nFrom your perspective as someone who’s interested in and researching in the area of technology ethics, right, do you see a kind of almost a benefit that the conversation around this has put technology ethics, the conversation around that, on the map? Or do you worry that we’re kind of obsessing over this one technology and this one application? We’re not looking at the field more broadly?\nStephanie Hare\nWell, there’s a few things to say on this. So like, first of all, a couple of weeks ago, a bunch of people working in AI, about 1000-plus people – including some fictitious people, by the way – sign this this letter calling for a moratorium on AI research for six months, which was unenforceable, clearly not going to happen, was signed by Elon Musk, who then very quickly announced he was developing his own rival to OpenAI, the company that invented ChatGPT. So you take all of that with a grain of salt. But again, if you’re, if you’re an historian, or if you just have a long memory, you’ll remember that there have been several letters like this. There’s always somebody, you know, very big-wig people. It’s not that we want to dismiss it. But Stephen Hawking and Elon Musk were warning, you know, over around 10 years ago that AI was going to kill humanity if we didn’t put guardrails on it. Professor Stuart Russell talked about this in his Reith Lectures a couple of years ago, which are still online and you can listen to them. And you know, he’s not an alarmist. He’s a serious person and a serious thinker. So we want to listen to him. But I guess what I’m just saying is, you know, every time some sort of new technology or new use case for technology comes up, there’s a group of people who come out and freak out and they get lots of op-eds. It’s usually men, I must say. There’s a lot of women doing some really interesting scholarship in this area that don’t get the op-eds and quite the publicity. So there’s that. Then it is interesting because it makes people think about technology ethics, usually, again, from a place of either fear, right – Are they going to kill us? Are they going to take our jobs? Are they going to remove human agency? – or money – How are people going to make a huge amount of money? Who’s going to make the money? And by displacing whom, right? So, we have two levers: incredible doom or incredible opportunity. And that leaves the rest of us, I think, probably somewhere in the middle, scratching our heads and going like, is this going to actually change my life? And if so, how, and do I really care, given that I’ve got like, you know, a cost of living crisis, recovering from the Covid pandemic for the past few years? Like, if you’re not in this world, it can seem like a lot of shouting. There’s also the question of, do we need new laws? So we know that the European Union has the AI Act coming down the pike. That’s supposed to be passed this year, and there’ll be a two year implementation grace period. So that’s interesting. It doesn’t cover stuff really, like ChatGPT specifically, but then I don’t know if you want good regulation to cover the technology itself, or how technology is used. And I talked about this in my book, like, do you want to regulate forks – a tool – or do you want to regulate use cases for forks? So if I’m, if I stab you, or kill you with a fork, which is totally possible, that is something that we’ve regulated; we’ve regulated the use case, if you will, of murder, or of injury with a fork or frankly, any other tool. So it’s the use case we focus on. We don’t really regulate forks. We do regulate some technologies like bioethics technologies, or biomedical technologies, excuse me, sort of human genetic stuff, anything nuclear. Those technologies we do specifically regulate. So we just need to think about where does AI fit with that? And also, do we need new regulations for everything, or can we use existing ones? And that’s what’s becoming really interesting is that in the US, where I’m from, the main regulator, the FTC, seems to think that it can use a lot of existing laws already. So they’ve been like, if your AI is claiming to do stuff that it can’t, we’re gonna come after you under, like, kind of sort of false advertising, if you will, misrepresenting yourself. They might come after some of the big AI companies based on anti-competition law, right? So no new laws needed for that. And then with the music industry, they’ve been going after all the people who are like, oh, let’s like remix a Drake song, and saying, well, actually, you can’t do that, because you’re violating copyright law, take it down. Right. So again, I don’t want to be like too, too calm about it. Like, we do need to look at some of the use cases that are really problematic and hurting people. But we might actually have a lot more in our arsenal to combat this than we’re currently using. And I think what’s going to happen is, unfortunately, the pace of crafting legislation, and then regulators never fully enforce regulations– Look at the GDPR: no company’s ever been given the full fine. And here in the UK, the ICO is famous for letting companies off the hook, giving them less than half of the original fine. It’s ridiculous. So if you’re going to pin your hopes on regulation, I’m not sure that’s great. I’m weirdly more optimistic about landmark legal cases. So we’re seeing an Australian mayor who was totally defamed by ChatGPT, in Australia, he’s going to be taking or is taking OpenAI to court. And then we might see some of these copyright issues, that could be taken to court, right. And like, that’s where people I think will get more action and, frankly, more respect, because these companies are really happy to pay a lot of money to lobby our lawmakers, and water stuff down. And they always say, Oh, my God, it’s going to constrain innovation. And if they really get desperate, they’ll be like, China! If we don’t, if we’re not allowed to do everything we want, China will win! That is like a– that is just a game that takes everybody nowhere, whereas in the lawsuit angle, that’s interesting, because you’re demonstrating responsibility, you’re discussing liability, you’re having to demonstrate harm. And in the process of discovery, right, you might be able to actually get some of these companies to open up their datasets, how their algorithms work, like, I’m much more intrigued to see where that’s gonna go.\nBrian Tarran\nYeah, but I think I mean, having read your book, I would, I would have thought you might perceive all of this sort of stuff as kind of sticking plasters to put over the the injuries that might be caused by these technologies, right? Your argument seems to be that we have an issue whereby we don’t have a culture of technology ethics. So when we’re thinking about building these tools, or when we’re starting off down the path of creating something like this, we’re not already thinking about, you know, the use cases, the harms that might arise from that and things like that. What does it take to build a culture of technology ethics, do you think, in our society, in our academic institutions in our companies?\nStephanie Hare\nHonestly, I think, I think this whole accountability piece is going to be what it takes. Because you see, like Alphabet CEO Sundar Pichai gave an interview recently to CBS 60 Minutes in the US where he was like, yeah, there’s a risk that this technology could get out of control, like dot dot dot, this would be terrible for mankind. And you see him kind of be like, hope somebody does something about that. And it’s like, I know somebody that might do something about that, Mr Pichai – you! But clearly he feels, and you can see his point, he feels that right now, if it’s not illegal, then it’s permissible. And he has to win market share. If he doesn’t do it, he’s going to lose. And companies have this all the time. If they wait too long, they lose their first-mover advantage, and they get destroyed. We can go through like countless examples of that in business, particularly in technology. So I get it. But what he isn’t understanding is that if his company is the one that puts out the technology that leads to terrible harm – you know, physically killing people, harming them, destroying the national security infrastructure, something like that – right now, I don’t think he’s thinking about how that’s going to affect him. And that’s because we don’t really penalise executives very often. The worst that might happen is they might leave with a huge golden parachute, and go off and sort of retire in Hawaii with their millions, right? Like nothing really happens to them. So how do you have a culture of technology ethics, where the people who are creating technology and have the power to stop, right, to maybe like back off on stuff, they aren’t really thinking about how will I personally be held responsible if this goes south? So like, Sam Altman and OpenAI, same thing, he was like, he gave an interview where he’s like, I’m really scared about this technology I’m building. It’s like, okay, you could slow down or back off, you could make your datasets open, you could make your algorithms open. You’re called Open AI, that was supposed to be your whole mission, why you were created was to benefit humanity, like, what are you doing? So it’s weird. And I think it comes from the fact that, you know, move fast and break things was the mantra for this culture for a really long time, at least out of the US. And it made a lot of people a lot of money, and they got worshipped by the media. And you know, they have a whole audience of bros who are fans of them. And they’ve never really, any of them, been held to account for what they’ve built.\nBrian Tarran\nSo your interest in technology ethics clearly predates you know, that all the noise at the moment around large language models and generative AI and things like that. What was it that got you interested in this subject? Was it a particular application, something that caused some concern? Or– How did it come about?\nStephanie Hare\nThis is gonna sound completely weird, but it didn’t come from my experience in tech, really, at all. I have had two paths in my adult life, one has been working in these technology companies with a brief but happy foray in political risk, which is now sort of part of the skill set for tech. But I trained as an historian, and I interviewed someone who was a French civil servant, who at the end of his life was put on trial for crimes against humanity for his actions as a young civil servant during the Second World War. So he collaborated, as so many French civil servants did. And in the course of that collaboration, over a period of many years, went from just, you know, just signing documents and kind of doing what he was told to do, to deporting people and sending them to Auschwitz. So I was very young when I interviewed him, and that marked me, as I would hope it would mark anybody. I talked with him on and off for about three years, until he died. And that was the subject of my PhD. And then I did a fellowship at St. Anthony’s College, Oxford, and spent years looking at it further. And that’s actually going to be my next book. I needed a long time to sit with that material and to read around it, but I had to get the interview while he was still alive. He was so old, he was 93 when I started talking to him, and so it was important to get that down for posterity’s sake first, and then circle back and do some analysis later when I was a bit older. When you talk to somebody who in his case was, you know, not antisemitic, not of the far right politically, was actually like centre-left, had lots of Jewish friends, etc. Was like top of his class, you know, came from a milieu and a background and a formation that I think many of us would read and be like, Okay, that seems pretty reasonable. You ask yourself, how on earth did that person in his, like, young period of his late 20s, early 30s end up being involved and actively participating in what ends up being mass murder. It’s probably the most extreme case study of ethics, or one of the most extreme case studies of ethics that I could have stumbled upon. And it stayed with me and to be honest, it shapes a lot of my work and how I think about human rights and civil liberties and the freedoms that we so often take for granted because I’ve studied, as an historian looking at France and Germany, how quickly those things can be taken away – very quickly, terrifyingly so, in fact. So that lens is always with me. And when I was then working in technology, and seeing some of the things that could be done with these tools and watching this lack of accountability, down to the point of gross negligence in some cases. And also, as a young technologist, not being given any training – we were given no training at all in ethics, in, like, discussing data protection – it was basically: this is the law, just obey the law, like, that’s the, that’s the box that you have to play in. Other than that, like, go for it. And when I look back on that now it’s like, Oh, my God, that’s the equivalent of putting your family in the car, and everybody goes off without wearing their seatbelts on and, you know, all this sort of safety design that we take for granted in cars now, it’s just mad when you think about it, or the way we used to fly. We’re in this phase, it’s really interesting, just over the course of my career – 25 years – where the stuff that we’re talking about today that dominates the headlines, right, that is dominating the discussion in the tech sector, was not discussed at all at the turn of the century, other than by maybe people in the science and technology studies domain or academics. But it wasn’t filtering into boardrooms. It wasn’t on the front pages of newspapers, and it wasn’t being covered in the national news, whereas like now that is all I’m doing. So it’s amazing. A whole field has sprung up.\nBrian Tarran\nI think that that kind of origin story, if you like, explains some of your, perhaps, belief in the importance of exploring this accountability question when it comes to technology, ethics?\nStephanie Hare\nYeah, because I watched it, and I think what was so fascinating– So as I say, I was in my early 20s. In fact, I was 20, when this man was put on trial, and I had just moved to France. It was the longest trial in French legal history – it was a big deal, you could not not watch it. So I was reading this and seeing it in the press every day, and I watched the French people discussing it around me, you know, really being divisive, this stuff does not go away. And his view was: I was just following orders, I was doing what I was told to do. Which you know, you hear that a lot from engineers or people who are like, this is the design spec I’ve been given, or this is what my boss has told me to do, or this is what our investors want, etc. Or people feel they don’t have the power to stand up because, you know what, they’ve got a mortgage, they’ve got kids, and employers know that, like, they know that and they use it as leverage against people to silence them. Or they’ve signed an NDA, because we get made to sign these NDAs when we work in tech, and then we get made to sign another NDA when we leave, right, so we can’t disparage our employer, and maybe we’re given some money so we don’t talk about the things we’ve seen. You know, it’s, it’s gross – it’s a gross little world, and like you have to be very, very solid and take good care of yourself to work in it, I reckon. To try and keep your ethical and moral compass. It’s hard. So I think because I saw that. And I saw that someone who – whether we believe him or not, this is what he claimed – in his 30s, he was just doing kind of what everybody around him was doing under a situation of crisis. He was let off the hook. I mean, he wasn’t just not persecuted in 1945, he was actually promoted. And then he became France’s top civil servant, and then he became an MP, and then he became budget minister. I mean, this guy’s career was not hurt in any way by what he did. On the contrary, right, he advanced. And yet, by the end of his life, French values had changed, so a new generation wanted to hold him to account. And I think about that a lot for all of us, right, who are sort of walking around in our 30s or 40s. Another generation or two, when we’re older, might look at some of what technology we’ve built or our behaviour on climate change, our track record – did we do what we could have done to slow global warming, to improve biodiversity? – and they might, they might hold us to account saying, you could have stopped this and you didn’t, right? It’s not just what you did. It’s what you did not do. Right. So we have to be super careful when we think about ethics, because ethics change, values change over time. And what seems okay today may not be okay in 10, 20, 30 years time, and we might be the 80- or 90-year-olds who are put on trial. That is on my mind all the time, right. It’s not very relaxing.\nBrian Tarran\nNo, and I guess it makes me think. Well, I mean, this is getting into the hypotheticals right. But is it– if we can’t necessarily predict or plan out how values might evolve over time, is it enough to be able to, to just say or to document that we asked the right questions at the time, rather than just doing things blindly. Is that where we need to kind of almost formalise our process of writing down, setting out, you know, we want to do this, we’ve considered these potential harms, we’ve considered these potential benefits, and we kind of document that so at least, you know, future generations can say well, “They thought about it. They might have not thought about it in the right way, but they tried”?\nStephanie Hare\nAbsolutely, I think, you know, show your work and be like, these were our, you know, these were our sort of first principles of where we were starting from, this is the context in which we were making this decision. Because again, I don’t, I don’t necessarily fear the judgement of history in terms of if I get something wrong. People get stuff wrong all the time. That’s just being human. It’s, did I not care? You know, was I like, well, sorry, little little boys and girls who are babies now, like, I need to do my stuff, and like, I don’t care about you, right? That attitude is tough. Or I decided I just really, you know, I really needed to buy a flat. So I decided to work for some dodgy company, or dodgy, dodgy company that’s owned by a foreign government, but I knew it was going to be fine, and they’re offering me a tonne of money, and now I can go on nicer holidays. I’ve had these conversations with people about this literally this past week, like, these are live issues for people. There’s a cost of living crisis, ethics can feel like a luxury for some people rather than a necessity. And human beings are very bad, all of us, at thinking about, you know, future selves, right? Like we kind of, we optimise for how we’re feeling now, and we’ll deal with 20 years from now later if we even get there. So I think there’s that. There’s also– this really inspired the writing of the book, Technology is Not Neutral. I knew, I had this weird sense – I had just gone independent, so I had left working for these companies, I was not under any NDAs anymore, which right there gives you a clue; I could say what I wanted – but I also knew there was a chance that I was going to have to go back either into industry, or maybe work for a government, I don’t know what I’m going to need to do in the future or who I’m going to want to work with or what reasons I might even have for that. But I knew I had this window of being an independent researcher and broadcaster, that I could say whatever I wanted, and I had that thing of like, okay, if you’ve had a window of, say, five years, for example, what would you say if you were not afraid? If you were not scared? If you were like, you know, screw the money, screw the corporate pressure, screw the government, whatever, what do you want to talk to the public about? And my views were, I really wanted to talk to them about facial recognition, because I feel people just fundamentally do not understand how dangerous that technology is and how it can be used. I wanted to talk about the pandemic technologies, because we were, you know, I was writing it during the pandemic, and I thought, well, if a pandemic ever happens again, let’s have a nice little tidy case study for potentially future historians or future medical personnel, public health officials to pull out, because when the pandemic hit, we all had to go back and look at stuff from the Spanish flu. You know, there’s a lot of discussion of like, why has this come as such a surprise? Are we going to use these technologies again or not? Right. Like, you know, is it worth it? Is the return on investment worth it in all senses – ethically, as well as medically, all of those things? So I thought I would lay down a couple of markers that I hoped would stand the test of time. But the big thing I wanted to do, because I was always thinking I might have to go and sign another NDA and go work because I too must earn my living, was I wanted to write something so that anyone who cares about technology, is working in it, is investing in it, right – it’s not just people who code, it’s people who fund the people who code. Buying technology – procurement is massive, you’re a really powerful person if you’re in charge of procurement. But also just consumers, and citizens and parents, and teachers and kids. If I could write up everything that I had learned in my 25 years, and succinctly as possible, right – as short as possible, because people are tired, they’re busy – I could pass that baton on, so that if I ever have to stop going on television and radio, and I’m no longer allowed to write in newspapers and warn people about the stuff I’m seeing and the abuses of power, and showing them examples of history of how this can go so terribly wrong, maybe it will have, like, lit somebody else. And I’m delighted to report – I mean, we’ll see; time will tell, it’s only been out a year – the amount of people who have brought me in to train their staff, to talk to their board. I’ve talked to children. I’ve talked to university students, I’ve taught classes all over the world, because we can now do online teaching. I’ve taken a lot of it on television and radio and in the newspapers. People wanted this, and I’m not the only person working on it, of course – there’s been a whole flowering of people, scholars, etc., working in putting out amazing books and documentaries. It’s really, we’re having some sort of moment with technology ethics – AI ethics being just a branch of that. So that’s really encouraging. So I sort of feel like, you know, again, if I, if I’m gonna have to account for myself at the age of 93, I would like to be able to point to that and go, I tried. I tried. And I have no idea if it will succeed or not, but I stood up to the plate and I swung the bat and, you know, I aimed for the bleachers.\nBrian Tarran\nOne of the things I thought was really interesting about the book, it comes towards the end when you’re kind of talking about, you’re summing up, and you talk about how your thinking about almost like the the approach or solution to the technology ethics issue has changed over the course of the writing of the book. You had, like, a list of potential, like, proposals, proposed actions that you wanted to analyse, but then you realised that actually technology ethics is a “wicked problem”. I wonder if you could explain what that term means for people who might not be familiar with it and and why you think of it that way?\nStephanie Hare\nYeah, I’m so grateful to have learned about the term “wicked problem”. My friend Jason Crabtree, who wrote an amazing book about electricity grids, like smart electricity grids, for Cambridge University Press, had asked me to read his manuscript maybe 10 years ago, and I read it, and one thing I took away that just absolutely blew my mind was this concept. So I shall gift it to you for those of you have not heard it. Because then suddenly, you’re like, God, it makes so much sense. There are certain problems, I would say, like, the climate crisis, and biodiversity loss would be a good example of this. There’s certain problems that have many causes, many causes, so there isn’t going to be one solution to fix them. So people constantly ask me, oh, is this the magic bullet? No, they’re like, there are certain problems that there is no magic bullet – the pandemic is probably another, actually. Then, if you do try to solve these wicked problems, the mere act of solving them can introduce a whole new set of problems to them. So like, it becomes even more of a head– You know, I’m trying not to swear, but a messing with your head moment. And it’s exhausting, you know, and it gives you your forehead wrinkles and makes you just sort of want to bang your forehead onto the nearest wall. And yet, you also can’t opt out and be like, well, it’s just too hard. It’s a wicked problem. There’s no solution, there’s nothing to be done, you know, throw up your hands, because you’re like, Yeah, but the problem is, is if we don’t do anything, like literally people are dying; literally, climates are becoming uninhabitable, we’re going to have massive climate migration, that’s going to cause all sorts of problems, water scarcity, we can have wars over this, like, we have to do something. So like, you have to still act on a wicked problem, all while knowing that it’s not going to be solved in a binary sense of like zero, one, black and white, I can point to it and measure it. And for people who like metrics, that’s a real pain, because they’re like, I want to know what good looks like and I want to know how we’ll know when we get there, you know, what’s the percentage, what’s the number? And you kind of have to be like, Well, with a wicked problem, you might never solve it, or you won’t solve it once. Because again, with something like climate change, or pandemics, these are things you’re probably gonna have to solve again, and again, and again, because it’s dynamic. And it’s constant, you know, we’re always going to be managing our relationship with the climate, with the environment. So, you know, we can pick a certain temperature, or a certain percentage of landmass that, you know, has trees or whatever, and like, come up with a little metric for our metric-oriented friends. But that’s still not very meaningful. So it’s more that when you think of a wicked problem, like facial recognition technologies, like, we need to be able to identify people in certain situations, and like, we want that, right. Like, we want to be able to catch criminals, and we want to be able to catch terrorists when they’ve managed to pull off a terrible act of, of harm to people. But at the same time, do you want to turn your society into a sort of permanent dragnet? Do we not care about privacy? If so, like, when do we care about privacy? And when are we okay with maybe sacrificing that for the greater good and who decides that? It’s really problematic if you live in London as I do, and your police force, which is using this technology, has admitted, they’ve admitted themselves to being misogynist, institutionally misogynist, homophobic, racist, right? And then we’re gonna give them a technology that doesn’t work very well on people with certain skin. It doesn’t identify people of a certain age as well. It’s got all sorts of problems. So you’re kind of like, hmm? Facial recognition is covered a bit under the EU AI Act. But even then there’s like so many loopholes. And the thing is, if you cite national security, it usually gets waved through because no one wants to be the person who said, I said we couldn’t use this technology, and then something bad happened. Right? So you err on the side of, like, the precautionary principle. The default is, let them use it. We must trust them. Except what do you do if your police force has given you quite ample evidence not to trust them? Or companies. You know, this is not to bash on the police, by the way – companies are some of the worst offenders in this area. So that’s what I mean about it being a wicked problem is, it’s out there. It’s installed. It’s all over the UK, it’s definitely all over the US as well. And we don’t really have a good framework for it.\nBrian Tarran\nBut then this is where we loop back around to the kind of culture, right, creating a culture of technology ethics? You know, we can’t just put a checklist in place once, do it, tick it off, yeah, we’ve done that for facial recognition technology, we’re good to go. Because there are always new potential use cases for it, new applications, new integrations with different systems that we always need to be thinking, every time, is this the right thing to do? Or–\nStephanie Hare\nI mean, I’m still a big fan of checklists. So it’s not that I’m anti-checklist. And I’m not saying that you said that, by the way, I’m more thinking aloud. Checklists can still be useful, right? Like, whenever I’m in a really bad mood, I’m like, Okay, hang on, have I slept? Do I just need a glass of water? Am I hungry? You know, what I think is angry may just be that I skipped lunch. You can kind of go through those things, and anybody who’s had to like troubleshoot why a baby or a small child is unhappy will also have a checklist and what’s on the thing – ah, they missed naptime, where’s their bear? That sort of thing. Companies need checklists, because you’re trying to get loads of people singing from the same hymn sheet, I get that. But what I wanted to get away from was this idea that, like, one person or one team gets this checklist and maybe does that once a year, once a quarter, pick your cadence – and everybody else gets a pass. Ethics doesn’t work that way, because again, ethics is kind of I think in the wicked problem scenario of, like, how do we decide what our values are and how to live them? And how do we know, where do we draw the line? And then how do you, how do you decide if you’ve gone over the line or not? And all of that, who decides who decides? Those are really complex questions that mean that really you can’t abdicate. This is, in a company’s case, it’s the CEO, it’s the board, all the way on down – it has to be baked in to every single employee, and also investors, mindset. And I was thinking about it in terms of cybersecurity, I once had a colleague who gave me an analogy that I think is helpful, so I’ll share it for what it’s worth: When you go on to, say, an oil rig, in the North Sea – a highly dangerous environment – you might be the most junior person there, and you’re there for your very first day of work. But if you spot something on that rig that is a health and safety risk, you have to speak up. You’re not going to go, like, Oh, my boss might say something, whatever, because, like, everybody’s life on that rig is depending on everyone having that culture of careful, that’s not okay. And that really put me in mind where it was like, Oh, wow, we’re going to have to like inculcate an entire new mindset. And we think about technology ethics a lot because of technology being the word, and we think it must mean like hardware or software, it’s always about coding, and it’s often guys in hoodies coding. But my preferred method of hacking is culture. Right. So like, again, if we tried to just solve everything through regulation and laws, that takes, you know – if you look at the average time it takes to pass a law and then for regulators to enforce it – ages, we’re talking years, like it’s too late. These technologies will have moved on. Ditto, calls for international treaties. Do it, by all means. Have a look at how long it takes most international treaties to get passed and then ratified – and then, P.S. What happens when people break them? Really, right. So like, they’re important, they’re necessary, but they’re insufficient. You can act a lot faster if you can get people preventing stuff from being built in the first place, and that means you need to have a culture of people working in technology, both within the organisations – whether that’s research labs, government, companies, universities, whatever – and on the outside – journalists, academics, thinkers, etc, just the public, an informed public – who can see something and do what I just described on the oil rig, like, sound the alarm and go, Wait a minute, hang on. That’s not okay. That is, that to me feels faster. And I’m way more into prevention than cure, for all sorts of reasons. So I think like, yes, to laws and regulations, yes, to treaties; this will be faster. And I think it will be more resilient.\nBrian Tarran\nYeah, I agree. And I have to say, wrapping up, that I think Technology is Not Neutral is a great place to start to inculcate that mind shift, that mindset change. So, Stephanie, thank you very much for joining us today.\nStephanie Hare\nThank you for having me.\nBrian Tarran\nYou said you’re working on a new book. Have you got a timeline for that? Or a title?\nStephanie Hare\nNo. I am the slowest thinker and writer, I’m like the opposite of move fast and break things, I’m like move slowly and like think it over maybe several times. So I’m just getting started out. I’ll sort of go five years. It’s gonna be, it’s a history book. Alright. So this is, this is different, I’m having to take my classes in French and German right now to get kind of match fit in those languages again, and then you know, I’ll be off and writing. But, yeah, I hope to have another book out, you know, in five years.\nBrian Tarran\nWell, if the year it took me to read Technology is Not Neutral is any indication, in three, four or five years time it will still be relevant today. So–\nStephanie Hare\nThat’s the thing with history, it always stands the test of time.\nBrian Tarran\nWell, thank you, thank you again for joining us on Real World Data Science. It’s been a pleasure talking to you.\n\nFind more Interviews\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This interview is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Photo of Stephanie Hare is not covered by this licence. Photo is by Mitzi de Margary, supplied by Stephanie Hare and used with permission.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “‘I’m way more into prevention than cure’: Stephanie Hare on why we need a culture of technology ethics.” Real World Data Science, April 28, 2023. URL"
  },
  {
    "objectID": "viewpoints/interviews/posts/02/03/heidi-seibold.html",
    "href": "viewpoints/interviews/posts/02/03/heidi-seibold.html",
    "title": "Why open science is ‘just good science in a digital era’",
    "section": "",
    "text": "Years are often dedicated to different causes and aims by different organisations. The United Nations, for example, has designated 2023 the International Year of Dialogue as a Guarantee of Peace, while for the European Commission it is the European Year of Skills. But over at the White House Office of Science and Technology Policy, 2023 has been declared the Year of Open Science.\nTo discuss what this means for science generally and data science in particular, Real World Data Science invited Heidi Seibold for an interview. Seibold is a statistician and data scientist, and also an open science trainer and consultant, and we talked about how she became involved in open science, what it means to her, the benefits of it, and how academic and industry researchers can move towards it.\nCheck out our full conversation below or on YouTube."
  },
  {
    "objectID": "viewpoints/interviews/posts/02/03/heidi-seibold.html#timestamps",
    "href": "viewpoints/interviews/posts/02/03/heidi-seibold.html#timestamps",
    "title": "Why open science is ‘just good science in a digital era’",
    "section": "Timestamps",
    "text": "Timestamps\n\nHow did Heidi become interested in open science? (1:46)\nWhat does open data science mean to Heidi? (7:50)\nWorking with PhD students on open science (12:00)\nHow do open data science principles fit into an industry environment? (14:10)\nKnowledge transfer and public science (17:29)\nYear of Open Science initiatives and lasting impacts (21:08)"
  },
  {
    "objectID": "viewpoints/interviews/posts/02/03/heidi-seibold.html#quotes",
    "href": "viewpoints/interviews/posts/02/03/heidi-seibold.html#quotes",
    "title": "Why open science is ‘just good science in a digital era’",
    "section": "Quotes",
    "text": "Quotes\n“Reproducibility [is] just like this minimum standard in research quality, where we say, ‘When we have the same data and the same analysis, we also want to see the same results’, and being able to check that from others is really, really important.” (2:45)\n“On the back of my wall here in my office I have written, ‘Open science is just good science in a digital era’… Before, we only had the printing press, and we had to print journals in order to distribute the knowledge that we have.” (5:23)\n“For me, open data science entails the part of the scientific process that focuses on everything that happens on the computer: the data processing and the data analysis, and then getting from the data analysis – getting the results and the knowledge, really – in sort of a pipeline where you go from one step to the next. And so the image that I have in my head, when I think about open data science, is of a pipeline.” (10:16)\n“Nobody’s perfect from the beginning. And open science and reproducible research is really hard, and it requires a lot of technical knowledge. And I always feel like people are so scared, because on one hand, they don’t know how to do it yet, and the goal is so far away. And so I always like [to say], you don’t have to be perfect right away; going one step into the right direction is super important.” (12:34)\n“If we think of companies, for example, like Microsoft – they put a lot of money right now into open source: they bought GitHub, they publish open source software, they put money into open source software projects like R, for example. So, somehow, this must be a good way of making profits.” (15:09)\n“[Open science for the public has value because] we don’t know what ideas people will have. There’s so many skilled people out there that probably will do amazing things… We have this with the software Stable Diffusion right now. That’s an AI that generates images from text, and it runs on my computer here. And I don’t need AI skills to be able to do that. And people are building such incredible images out of this, and it’s really fun to see.” (18:50)"
  },
  {
    "objectID": "viewpoints/interviews/posts/02/03/heidi-seibold.html#transcript",
    "href": "viewpoints/interviews/posts/02/03/heidi-seibold.html#transcript",
    "title": "Why open science is ‘just good science in a digital era’",
    "section": "Transcript",
    "text": "Transcript\n\n\n\n\n\n\nThis transcript has been produced using speech-to-text transcription software. It has been only lightly edited to correct mistranscriptions and remove repetitions.\n\n\n\nBrian Tarran\nHello, and welcome to another Real World Data Science interview. I’m Brian Tarran. And today I’m joined by Heidi Seibold, a statistician and data scientist. I invited Heidi along to speak about open science, what it means, the benefits of it, and how to move towards it. So welcome, Heidi, how are you?\nHeidi Seibold\nHello, thanks for having me. I’m good.\nBrian Tarran\nExcellent. Excellent. Well, Heidi, I contacted you because, you know, I know you have a real deep interest in open science. And the conversation I think was really motivated by the White House Office of Science and Technology Policy declaring 2023 to be the year of open science. So first off, I wanted to get your reaction to that announcement.\nHeidi Seibold\nYeah, I think in general, that’s a really cool thing for open science to happen. Right? We, there’s this movement that’s been going on for a while, and people have been doing these grassroots communities and growing open science from the bottom up. And now we see more and more also top down decisions, which I think is a very good sign for, yeah, quality of science, really.\nBrian Tarran\nYeah, definitely. So I mean, we’ll get into some of the details of the initiatives a little later, maybe, but I thought that maybe we could kick off by asking you what you thought of the official definition of open science that the US government has come up with, and, and for the benefit of people watching that’s, in quotes, the principle and practice of making research products and processes available to all while respecting diverse cultures, maintaining security and privacy and fostering collaborations, reproducibility and equity. So as definitions go, does that, you know, does that hit the mark, for you, do you think?\nHeidi Seibold\nYeah, I think given that this is a federal definition, right, from the US. And they have to like, yeah, take so many opinions into account, I think it’s a great definition. I also asked like on social media, what people thought about it, and I think the response generally was pretty good. What I liked especially is that they focus on collaboration, reproducibility, and equity, which aligns very much with how I personally see open science. So collaboration means like, I always think of the term like building on the shoulders of giants, right. So this is what we want to do in research, we want to build on the work of others. They might be like famous people, but they might also be our colleagues next door. And it’s so important to take that into focus. And also reproducibility, just like this minimum standard in research quality, where we say, when we have the same data and the same analysis, we also want to see the same results, and being able to check that from others is really, really important. And so I think, this focus, I personally like it a lot.\nBrian Tarran\nExcellent. Excellent. So can you tell me a little bit about your background and how you became interested, and I think committed to open science would be a great way of describing it, right?\nHeidi Seibold\nYeah, so I’m a trained statistician, I studied statistics. And then how did I get into this whole open science thing was, for me, really through reproducible research. So my first research project was during my master’s programme, and we wrote a paper and it was a very computationally complex project, and we had lots of files and folders, and, you know, scripts and stuff like this. And then, at some point, it just all got so messy. And I felt like, oh, no, I’m the worst scientist in the world. And then I told my colleague who I was working with on this project. And he was like, No, this is normal. And I was like, No, this can’t be normal. I want to be on top of things when I do research. And I want to be sure that, like, the code that I’m using for this is the correct code that I actually wanted to use, right. So that must be like a minimum standard that we have. And so through that, I learned about reproducible research and good coding practices. And then I also thought more and more about like, well, if I do all this, I should also publish it so that others can actually check that I’m good doing good work here. Right. And so through that I got more and more into open science and really always felt like that’s the right thing to do, especially if you’re funded through public money.\nBrian Tarran\nYeah, yeah. Can I ask you what you think is kind of driving the shift to open science because the way you’ve described open science to me just there, you know, that sounds like to me like, just good scientific practice, and the fact that it’s not something that’s been done before, I wonder whether is it a cultural change, a philosophical change, a technological development, that’s kind of spurring this shift?\nHeidi Seibold\nYeah. So that’s a really, really good and deep question. So on the back of my wall here in my office, I have written open science is just good science in a digital era. And I think that describes the answer to your question pretty well. So there was a technological change, right? Before, we only had the printing press. And we had to print journals in order to distribute the knowledge that we have. And of course, that costs money. So journals cost money historically. But now, journal costs are really super low, because nobody needs them printed anyhow. And you only ever want to read like one or two articles out of one issue. So it doesn’t really make sense anymore, right. And also publishing data and code. It’s just like, the cost is so so low, that now in this digital age of the internet, really, we have such a low burden of, of doing the right thing. But on the other hand, we need to make this social shift, because we’ve been always, like, researchers have always done it a certain way. And there are especially certain fields that are really into, like, they feel like this is my data. This is my code. This is my research. Why should I share it? But I feel like the young generation of researchers are like, well, because it’s the right thing to do. And the reason why we’re in research is because we want to have scientific progress and scientific progress comes when we can build upon each other’s work.\nBrian Tarran\nOf course, and you mentioned, obviously, journal publications, but I wonder to what extent is it that the scale of science has almost kind of outgrown the ability to kind of condense it all down into a, even if it’s a 20 or 30 page, academic paper, right? Because it’s not just about, you know, setting out the research question describing the methods, you know, presenting a table of the data, all that stuff can’t be published now, or it can be, but it can’t be fit within the framework of an academic paper, it has to be on a GitHub repository or in a Jupyter notebook or something like that. So kind of open science encourages us to, to think about distributing that knowledge in different ways.\nHeidi Seibold\nI think that’s completely true. So, research now is so much more complex than it used to be right. There used to be single researchers who did like, yeah, such breakthroughs within one paper. And now, we already have so much knowledge, and the questions are so much more detailed and complicated. And also the data is so much more and the things that we can do is so much more complex. And so I feel like one paper doesn’t– isn’t enough to describe the research that we’re doing. And we did this one project with, with a group of students, actually, where we took 11 papers that were related to the topic that we were teaching about. And we took these 11 papers and data was available for the papers. And so we thought, well, we try to reproduce the results that they have there. And we found that it is really hard if we don’t have the code, because the method section in papers is really super short, right? It’s a super short section, if it’s not like a statistics paper. And it’s impossible to describe all the intricate steps that you took from the data to the complex model and analysis that you did. It’s just not enough space within the paper. And the code is the perfect description of what you did, right? So why not publish it with it, and especially in cases where data privacy is not an issue, then, and you already publish the data, then it just makes so much sense to also publish the code, because the code is not like there’s no privacy issues to that – usually at least.\nBrian Tarran\nSo is there a kind of you know– when we say open data science, you know, what does that look like? And do you have a kind of example or simplified example of what that would look like, right? And how it’s more than just, you know, the finished paper, the product of the science scientific process?\nHeidi Seibold\nYeah, so for me, open data science entails the process of– the part of the scientific process that focuses on everything that happens on the computer. So the data processing and the data analysis, and then getting from the data analysis, getting the results and the knowledge really, in sort of, yeah, in sort of a pipeline where you go from one step to the next. And so what the image that I have, in my head, when I think about open data science, I really think of a of a pipeline where you stick the different parts of the pipeline together, in a consecutive order. But of course, research projects are really complicated. And the pipeline just looks really messy, in some ways, but if you still manage to organise it in a way that the different bits all fit together, then you can make it so that in the end, you can imagine something that goes from the start to the finish. And you really understand every step of this pipeline every step of the way, from the raw data to the paper.\nBrian Tarran\nYeah. So the idea would be that people provide almost like a framework for you to recreate that yourself if you want to either check the results or yeah, replicate, just replicate the process generally. Right? So is your– so your job now is, I guess, as an open science trainer and consultant, so are you the person that goes into organisations and kind of shows them how to build that pipeline? Or how to, you know, map it out and to present that to people?\nHeidi Seibold\nYeah, so what I do a lot is do workshops and trainings with PhD students. So graduate programmes will ask me to come in for a day, or, also, we do longer trainings, which are often more useful, because people can in between take the steps that I recommend. And then we just, like, work together on ideas and steps that they can actually take. And I think what’s always important there is to know that nobody’s perfect from the beginning. And open science and reproducible research is really hard. And it requires a lot of technical knowledge. And I always feel like people are so scared, because on one hand, they don’t know how to do it yet. And the goal is so far away. And so I always like, you don’t have to be perfect right away, going one step into the right direction is super important. And that also helps with like the social change, because then the question is, well, I do want to do this, but my supervisor doesn’t know the technology, what do I do? And then we always try to find like one step that they can take, rather than trying to be perfect right away.\nBrian Tarran\nAnd it’s, I guess, it’s encouraging to see people like you being brought in to work with people on graduate programmes. So you’re, we’re trying to almost train the next generation of scientists to be thinking, open science first, rather than, you know, falling into bad habits or old habits that, you know, we are trying to do away with.\nHeidi Seibold\nYeah, and really, the young researchers, my feeling so far has been that the only pushback I get for my work is from established researchers who feel like well, this is the way I did it, and so it has to be the right way. But the younger generation for them, it’s super obvious that this is the way we should go in order to achieve scientific progress and good scientific practice.\nBrian Tarran\nYeah, yeah. A lot of data science now is obviously now done within industry. I wondered how open data science or open science principles fit in an environment you know, where competitive advantages is linked to keeping things in house and confidential and not wanting to share too much. Do you see a conflict there or can the two work together?\nHeidi Seibold\nSo I think there’s– first of all, I think, if we, if we get it done in academia, I’d be already super happy, right? If we get it done in the space where we have public funded projects that then are available for the public, I’d be already like super stoked. But in industry, it’s really interesting because a lot of the work that is done in industry is already done pretty well. So we if we think of, for example, companies, for example, like Microsoft, right, they put a lot of money right now into open source, they bought GitHub, they publish open source software, they put money into open source software projects like R, for example, right. So somehow, this must be a good way of making profits as well. And we see lots of companies investing in open source. So why not think about other research products, like data, and so on, also in the same line as we do of open source, because software is just a similar product. And I don’t think that there’s, I mean, there is some software, or some products, where it makes sense to have a patent or a trade secret or something. But sometimes it’s just more profitable, to have something where people can look into it and trust it. And I’d also helps like finding the next coders, and the next researchers to work on these projects, because, well, we like looking into what we’re going to do next. Right. And also, if we look, for example, into pharma industry, there, we see that a lot of the work they’re doing is already pretty good. For example, if we look at clinical trial transparency, pharma is doing better than academia there. And also, they’re pretty good on reproducible research, because they have to stick to certain rules. And when we look on the other side, so industry also benefits from open science, right? Because if we do open science, in academia, and publicly funded projects, then this will help companies make more money, because they have access to more knowledge and maybe interesting ideas as well, that they don’t have right now. So I think this is a win-win situation for companies as well. And I feel like if academia gives more to the industry, then eventually there will be like a mindset change. And industry will also give more back as well.\nBrian Tarran\nYeah, I see– I can see that and I guess it’s, you know, knowledge transfer is a big objective of a lot of academic institutions, right, we want our outputs to be of use to wider society, and that includes business and industry, right. So if people can pick those things up, you know, download it off of the web without having to, you know, make one-to-one links with the researchers who’ve done that project, it just it smooths that transition, and that that knowledge transfer becomes a lot more straightforward. You did, you mentioned about, you know, open science is about public science, essentially putting these, when it’s publicly funded research, this data and this work goes into the public space. You know, I’m guessing that, to a large extent, a lot of, you know, regular members of the public aren’t going to be interacting with open science. They’re not going to be downloading the datasets, they’re not going to be rebuilding the pipelines and rerunning the analysis. But do you still see that there’s a value there for the public in that information being there should they need it? And where are the kind of areas of value that you think the public will exploit?\nHeidi Seibold\nYeah, I think that’s an interesting question. And I think the biggest answer to that is that we don’t, we don’t know what ideas people will have, right? There’s so many skilled people out there, that probably will do amazing things. And we see that over and over again, when we put stuff out there, that people just get the super cool ideas. So we have this with the software Stable Diffusion right now, right? So that’s an AI that generates images from text, and it runs on my computer here. And I don’t need AI skills to be able to do that. And people are building such incredible images out of this. And it’s really fun to see. So I think, yeah, we just don’t know what’s going to happen. And on the other hand, I think, well, I am a researcher, but I’m also the public, right? And so if I have a question about something that concerns my private life or my friends’ private life, then I can also– I do have the skills to go into this and look at some research for example, I don’t know, how to best raise children or whatever. Um, that’s– so I have a friend, she’s an epidemiologist, and she always goes and looks at research on like, how to feed her child best and what to do. There’s, there’s all kinds of questions you have as a mother. And she, as a researcher can just go into the research and figure out, like, what is the best path for me to take. And I think the more we do open science, the more we can also do, like science communication, that adds on to that as well. Right? So if we have her now, she could now help other mothers make the same good decisions as well. And she would be sort of a science communicator for research that isn’t even hers. Um, so that is pretty cool, I think.\nBrian Tarran\nI think so, and it’s speaks to me of breaking down silos that, and I guess, blurring the lines between our roles, our professional roles and our personal lives, right. It’s about bringing science into that kind of public sphere, so I can see why the benefits would accrue from doing so. Just to wrap up, let’s go back to that year of open science, that was announced by the White House, are there any other specific initiatives in that big long list that constituted the announcement that you’re excited about?\nHeidi Seibold\nYeah, there is. I think, especially what I really liked was that, yeah, federally funded research, um, needs to be accessible, including the data when possible. So again, there’s open when possible, closed when necessary. But I think this is a very good first step, to say, okay, it depends on the funding, if it’s publicly funded, then it should also be publicly available. And I think that’s a really good sign. And then also, these, the statement had like, mentioned all these open science initiatives in different fields, which I really liked. So for example, from NASA, they have this transform to open science programme, and they’re already super active. And it’s really cool to see what comes out of that. For medicine, they have requirements for data management plans, which I think is a very solid step towards open science in medicine, because they are we have the issue of data privacy. And we really have to think from the get-go of a project about what should we do in terms of best practices of data management and having a requirement on that is, I think, a really, really solid step. And also, I’m thinking about open science in the field of like federal government even, because open data and federal government is a huge topic, right. And it, that’s definitely something that a lot of people will be interested in as well.\nBrian Tarran\nLast question for you, then Heidi, you know, what would you want the lasting impact of an initiative like this to be? And you know, would you like to see this sort of thing replicated in other countries around the world? If indeed, you know, other countries may already be doing this and have already done this?\nHeidi Seibold\nI think in general, it’s a very, very good sign that we’re seeing right now. We’ve seen lots of movement, for example, in the Netherlands – Netherlands is really big on open science – and seeing such a big country [the US] that also plays such an important role in the world, doing– taking this step, is a great sign for the entire, like all of research, really. And yeah, I think it’s also nice to see that we’re going from like, Oh, this is a niche topic that only experts are interested in, and people that are like advocates and nerds focus on, to something that really governments are thinking about.\nBrian Tarran\nSo, open science is going mainstream, I think is the message. And let’s hope that it continues to do so. So Heidi, thank you very much for your time today. Where can people find you online, if f they want to find out more about you and your work and your thoughts on open science?\nHeidi Seibold\nYeah, thank you so much for having me. It was wonderful. And I always like talking about open science, so people can find me on my website, heidiseibold.com. And I’m also on Mastodon, Twitter, LinkedIn, YouTube, wherever your search for Heidi Seibold, you’ll find me.\nBrian Tarran\nExcellent. Excellent. Well, thank you again. And thank you to those of you who are tuning in today. Make sure to check realworlddatascience.net for more interviews. Take care.\n\nFind more Interviews\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “Why open science is ‘just good science in a digital era’.” Real World Data Science, February 3, 2023. URL"
  },
  {
    "objectID": "viewpoints/interviews/posts/01/25/erica-thompson.html",
    "href": "viewpoints/interviews/posts/01/25/erica-thompson.html",
    "title": "How to ‘Escape from Model Land’: an interview with Erica Thompson",
    "section": "",
    "text": "Erica Thompson’s new book, Escape from Model Land, offers a fascinating and important perspective on mathematical models as being not just models of the real world, or real processes or systems, but also “subjective versions of reality” that encode all sorts of assumptions and value judgements.\nIn this interview with Brian Tarran, editor of Real World Data Science, Thompson talks about the “social element of modelling” and how it manifests, how to counter the subjectivity of individual models with a diversity of models, and whether human-made models are held to the same standards of transparency that are expected of AI-“created” models.\nErica Thompson is a senior policy fellow in the ethics of modelling and simulation at the London School of Economics Data Science Institute."
  },
  {
    "objectID": "viewpoints/interviews/posts/01/25/erica-thompson.html#timestamps",
    "href": "viewpoints/interviews/posts/01/25/erica-thompson.html#timestamps",
    "title": "How to ‘Escape from Model Land’: an interview with Erica Thompson",
    "section": "Timestamps",
    "text": "Timestamps\n\nWhat led Erica to write the book, and why now? (2:27)\nCritiquing climate models (7:30)\nExploring the “social element” of modelling (11:36)\nCountering subjectivity with a diversity of perspectives (20:11)\nAI models, human-made models, and questions of transparency (25:54)\nWhy write a popular science book about these issues? (30:11)\nWill the UK Prime Minister’s “maths to 18” proposal help or hinder our Escape from Model Land? (34:01)"
  },
  {
    "objectID": "viewpoints/interviews/posts/01/25/erica-thompson.html#quotes",
    "href": "viewpoints/interviews/posts/01/25/erica-thompson.html#quotes",
    "title": "How to ‘Escape from Model Land’: an interview with Erica Thompson",
    "section": "Quotes",
    "text": "Quotes\n“Putting things in a mathematical language does tend to make people think that it is truth from on high. And so my book, in some way, goes towards saying actually, these models, obviously we hope that they’re based on facts and they’re based on data that we gather, but they also do have this value judgement content as well” (1:51)\n“It is arbitrary how we choose to model a situation. There are infinitely many different ways that you could choose to simplify reality - this huge, messy, complex thing in front of us, with physical laws that we don’t fully understand and things going on that we can only measure by proxy.” (12:09)\n“The choice of assumption has a very direct result in the model output and in the information and advice that you’re giving to policymakers… [In climate models] maybe we have a cost of however many dollars per tonne of carbon dioxide for nuclear electricity or for renewables. But what kind of price would you put on behaviour change? How many dollars per tonne of CO2 avoided does it cost to change the behaviour of a population such that they use less energy? If you put it in at $2 per tonne of CO2, it would be heavily relied on [as a policy response]; if you put it in at $2,000 per tonne of CO2, it’ll never happen.” (16:42)\n“There needs to be more frank discussion of values and value judgments, and politics and social assumptions within models. And I think we are starting to see that with the pandemic models, particularly because it’s been so high profile. [But] it’s really hard to unpick your own value judgments. It’s easier for somebody with a different perspective to come in and say, ‘Oh, actually, you know, you’ve assumed that. Why did you assume that?’ When we are embedded in a particular culture of modelling, it’s particularly hard to imagine that anything could possibly be done differently.” (27:20)\n“I think some people maybe read the book and think, ‘Oh, this is just a sort of woke advertisement for diversity’. Well, it’s not; it’s a way of doing the maths better. The whole point is to do the maths better, make better forecasts, understand the future more effectively, and be able to make better decisions based on that information.” (33:41)"
  },
  {
    "objectID": "viewpoints/interviews/posts/01/25/erica-thompson.html#transcript",
    "href": "viewpoints/interviews/posts/01/25/erica-thompson.html#transcript",
    "title": "How to ‘Escape from Model Land’: an interview with Erica Thompson",
    "section": "Transcript",
    "text": "Transcript\n\n\n\n\n\n\nThis transcript has been produced using speech-to-text transcription software. It has been only lightly edited to correct mistranscriptions and remove repetitions.\n\n\n\nBrian Tarran\nHello, and welcome to the very first instalment of the Real World Data Science interview series. I’m Brian Tarran, the editor of Real World Data Science, and I’m very pleased to be joined today by Erica Thompson, a senior policy fellow in ethics of modelling and simulation at the London School of Economics Data Science Institute, and the author of a fantastic new book - which I have a copy of here - Escape from Model Land, which is subtitled, How mathematical models can lead us astray and what we can do about it. So hello, Erica, thank you for joining us. I hope 2023 got off to a positive start for you.\nErica Thompson\nYes, it has so far.\nBrian Tarran\nGood, good. Because the book came out, is it just before Christmas or just after?\nErica Thompson\nYeah, just before Christmas. So I’ve had all sorts of things flooding in saying, Oh, I liked your book, or I hated this bit or no, it’s exciting.\nBrian Tarran\nYeah, no, well, it’s, I have to say, I think, I thought it was a genuine– I finished reading it over, over Christmas. And I think it offers a genuinely fascinating and important perspective on mathematical models as being not just I guess, models of the real world, or, you know, real processes or systems, but subjective versions of reality, you know, encoding all sorts of assumptions and value judgments of the people who, who create the models. And I mean, I guess that shouldn’t really come as a surprise, right? But, but is it a point that is often lost in the discussion around models, particularly where decisions might be, like, informed or driven by model outputs?\nErica Thompson\nI think it is something that’s easy to miss. I mean, especially because we’re sort of, maybe as mathematicians were used to living in model land and doing things which, which we see as being logical consequences of previous things. And then more generally, the public look to science and mathematics and statistics as being objective arbiters, perhaps, of how things are and how things ought to be. And so, so yes, that that kind of putting things in a mathematical language does tend to make people think that it is truth from on high. And so my book, in some way goes towards saying actually, these models, they are, obviously we hope that they’re based on facts, and they’re based on data that we gather, but they also do have this value, judgement content, as well. And so we need to think about what that is and how we deal with it, and how we sort of express it and how we understand it.\nBrian Tarran\nRight, yeah.\nErica Thompson\nEspecially where we’re using those models to inform decision making or public policy, then it becomes particularly important.\nBrian Tarran\nYeah, yeah, no, and obviously, your book draws up quite a bit on the Covid-19 pandemic, and how models were used there. But I thought it was interesting, actually, that, you know, in reading the acknowledgments that you– that while the pandemic lent the topic, additional relevance, right, you actually started writing the book before that. So what led you to think now’s the time? What was the tipping point, if you like, of thinking, I want to write this book now?\nErica Thompson\nYeah, okay. Well, that’s an interesting question. I mean, because it builds on the last sort of 10 or 15 years of my work. So I started out doing a PhD in climate physics. And my background before that was maths and physics. And so I was doing a PhD on the physics of North Atlantic storms, looking at how they would change given climate change. And so, obviously, the first thing you do is a literature review. And I started looking at different models and what the what they were saying about what would happen to North Atlantic storms. And what I found there was that there were models saying that the storm tracks would go north, they’d go south, they get stronger, they get weaker, they’d, you know, anything you name it. And interesting, particularly, interestingly, was that they, they had relatively small uncertainty ranges. So they they didn’t agree within their own uncertainty ranges. And that made me think, well, we this isn’t telling me very much about North Atlantic storms. But it’s telling me a great deal about modelling and the way that we do modelling and perhaps we need to start thinking more about how these uncertainty ranges are calculated, what does it mean? What, how can we end up in a situation where we have this level of disagreement between models. And so since then, I’ve been looking at, you know, those kinds of concepts in different areas I’ve been looking at sort of insurance and finance and weather and climate and humanitarian forecasting as well. And, and so in all of those application areas, I found the same questions about uncertainty and how we make inferences from model output to be particularly interesting and how common problems may be solved in different ways as well. So it’s interesting to do the compare and contrast. And so, yeah, then I guess I, I’d been on all these sorts of bitty little projects and thought actually, I’d really like to bring this together into something more coherent, you know, to actually say, look, there’s a, there is a common theme here and we need to be putting it together and drawing conclusions. And we can, we can learn a lot from doing that. And we can share the best practice throughout the sector.\nBrian Tarran\nWhen you’re starting down this path of, I guess, looking into the, I guess, the ethics and process of modelling, did it, was there a lot of other work that you identify that you could kind of draw on a lot of other thinking around this area? Or was it kind of under studied, under researched sort of aspect of the literature?\nErica Thompson\nI think it’s under studied, I mean, of course, everybody who does some modelling, you know, you, you do your modelling, and then somebody says, Okay, we need to put some error bars on the outputs, and you go back and, and think about how we’re going to put the error bars on the outputs. And probably, I would say, most people doing that realise that it’s much more difficult than they have the time to do or the ability within the scope of whatever project they’re doing. But the aerobars, the uncertainties always ended up being tacked on at the end, you do it after you’ve done all the modelling, there’s less incentive to do it. And there’s less resource to do it than to make the model itself better. And I think that’s a very common story, that people realise that they ought to be doing more, but they just don’t have the time the resource, the ability to go and do that. So then yes, there are there are people, and there are particular areas that I think have taken more time to investigate this. So in physical science hydrology, I’d say in particular, has a very well developed history of thinking about the uncertainties in models, maybe because, you know, they are constantly being challenged by events happening, which were not within the models, you know, you’ve got your flood forecast model, and then something happens, and it goes way beyond what you were expecting. And you you have to go back and say, What does this mean for our modelling process. And other areas have much less well developed considerations of uncertainty. And so that’s where I think actually, we could we could really benefit by sharing good practice across these different application areas, because people have looked in different ways. And, you know, with with different levels of statistical interest, you know, some areas go into the stats, much more, some areas are very philosophical about sort of the conceptual foundations of how we should think about models and how we should think about the range of outputs that we get from models. And so what I’m trying to do is bring those together a bit. Yeah.\nBrian Tarran\nYeah, I think you certainly achieve that. It is really interesting, the different, the variety of examples that you present, and the ways you talk around these issues. I did want to focus in particular on climate models, though, because, you know, I was looking around your website, finding a bit more about about you, and I noticed on there that you talk about, you no longer fly to conferences, and that’s in order to kind of reduce your own ecological footprint. So I guess I wanted to ask, you know, when you set about writing a book, and it’s going to be a book that’s kind of critiquing models and the ways that they don’t often agree? Did you have like a nagging concern that, you know, the points you wanted to make about models in general, but climate models in particular, that that would kind of lend fodder to the kind of groups that might want to discredit climate models or downplay the risks? Or, indeed, the reality of climate change?\nErica Thompson\nI mean, yes, I did have that worry, I still have that worry. And I, but I hope that my book is clear throughout that, you know, that models are not irrelevant, you know, the answer is not to throw them away. If you come to it from this sort of sceptical position, saying, you know, we need to think more carefully about how we make inferences from models, you could go all the way down the rabbit hole and say, Oh, they’re all terrible, let’s just throw them away. But I think that would be completely unjustified. We have good evidence, sort of from from one end of the spectrum of relatively simple linear models, which are incredibly, wildly successful and form the foundation of modern life and modern technology. And, you know, with that, as a basis, we hope that we can, you know, work from there to find the limit of the knowledge that we can get from these more complex models, which are looking at making predictive statements in more extrapolatory domains where the underlying conditions are changing, and we therefore have less ability to rely on what I call the quantitative route of escape from model land, by challenging with relevant past data. You know, we’re looking at extrapolatory conditions like climate change, or social and economic systems, and therefore, we think that the data that we have, while they may be useful and indicative, are not, we can’t just calibrate with respect to past data and expect that to be enough to warrant performance in the future. And so, so I think that sort of one answer is that we shouldn’t be throwing away the models completely because they are demonstrable useful, and the question is to quantify the limits of what we can say, rather than just get rid of them. And then maybe the slightly more nuanced answer is that actually, if we have less confidence in the models, and our uncertainty ranges are wider, then because in many of these application areas and climate change, in particular, the damage function is convex, you know, we are expecting that as we go further from today’s climate, the consequence will be not just linearly worse, but sort of increasingly worse. And therefore, if you have, if you’re considering, you know, just to have a sketch of a kind of cost benefit analysis on some sort of expected utility from taking action to mitigate carbon emissions, for example, if you have more uncertainty, then your range is wider. And so the, the lack of quantification of the top tail becomes dominant in in the expected utility of the outcome. And therefore, you should be choosing to mitigate more, not less, because of that uncertainty. So, you know, the sceptics, I suppose the climate sceptics would say, oh, there’s a possibility that climate change might not be as bad as we expect, and therefore we shouldn’t bother doing anything. But I would say actually, that argument should be turned on its head, if we have greater uncertainty, that should be a bigger motivating factor to reduce carbon emissions rather than the opposite.\nBrian Tarran\nYeah, and I think you make that point quite clearly in the book. And the other point you make is that, I guess, building trust in models is about understanding their limitations. And the quote that I thought was really interesting was about acknowledging the social element of modelling. And I wonder, do you mind explaining what that social element is for people who are watching or listening? And how will that kind of manifests itself in models? Maybe you’ve got like a simple example that you might want to talk to? I don’t know.\nErica Thompson\nYeah, okay. So, I mean, the social element of models is, because it is arbitrary how we choose to model a situation, there are infinitely many different ways that you could choose to simplify reality, this, you know, huge, messy, complex thing in front of us with physical laws that we don’t fully understand and things going on that we can only measure, sort of by proxy, with, you know, with models themselves to make many of our measurements of the system. And so, so you might choose to simplify a system in one way to model it. And I might choose to simplify it in a different way. And you might choose one programming language, and I might choose another and they would implement functions in different ways. And so all of our choices change the way that the model will then look at the end of the day. Now, then you say, okay, but supposing I’m modelling you know, what will happen to a ball when I throw it up in the air? Surely, that’s not a, you know, that has no social content, does it? And I’d say basically, no, it doesn’t really have any social content. It has some social content insofar as you’re deciding that this is what we want to make a model of. But ultimately, you and I would probably come up with very similar models, regardless of our background or our perspective, or our interests, or even our education to a large extent. And so, so those relatively simple linear situations, which I refer to as interpolatory models don’t have very much social content. Now, the ones that I’m particularly interested in and that I talk about in the book are things that are extrapolatory, where we’re interested in situations where we are trying to predict into the future a system where we expect the underlying conditions to be non-stationary, to be changing. So climate change is one example. Social and economic systems would be another example. And when we’re modelling systems like that, we have to be much more careful because we could choose to model them in radically different ways. We could, if you want to model an economic system, you might choose to disaggregate with respect to the social class of different households. And I might choose to make a sort of bulk model of the whole system with a representative household. And you could imagine hundreds of different ways to do these sorts of things. So maybe you think about pandemic models and how you could simplify it into individuals or you could make an agent-based model with, you know, actual agents walking around and infecting each other. Or you could just write some differential equations for how the transfer happens. So you could do it in, again, in many different ways. And the choice of simplification is then much more important, and it will have much larger first order effects on the outputs, and then on the framing of the question, you know. So you decide to model it in a certain way, with a certain kind of mathematics, and that changes the way that you might think about intervening in the system. If you’re presenting your model to a policymaker with the intent of informing them about their policy options, you might, if you have a model which can represent the effects of say, closing schools, or universities on pandemic transmission, then then that becomes a policy option. If you have a model which can’t represent those kinds of interventions, then it’s not a policy option. And similarly, with climate change, one of the examples that I talk about in the book is integrated assessment models of energy and climate. And so these are models which consider the energy system out to say 2100, and they put a price on nuclear electricity and renewables and all the other things that go into the energy system and basically say, how can we achieve our carbon targets at the lowest cost? Now, if you put in, if you choose to put in a certain price for a certain technology or assumptions about how that technology will develop in future, then you get a particular answer. And you put emphasis on certain kinds of policy options. And so that has, the choice of assumption has a very direct result in the model output and in the information and advice that you’re giving to policymakers. And of course, you might choose to put in something like behaviour change. So maybe we have a cost of however many dollars per tonne of carbon dioxide, for nuclear electricity or for renewables. But how much, what kind of price would you put on behaviour change? How many dollars per tonne of CO2 avoided does it cost to change the behaviour of a population such that they use less energy? Well, that’s not really in the models. And if it was, it would, again, it would be first order because that would be you know, if you put it in at $2 per tonne of CO2, it would be heavily relied on, if you put it in at $2,000 per tonne of CO2, it’ll never happen. And where you choose to put that in between influences how it looks, and then that influences the pathway that’s projected, and it influences the advice that you give to policymakers.\nBrian Tarran\nYeah. You mentioned the example of school closures and stuff when you’re talking about Covid-19. I actually thought that, that one really helped me understand, I guess, and made it clear to me was that, there’s often been that argument about whether the lockdown was the right thing to do given the other impacts, but you actually say that if different types of people were doing the modelling, maybe if it was school aged people, and I guess encoding the impact that that would have had on them, and how much they value say not being able to go out and see their friends, the impacts potentially on mental health and things like that, it does change, I guess, the calculation of what the right intervention is or the right response is.\nErica Thompson\nYeah, exactly. And I think I think we haven’t anywhere near bottomed out all of these impacts of the pandemic, you know, both the health impacts, and also, mental health and economic impacts will be rippling on for a very long time to come. So we, you know, we can’t even now retrospectively look back and say what was the right decision? It’s really not clear, depends how you value the different outputs, the different outcomes of a decision. And yes, we didn’t have economic models of what the impact of lockdown would be. And if we, if those had been available and developed the same kind of mathematical complexity and credibility as the models of infection and transmission we had in those early stages of the pandemic, which had essentially morbidity, mortality, and the, you know, the impact on the NHS, you know, number of hospital beds occupied. Those were the bottom lines, and there was nothing else. And so that was given as an input to the policymakers. Now, that’s, of course not everything that the policymakers rely on, they have to, their role is to weigh up everything else as well. But if we’d had models which contained more information about all of those other impacts, I think it’s quite plausible that we would have seen different kinds of decision making. And then there’s also the communication aspect, that these models were used to communicate and justify and persuade the public of the importance of the actions that were taken. And, you know, I think it’d be hard to disagree that the actions that were taken immediately where necessary, we certainly did need some kind of lockdown straight away. But then the question of exactly what you do thereafter is much more difficult.\nBrian Tarran\nYeah, yeah. So in the book, you kind of make the point that it’s somewhat of a fool’s errand to try and make models objective, and they can’t ever really achieve this, you know, principle of scientific objectivity. But that we instead should look to counter subjectivity of individual models with a diversity of models that do encode these different perspectives, like we’ve just been talking about. I wonder, you know, how would you see this working in practice? Or how would you like to see this working in practice?\nErica Thompson\nI mean, that’s a difficult question. So it’s, it’s nice to think that, you know, we have these models, and they are unavoidably subjective. Essentially, my view is that the model encapsulates the expert opinion of a particular expert, and it comes laden with their own perspectives, and biases and preconceptions, as well as their expertise and their education and their experience of a subject, you know, which shouldn’t be set aside. So if we are trying to understand a situation, then we want to get as many perspectives as possible. And so in theory, incorporating the widest possible diversity of different backgrounds into modelling and making a multiplicity of different models and trying to see the problem from these different perspectives will help us to understand it better. Now, then the statistician will jump in and say, Aha, can we, you know, can we in some way average those models or use some sort of statistical inference to take those models and put them together and come up with an even better answer? And I would, I would sort of counter that by saying that there’s no reason to believe that our, that a set of models generated as essentially just one set of opinions will be an independent and identically distributed sample from some distribution, underneath which will be an estimator of, of the truth, if that even exists. And so many of the formal statistical methods that we would quite like to apply to an ensemble of models, a large group of models put together aren’t really conceptually valid at all. I mean, that doesn’t stop people doing it. And maybe you get some interesting information from it, but you certainly can’t rely on it as an estimator. So there’s a sort of statistical problem there. And then the other question is your reference class. So, to what extent do you believe that these models are all equally valid or equally plausible? So that then brings the social question back to the forefront. Because then you say, you know, if I believe that, you know, somebody from Imperial College, say, who is the head of an institute for epidemiology, and has many, many years of experience making this kind of model, you know, is an expert and is qualified to create a model and for that to be recognised as a valid expert opinion, who else has got the credibility to do that? How do we define that? You know, what do you call a plausible model? So then it’s a question of the sort of scientific gatekeeping. What kind of qualifications do you expect from somebody or from an institution? What kind of expertise counts as being relevant and valid expertise? Does it have to be mathematical expertise? Can it be lived experience? Can it be, does the model have to be a mathematical model? What kinds of mathematics are appropriate for the situation? If we disagree about assumptions, does that mean that we can’t consider the the two sets of models in the same sort of class of plausible models? Or are we going to start pruning it by saying, I believe your assumptions, and I don’t believe your assumptions? And if so, who gets to do that? Who gets to decide what is plausible and what is not plausible? And what is allowed to enter into this set? Because as soon as we start pruning it, then we make the statistical inference more difficult. You know, if you want to say, if you want to start applying your methods that assume that the models are on, you know, that the models are independent, then you can’t start pruning because then that introduces huge dependencies on your own expert judgement. So it just becomes extremely difficult. And this is where all of then the social questions about expertise and credibility and sort of scientific gatekeeping and how we assign that credibility and trust, trust in science, you know, who has trust in which kinds of models? This is something, this is a theme that we see coming out of climate science and, you know, hopefully less now than maybe 10 years ago. But in the pandemic, of course, we’ve seen it coming right up again with questions about lockdowns, and about vaccination strategies, and all of that sort of thing. Trust in science is really important. And maybe one of my themes is that trust in science actually is first order in the modelling process itself. It’s not something that is sort of added on afterwards, I’m going to go away and make my model and then the question is whether or not you trust it. Actually, trust and expertise and credibility are in the modelling process directly.\nBrian Tarran\nDo you mind if we segue to talking about artificial intelligence models, or models made by artificial intelligence? Because that’s, I think that touches on a lot of the same issues, right? And I wanted to think about, well, first of all, you say that, obviously, artificial intelligence models made by AI, they’re not objective, even though there’s like, they’re kind of building the models, if you like, those AIs have still been trained by people, been coded by individuals and those personal judgments and assumptions and all that get embedded into the artificial intelligence. But I think, I guess my question for you is, we’re starting, I think, to have a very frank and public debate about AI ethics, and to demand transparency and explainability of things like automated decision making systems. But do you think we’re kind of, are we falling short of holding ourselves as people to the same standards of transparency and being clear about the choices and decisions we make? And also documenting that subjectivity when we’re preparing these sorts of models and these sorts of decision making systems for policymakers to use?\nErica Thompson\nYeah, so I mean, I suppose there are two questions there. And one’s about what we do and one’s about the AI. So for the humans, maybe, yes, I think that there needs to be more frank discussion of values and value judgments and politics and social assumptions within models. And I think we are starting to see that with the pandemic models, particularly because it’s been so high profile. I mean, remembering that actually, it’s really hard to unpick your own value judgments. And it’s easier for somebody with a different perspective to come in and say, Oh, actually, you know, you’ve assumed that, why did you assume that? And, you know, when we are embedded in a particular culture of modelling, it’s particularly hard to imagine that anything could possibly be done differently. And so I think that’s, again, where diversity is really important, because introducing those perspectives will help to challenge dominant strains of thinking which can end up in sort of accidentally, and not deliberately at all, in a form of groupthink. So that’s the humans and then the, with the AI, yes, you know, they inherit their value judgments from their creators. And so, for example, on the statistical side, one might think about the kinds of loss functions that are used to calibrate machine learning programmes, you know, how does the machine decide what is better and what is worse? You know, it is learning to model a situation, but there will be some kind of loss function in there, which it is minimising in order to decide what is the best model. And so, being explicit about that loss function, I think, actually is really interesting, you know, the fact that the model has got written down an explicit loss function which it is minimising means that we can then analyse that and think about what are the value judgments inherent in that choice of loss function, which is something that we don’t have when humans are calibrating a model and they’re twiddling a knob here and a knob there and saying, Oh, does it look realistic? Am I getting the right kind of behaviours, you know, does it match up with the map I have from observations or whatever. And so, having that I think we can then say, what are the implications of writing a loss function in this way? And what are the values that are implied? I mean, even just modelling itself, you know, like choosing to solve a problem with recourse to mathematical modelling is a value judgement and implies a certain kind of solution, doesn’t it? So if we say that we even can come to a decision, that the models input will be relevant and interesting and help us, that is a value judgement.\nBrian Tarran\nYeah, and perhaps we could return to that point a bit later, because there was a line again that jumped out in the book about decision makers needing to maybe curb some over enthusiasm for mathematical solutions. You do talk about documenting value judgments as being kind of one of five principles that you set out for mathematical modellers to adopt to support responsible modelling. I think it’s fascinating to me that you’ve used the vehicle of a popular science textbook to speak to this community, and to sort of set out these principles rather than, say, a journal paper or conference presentation. So I wanted to ask, is there a particular strategy to that decision? I mean, I have my own theory on that, but maybe I’ll let you go first.\nErica Thompson\nI’d love to hear your theory. I mean, I guess partly because I suppose I’m very interdisciplinary. And I’m, I’m hopping between these different areas, as we were discussing before, so I have sort of a climate science community and statistics and data science and other application areas in humanitarian forecasting, sort of hydrology, geophysics, weather and climate and all the rest of it. And actually, I find it really hard to get these thoughts published in journal form, because I suppose partly because it feels too general for any specific journal. And perhaps it feels too simplistic that, that the reviews I get tend to be either Oh, we’ve heard this all before it’s not new, or this is too radical and this isn’t an appropriate journal for it, you know, that sort of thing. And actually, I kind of got to the point of thinking, Well, you know, do people even read these journals anyway? Actually, maybe really what I need to be doing is trying to provoke a wider discussion about models. And I do tend to get a, you know, a really good response, when I speak at conferences or talk to people about these things. People go yes, yes, you know, this is really important. Actually, this is something I’ve really struggled with, we don’t know how to do it, the uncertainty is always just an add on at the end that doesn’t have enough time allocated for it. But I don’t have the resource to do it, I’m not able to grapple with these questions, because they’re so fundamental and so wide ranging, and it would be really helpful to have more of a sort of walkthrough of how people tackle these questions in different fields. And so, so I’ve been trying to do that. And I felt that the book was a good way to sort of spark the conversation and maybe also get it to some different audiences. So I’ve had people contacting me since the book came out saying, oh, you know, I’m working in, like, asset valuation for disputes between states, really random things, quite different. And they say, actually, your book really struck a chord, and we have difficulties with this in this particular area. And so I’m really hoping that, you know, the book will help me then to find, to bring together people working on these sorts of issues with common themes from really different application areas and try to make some headway on how we can actually go about practically changing modelling practice to make it to make it work better, and assess uncertainty better. So it’s not just– I think some people maybe read the book and think, Oh, this is just a sort of woke advertisement for diversity. Well, it’s not; it’s a way of doing the maths better. The whole point is to do the maths better, make better forecasts, understand the future more effectively, and be able to make better decisions based on that information.\nBrian Tarran\nYep, well, that’s not too far away from my theory on why you did it. I thought it was that it’s a great way of getting– if you can get policymakers and the public to read this, right, you can get them to hold modellers to these principles, rather than having it just be something that you kind of talk about within the community and it doesn’t really go outside that, right? It’s a way of people, you know, the next pandemic or whatever it might be, the next time a model is the focus of a debate, the public are equipped to ask the right sort of questions about the process. Okay, I’ve got one more question to you because we’re running out of time. And it’s back to that over enthusiasm for mathematical solutions point. I thought was somewhat serendipitous to read about, read that quote, in the same week that the UK Prime Minister Rishi Sunak announced a plan for all pupils in England to study maths to the age of 18. So, I wanted to ask you what you make of that plan, first of all, but also, I think, more importantly, does a more mathematically minded populace, are they better equipped to understand the mathematical descriptions of the world and that they are incomplete descriptions? Or is there kind of maybe some other curriculum that we need to tack on to this maths to 18, in order that people are able to better differentiate between model land and the real world?\nErica Thompson\nYeah, so I mean, I’m not a fan of– I mean, I like the idea of people studying maths to 18. I think more maths is a good thing. I loved maths, I still love maths, I think if more people were more generally numerate then society would be better and life would be better. But if you haven’t enthused people about the value and the interest of maths by 16, forcing them to study it for another two years is absolutely not the answer. It will just put people off staying in school past 16. So I, you know, I think that we need better teaching of maths before 16, rather than forcing people to study maths post 16. And part of that is helping people to understand how mathematics is relevant to the real world that they live in and teaching them the kind of things that they will use in their adult life. You know, people, most people don’t use Pythagoras theorem, but most people do need to fill in a tax return, you know, these sorts of things. Understanding orders of magnitude, and the difference between millions and billions, would be incredibly helpful, wouldn’t it? So, yeah, I think there are more basic questions there that need to be answered before we go into the details of sort of complex maths. And then, so what was the next question?\nBrian Tarran\nIt was about whether whether you think, the more mathematically equipped we are, does that make us better able to understand the limitations of models? Or do we need something else to kind of train us or encourage us to think about these two separate realities, model land and the real world? And I say realities in inverted commas.\nErica Thompson\nYeah, I mean, I think the general public has a good understanding that the model land and the real world are not the same. There is actually a healthy scepticism of models out there. And I think that’s probably a good thing. I give a couple of funny anecdotes in the book about that. So I mean, one was a, I think, a YouGov poll about people going to the moon and saying, you know, would you go to the moon if you could be guaranteed a safe return, blah, blah, blah, and like, a large percentage of those said, Well, no, I just don’t think you could give me a safe return, they reject the model land. And then there was another example about intelligence analysts being asked to sort of calibrate a probability language scale. So they say, like, likely, however many percent and very likely however many percent and unlikely however many percent. And so the study was looking at different ways of doing that. And one way was to accompany the word likely, or unlikely, or whatever, with a written number of what the probability it referred to was. So it would say, like, likely, I can’t remember the number, but sat it was, like 50 to 70%, written down in the question, and then the question was, what is the probability of an event, which is deemed to be likely brackets 50 to 70%? And what people write down was not 50 to 70%. You know, as a mathematician, that’s completely ridiculous. Because the answer was in the question, why wouldn’t you write that down? But of course, what you’re seeing there is the rejection of model land. Somebody has assessed it as 50-70%. The question is, do you believe it? Well, no, actually, you might write something like 40 to 80%, because you expect there to be, you know, the model to be generically overconfident. And so this is sort of what I mean, by curbing over enthusiasm for mathematical solutions is that, you know, we have to understand that the mathematical solutions are living in model land, and that we can, in order to get out of model land, we have to say, do we actually expect this result to refer to the real world? Or is it only saying what the next model run is going to tell us? And so the act of doing that is difficult, and it’s more difficult for mathematicians than for the general public because as mathematicians, we sort of are used to living within model land and noticing when the answer is in the question and then writing it down. And we’re not very good at saying, Well, what’s my subjective estimate of the probability of this model being inadequate in some way? That’s not something that you can necessarily do with respect to data and so it’s a tricky one. So in terms of the over enthusiasm, you know, it’s curbing over enthusiasm, not curbing enthusiasm, because as I said at the beginning, and I returned to a lot in the book, actually, mathematical models are incredibly valuable. And they contain a huge amount of information and insight that we’re, we would be fools to throw away. But we need to understand it, you know, in a more nuanced way and be clear about what it’s telling us and what it’s not telling us. And that answers in model land aren’t necessarily the answers that we need in reality, though they may be informative about them.\nErica Thompson\nWell, Erica, thank you very much for your time today, for talking through the book, which is out now. Do you have some some links or information about where people can find out more about the book?\nErica Thompson\nYep, look on my on my website, ericathompson.co.uk. And it’s available through all the usual booksellers.\nBrian Tarran\nExcellent, excellent. Well, I wish you the best of luck with the book. As I say, I think it’s fantastic. And well, I hope we get to talk again, maybe a bit further down the road and see whether some of these principles and this ethical framework that you talk about for mathematical modelling, whether that kind of comes to fruition because I think we need to watch that closely. So, Erica, thank you.\nErica Thompson\nThank you very much. Thanks for having me.\n\nFind more Interviews\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “How to ‘Escape from Model Land’: an interview with Erica Thompson.” Real World Data Science, January 25, 2023. URL"
  },
  {
    "objectID": "viewpoints/interviews/index.html",
    "href": "viewpoints/interviews/index.html",
    "title": "Interviews",
    "section": "",
    "text": "‘I’m way more into prevention than cure’: Stephanie Hare on why we need a culture of technology ethics\n\n\n\n\n\n\n\nTechnology ethics\n\n\nAI ethics\n\n\nCulture\n\n\nRegulation\n\n\n\n\nStephanie Hare, author of ‘Technology is Not Neutral’, talks to Real World Data Science about the ‘wicked problem’ of technology and AI ethics, and why laws and regulations are ‘necessary but insufficient’ to minimise harms. ‘We’re going to have to inculcate an entire new mindset,’ she argues.\n\n\n\n\n\n\nApr 28, 2023\n\n\nBrian Tarran\n\n\n\n\n\n\n  \n\n\n\n\nUS legislators get their data science act together\n\n\n\n\n\n\n\nData science education\n\n\nData literacy\n\n\nPolicy\n\n\n\n\nA bill introduced in the US Congress wants to make funds available to develop data science and data literacy education across the United States. We sit down with education and policy experts to discuss the challenges and opportunities ahead.\n\n\n\n\n\n\nMar 6, 2023\n\n\nBrian Tarran\n\n\n\n\n\n\n  \n\n\n\n\nWhy open science is ‘just good science in a digital era’\n\n\n\n\n\n\n\nOpen science\n\n\nReproducible research\n\n\n\n\nReal World Data Science speaks with statistician and data scientist Heidi Seibold about open science: what it means, the benefits of it, and how to move towards it.\n\n\n\n\n\n\nFeb 3, 2023\n\n\nBrian Tarran\n\n\n\n\n\n\n  \n\n\n\n\nHow to ‘Escape from Model Land’: an interview with Erica Thompson\n\n\n\n\n\n\n\nModelling\n\n\nEthics\n\n\n\n\nAuthor Erica Thompson talks to Real World Data Science about the ‘social element’ of mathematical modelling, how it manifests, and what to do about it.\n\n\n\n\n\n\nJan 25, 2023\n\n\nBrian Tarran\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "ideas/datasciencebites/posts/2022/analytical-problem-solving.html",
    "href": "ideas/datasciencebites/posts/2022/analytical-problem-solving.html",
    "title": "How machine learning relates to other algorithmic approaches to problem solving",
    "section": "",
    "text": "About the paper and this post\n\n\n\n\n\nTitle: Analytical Problem Solving Based on Causal, Correlational and Deductive Models\nAuthor(s) and year: Jeroen de Mast, Stefan H. Steiner, Wim P.M. Nuijten & Daniel Kapitan (2023)\nStatus: Published in The American Statistician 77:1, 51-61, DOI: 10.1080/00031305.2021.2023633, PDF: paper, supplement\n\n\n\nMany approaches for solving problems in business and industry are based on analytics and statistical modeling. Analytical problem solving is driven by the modeling of relationships between dependent (Y) and independent (X) variables, and we discuss three frameworks for modeling such relationships: cause-and-effect modeling, popular in applied statistics and beyond, correlational predictive modeling, popular in machine learning, and deductive (first-principles) modeling, popular in business analytics and operations research. We aim to explain the differences between these types of models, and flesh out the implications of these differences for study design, for discovering potential X/Y relationships, and for the types of solution patterns that each type of modeling could support. We use our account to clarify the popular descriptive-diagnostic-predictive-prescriptive analytics framework, but extend it to offer a more complete model of the process of analytical problem solving, reflecting the essential differences between causal, correlational, and deductive models."
  },
  {
    "objectID": "ideas/datasciencebites/index.html",
    "href": "ideas/datasciencebites/index.html",
    "title": "Data Science Bites",
    "section": "",
    "text": "How machine learning relates to other algorithmic approaches to problem solving\n\n\n\n\n\n\n\nstatistics\n\n\n\n\nProblem solving in its broad sense is the task of figuring out how to go from an unwanted to a wanted situation. Solving problems is the crux of many tasks in management, engineering and science, and methods, techniques and procedures for solving business and engineering problems are studied in various fields. Many approaches for solving problems are based on analytics and statistical modeling, and many techniques in statistics are intended to support problem solving. This paper explains the similaraties and difference between three different frameworks of problem solving: causal modeling, correlational modeling and deductive modeling.\n\n\n\n\n\n\nMay 1, 2022\n\n\nDaniel Kapitan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "ideas/index.html",
    "href": "ideas/index.html",
    "title": "Ideas",
    "section": "",
    "text": "A demonstration of the law of the flowering plants\n\n\n\nprediction\n\n\nhistory of data science\n\n\nstatistics\n\n\n\nThe day a flower blooms is one of the earliest phenomena studied with systematic data collection and analysis. The prediction rule developed nearly three centuries ago is…\n\n\n\nJonathan Auerbach\n\n\nApr 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow machine learning relates to other algorithmic approaches to problem solving\n\n\n\nstatistics\n\n\n\nProblem solving in its broad sense is the task of figuring out how to go from an unwanted to a wanted situation. Solving problems is the crux of many tasks in management…\n\n\n\nDaniel Kapitan\n\n\nMay 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComet charts in Python: visualizing statistical mix effects and Simpson’s paradox with Altair\n\n\n\nvisualization\n\n\n\nZan Armstrong’s comet chart has been on my list of hobby projects for a while now. I think it is an elegant solution to visualize statistical mix effects and address…\n\n\n\nDaniel Kapitan\n\n\nMay 1, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "ideas/tutorials/posts/2023/flowers.html",
    "href": "ideas/tutorials/posts/2023/flowers.html",
    "title": "A demonstration of the law of the flowering plants",
    "section": "",
    "text": "This tutorial will demonstrate a popular method for predicting the day a flower will bloom. There are many reasons why you might want to predict a bloom date. You might be a scientist studying ecosystems stressed by climate change. Or you might be planning a trip to Amsterdam and would like to time your stay to when the tulips are in bloom. Or maybe you are participating in the annual Cherry Blossom Prediction Competition and want some ideas to help you get started.\nIn any case, you might be surprised to learn that the day a flower blooms is one of the earliest phenomena studied with systematic data collection and analysis. The mathematical rule developed in the eighteenth century to make these predictions – now called the “law of the flowering plants” – shaped the direction of statistics as a field and is still used by scientists with relatively few changes.\nWe present the law of the flowering plants as it was stated by Adolphe Quetelet, an influential nineteenth century statistician. Upon completing this tutorial, you will be able to:\nAt the end of the tutorial, we challenge you to design an algorithm that beats our predictions. The tutorial uses the R programming language. In particular, the code relies on the following packages:"
  },
  {
    "objectID": "ideas/tutorials/posts/2023/flowers.html#the-law-of-the-flowering-plants",
    "href": "ideas/tutorials/posts/2023/flowers.html#the-law-of-the-flowering-plants",
    "title": "A demonstration of the law of the flowering plants",
    "section": "The law of the flowering plants",
    "text": "The law of the flowering plants\nWe begin by reviewing the law of the flowering plants as it was stated by Adolphe Quetelet. You may already know Quetelet as the inventor of the body mass index. Less known is that Quetelet recorded the bloom dates of hundreds of different plants between 1833 and 1852 at the Brussels Observatory, which he founded and directed. Quetelet reported that a plant flowers when exposed to a specific quantity of heat, measured in degrees of Celsius squared (°C²). For example, he calculated that a lilac blooms when the sum of the daily temperatures squared exceeds 4264°C² following the last frost.\nHe communicated this law in his Letters addressed to HRH the grand duke of Saxe-Coburg and Gotha (Number 33, 1846; translated 1849) and in his reporting On the climate of Belgium (Chapter 4, Part 4, 1848; data updated in Part 7, 1857). A picture of Quetelet and the title page of On the climate of Belgium are displayed in Figure 1.\n\nQuetelet, Adolphe. 1846. Lettres à s.a.r. Le Duc Régnant de Saxe-Coburg Et Gotha: Sur La Théorie Des Probabilités, Appliquée Aux Sciences Morales Et Politiques. Bruxelles: M. Hayez. https://catalog.hathitrust.org/Record/001387625.\n\n———. 1849. Letters Addressed to h.r.h. The Grand Duke of Saxe Coburg and Gotha on the Theory of Probabilities as Applied to the Moral and Political Sciences. London: C. & E. Layton. https://catalog.hathitrust.org/Record/008956987.\n\nObservatoire royal de Bruxelles. 1848. Annales de l’observatoire Royal de Bruxelles. Bruxelles: M. Hayez. https://catalog.hathitrust.org/Record/000553895.\n\n———. 1857. Sur Le Climat de La Belgique : De l’état Du Ciel En Général. Bruxelles: M. Hayez. https://catalog.hathitrust.org/Record/000553895.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Quetelet reported on the law of the flowering plants in On the climate of Belgium (1857). Sources: Wikimedia Commons, Gallica.\n\nQuetelet was not the first to study bloom dates. Anthophiles have recorded the dates that flowers bloom for centuries. Written records of cherry trees go back as far as 812 AD in Japan and peach and plum trees as far as 1308 AD in China. Systematic record keeping began a century before Quetelet with Robert Marsham’s Indications of Spring (1789).\n\nMarsham, Robert. 1789. “XIII. Indications of Spring, Observed by Robert Marsham, Esquire, f. R. S. Of Stratton in Norfolk. Latitude 52° 45’.” Philosophical Transactions of the Royal Society of London 79: 154–56. https://doi.org/10.1098/rstl.1789.0014.\n\nDe Réaumur, René. 1735. “Observations Du Thermometre, Faites a Paris Pendant l’annees 1735, Comparees a Celles Qui Ont Ete Faites Sous La Ligne, a l’isle de France, a Alger Et En Quelques-Unes de Nos Isles de l’amerique.” Mémoire de l’Académie Royale Des Sciences, 545–76. https://www.academie-sciences.fr/pdf/dossiers/Reaumur/Reaumur_pdf/p545_576_vol3532m.pdf.\nQuetelet was also not the first to study the relationship between temperature and bloom dates. René Réaumur (1735), an early adopter of the thermometer, noted the relationship before Marsham published his Indications. But Quetelet was the first to systematically study the relationship across a wide variety of plants and derive the amount of heat needed to bloom. An example of Quetelet’s careful record keeping can be seen in Figure 2, one of many tables he reported in his publications.\n\n\n\n\n\n\nFigure 2: Bloom dates at Brussels Observatory observed by Quetelet between 1839 and 1852. Source: Gallica."
  },
  {
    "objectID": "ideas/tutorials/posts/2023/flowers.html#reproducing-quetelets-law-of-the-flowering-plants",
    "href": "ideas/tutorials/posts/2023/flowers.html#reproducing-quetelets-law-of-the-flowering-plants",
    "title": "A demonstration of the law of the flowering plants",
    "section": "Reproducing Quetelet’s law of the flowering plants",
    "text": "Reproducing Quetelet’s law of the flowering plants\nTo reproduce Quetelet’s law, we combine the data in Figure 2 with additional observations from his Letters. We focus on Quetelet’s primary example, the bloom date of the common lilac, Syringa vulgaris, row 18 of Figure 2. We do this because Quetelet carefully describes his methodology for measuring the bloom date of lilacs. For example, Quetelet considers a lilac to have bloomed when “the first corolla opens and shows the stamina.” That event is closest to what the USA Phenology Network describes as “open flowers”, depicted in the center image of Figure 3 below. This detail will become relevant when we attempt to replicate Quetelet’s law in a later section. Note that although we focus on lilacs in this tutorial, the R code is easily edited to predict the day that other plants will bloom.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: The bloom date occurs when the first corolla opens and shows the stamina (center image). Source: USA National Phenology Network.\n\nIn the R code below, the five-column tibble lilac contains the date each year that Quetelet observed the lilacs bloom at Brussels Observatory. The first three columns are the month, day, and year the lilacs bloomed between 1839 and 1852. These columns are combined to form the fourth column, the full date the lilacs bloomed. The last column converts the date to the day of the year the lilacs bloomed, abbreviated “doy.” That is, “doy” is the number of days it took for the lilacs bloom following January 1. Both “date” and “doy” representations of Quetelet’s observations will be useful throughout this tutorial.\n```{r}\nlilac &lt;-                   \n  tibble(month = c(\"May\", \"April\", \"April\", \"April\", \"April\", \"April\", \"May\", \n                   \"April\", \"May\", \"April\", \"May\", \"April\", \"May\", \"May\"),\n         day   =  c(10, 28, 24, 28, 20, 25, 13, 12, 9, 21, 2, 30, 1, 12),\n         year  = 1839:1852,\n         date  = as.Date(paste(month, day, year), format = \"%B %d %Y\"),\n         doy   = parse_number(format(date, \"%j\"))) \n\nlilac %&gt;% \n  kable(align = \"c\",\n        caption = \"Table 1: Bloom dates of lilacs observed by Quetelet between 1839 and 1852.\") %&gt;%\n  kable_styling() %&gt;%\n  scroll_box(width = \"100%\", height = \"400px\")\n```\n\n\n\n\n\nTable 1: Bloom dates of lilacs observed by Quetelet between 1839 and 1852.\n\n\n\n\nmonth\n\n\nday\n\n\nyear\n\n\ndate\n\n\ndoy\n\n\n\n\n\n\nMay\n\n\n10\n\n\n1839\n\n\n1839-05-10\n\n\n130\n\n\n\n\nApril\n\n\n28\n\n\n1840\n\n\n1840-04-28\n\n\n119\n\n\n\n\nApril\n\n\n24\n\n\n1841\n\n\n1841-04-24\n\n\n114\n\n\n\n\nApril\n\n\n28\n\n\n1842\n\n\n1842-04-28\n\n\n118\n\n\n\n\nApril\n\n\n20\n\n\n1843\n\n\n1843-04-20\n\n\n110\n\n\n\n\nApril\n\n\n25\n\n\n1844\n\n\n1844-04-25\n\n\n116\n\n\n\n\nMay\n\n\n13\n\n\n1845\n\n\n1845-05-13\n\n\n133\n\n\n\n\nApril\n\n\n12\n\n\n1846\n\n\n1846-04-12\n\n\n102\n\n\n\n\nMay\n\n\n9\n\n\n1847\n\n\n1847-05-09\n\n\n129\n\n\n\n\nApril\n\n\n21\n\n\n1848\n\n\n1848-04-21\n\n\n112\n\n\n\n\nMay\n\n\n2\n\n\n1849\n\n\n1849-05-02\n\n\n122\n\n\n\n\nApril\n\n\n30\n\n\n1850\n\n\n1850-04-30\n\n\n120\n\n\n\n\nMay\n\n\n1\n\n\n1851\n\n\n1851-05-01\n\n\n121\n\n\n\n\nMay\n\n\n12\n\n\n1852\n\n\n1852-05-12\n\n\n133\n\n\n\n\n\n\n\nTo reproduce Quetelet’s law of the flowering plants, we will combine these bloom dates with daily temperature. The daily maximum and minimum temperatures at Brussels Observatory between 1839 and 1852 are available from the Global Historical Climatology Network. The data can be downloaded using the ghcnd_search function contained within the R package rnoaa (2021). The station id for Brussels Observatory is “BE000006447”.\n\nChamberlain, Scott. 2021. “’NOAA’ Weather Data from r [r Package Rnoaa Version 1.3.8].” The Comprehensive R Archive Network. Comprehensive R Archive Network (CRAN). https://CRAN.R-project.org/package=rnoaa.\nThe ghcnd_search function returns the maximum and minimum temperature as separate tibbles in a list. In the R code below, we join the tibbles using the reduce function. Note that the temperature is reported in tenths of a degree (i.e. 0.1°C) so we divide by 10 before calculating the temperature midrange, our estimate of the daily temperature.\nThe result is a five-column tibble temp, which contains the year of the temperature record (“year”), the date of the temperature record (“date”), the maximum temperature (“tmax”), the minimum temperature (“tmin”), and the midrange temperature (“temp”). The first 10 rows of the table are below. When you produce the full table yourself, you may notice that a small portion of temperature records are missing. We found that imputing these missing values does not significantly change the results. Therefore, we ignore these days when conducting our analysis.\n```{r}\ntemp &lt;- \n  ghcnd_search(stationid = \"BE000006447\",\n               var = c(\"tmax\", \"tmin\"),\n               date_min = \"1839-01-01\",\n               date_max = \"1852-12-31\") %&gt;%\n  reduce(left_join) %&gt;%\n  transmute(year = parse_number(format(date, \"%Y\")), \n            date, \n            tmax = tmax / 10, \n            tmin = tmin / 10, \n            temp = (tmax + tmin) / 2)\n  \ntemp %&gt;% \n  kable(align = \"c\", \n        col.names = c(\"year\", \"date\", \"maximum temperature (°C)\", \n                      \"minimum temperature (°C)\", \"midrange temperature (°C)\"),\n        caption = \"Table 2: Temperature observed at Brussels Observatory between 1839 and 1852.\") %&gt;%\n  kable_styling() %&gt;%\n  scroll_box(width = \"100%\", height = \"400px\")\n```\n\n\n\n\n\nTable 2: Temperature observed at Brussels Observatory between 1839 and 1852.\n\n\n\n\nyear\n\n\ndate\n\n\nmaximum temperature (°C)\n\n\nminimum temperature (°C)\n\n\nmidrange temperature (°C)\n\n\n\n\n\n\n1839\n\n\n1839-01-01\n\n\n5.7\n\n\n-0.2\n\n\n2.75\n\n\n\n\n1839\n\n\n1839-01-02\n\n\n6.3\n\n\n0.8\n\n\n3.55\n\n\n\n\n1839\n\n\n1839-01-03\n\n\n7.2\n\n\n1.8\n\n\n4.50\n\n\n\n\n1839\n\n\n1839-01-04\n\n\n8.0\n\n\n1.8\n\n\n4.90\n\n\n\n\n1839\n\n\n1839-01-05\n\n\n5.3\n\n\n0.8\n\n\n3.05\n\n\n\n\n1839\n\n\n1839-01-06\n\n\n10.0\n\n\n1.3\n\n\n5.65\n\n\n\n\n1839\n\n\n1839-01-07\n\n\n8.9\n\n\n1.4\n\n\n5.15\n\n\n\n\n1839\n\n\n1839-01-08\n\n\n3.0\n\n\n0.1\n\n\n1.55\n\n\n\n\n1839\n\n\n1839-01-09\n\n\n0.8\n\n\n-0.1\n\n\n0.35\n\n\n\n\n1839\n\n\n1839-01-10\n\n\n2.8\n\n\n-2.8\n\n\n0.00\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n\n\n\nReproducing Quetelet’s law is now a simple matter of calculating the sum of the squared daily temperature from the day of last frost until the bloom day. We could use the day of last frost reported in Quetelet’s Letters. However, since we will replicate Quetelet’s analysis with recent data in a later section, we use our own definition of the day of last frost. We define the day of last frost to be the day following the last day the maximum temperature is below 0. The R code below creates the function doy_last_frost to extract the day of last frost from the maximum temperature. To demonstrate this function, we then compare the bloom date with the last frost date in 1839, the first year Quetelet observed.\n```{r}\ndoy_last_frost &lt;- function(tmax, doy_max = 100) {\n  dof &lt;- which(tmax[1:doy_max] &lt;= 0)\n  if(length(dof) == 0) 1 else max(dof) + 1\n  }\n\nbloom_day &lt;- \n  lilac %&gt;% \n  filter(year == 1839) %&gt;%\n  pull(doy) + \n  as.Date(\"1839-01-01\")\n  \nfrost_day &lt;- \n  temp %&gt;% \n  filter(year == 1839) %&gt;% \n  pull(tmax) %&gt;% \n  doy_last_frost() + as.Date(\"1839-01-01\") \n\ntibble(`last frost date` = frost_day, \n       `bloom date` = bloom_day) %&gt;%\n  kable(align = \"c\",\n        caption = \"Table 3: Last frost date and lilac bloom date at Brussels Observatory in 1839.\") %&gt;%\n  kable_styling()\n```\n\n\n\n\nTable 3: Last frost date and lilac bloom date at Brussels Observatory in 1839.\n\n\n\n\nlast frost date\n\n\nbloom date\n\n\n\n\n\n\n1839-03-08\n\n\n1839-05-11\n\n\n\n\n\n\nIf Quetelet’s law of the flowering plants is correct, Table 3 has the following interpretation. On March 8, 1839 the lilacs at Brussels Observatory began “collecting” temperature. The lilacs continued to “collect” temperature until May 11, at which point they exceeded their 4264°C² quota and bloomed. We visualize this theory in Figure 4 with the R packages ggplot2, a member of the set of packages that constitute the “tidyverse” (2019), and plotly.\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n```{r}\n(temp %&gt;% \n  filter(date &lt; as.Date(\"1839-06-01\")) %&gt;% \n  ggplot() + \n  aes(date, temp) + \n  geom_line() + \n  labs(\n    x = \"\",\n    y = \"midrange temperature (°C)\",\n    title = \n      \"Figure 4: According to Quetelet's law, the lilacs bloom when exposed to 4264°C² following the last frost.\") +\n  geom_vline(xintercept = as.numeric(c(bloom_day, frost_day)), \n             linetype = \"dotted\")) %&gt;%\n  ggplotly() %&gt;% \n  add_annotations(x = as.numeric(c(frost_day, bloom_day)),\n                  y = c(-4, -4),\n                  text = c(\"last\\nfrost\", \"first\\nbloom\"),\n                  font = list(size = 14),\n                  ay = 0,\n                  xshift = c(-10, -12)) %&gt;%\n  config(displaylogo = FALSE)\n```\n\n\n\n\n\n\nFigure 4: According to Quetelet’s law, the lilacs bloom when exposed to 4264°C² following the last frost. Author provided, CC BY 4.0.\n\nWe now have all the ingredients necessary to reproduce Quetelet’s findings. Our reproduction is greatly simplified by using the nest function from the tidyr package, another member of the “tidyverse”. For an overview of nest, see the “Nested data” section of Grolemund and Wickham (2017). We will group the data by year, nest, calculate the cumulative squared temperature from the frost date to the bloom date within each year, and then unnest. We ignore temperatures below 0°C. That is, temperatures below 0°C are set to 0°C. We do this because it is clear from Quetelet’s derivation of the law that only positive temperatures should be squared. See the next section for details.\n\nGrolemund, Garrett, and Hadley Wickham. 2017. R for Data Science. Sebastopol, CA: O’Reilly Media.\n```{r}\nquetelet &lt;- \n  temp %&gt;% \n  group_by(year) %&gt;% \n  nest() %&gt;% \n  left_join(lilac) %&gt;% \n  mutate(law = map(data, ~ sum(pmax(.$temp, 0, na.rm = TRUE)[(doy_last_frost(.$tmax) + 1):doy]^2))) %&gt;% \n  unnest(law) %&gt;% \n  ungroup()\n\nquetelet %&gt;% \n  summarize(Quetelet = 4264, \n            est = mean(law), \n            se = sd(law)/sqrt(n()),\n            ci  = str_c(\"[\", round(est - 2 * se), \", \", round(est + 2 * se), \"]\")) %&gt;%\n  kable(dig = 0, \n        align = \"c\", \n        col.names = c(\"Quetelet's law (°C²)\", \"estimate (°C²)\", \n                      \"standard error (°C²)\", \"95% confidence interval (°C²)\"),\n        caption = \"Table 4: Reproduction of Quetelet's analysis.\") %&gt;%\n  kable_styling()\n```\n\n\n\n\nTable 4: Reproduction of Quetelet’s analysis.\n\n\n\n\nQuetelet’s law (°C²)\n\n\nestimate (°C²)\n\n\nstandard error (°C²)\n\n\n95% confidence interval (°C²)\n\n\n\n\n\n\n4264\n\n\n4261\n\n\n197\n\n\n[3867, 4656]\n\n\n\n\n\n\nThe results show that Quetelet’s findings are indeed reproducible. Quetelet estimated that lilacs bloom once exposed to 4264°C² following the last frost. Our reanalysis suggests a similar amount. However, 4264°C² is the overall average across all years – the estimated amount needed to bloom varies year to year. As a result, the average has a 95% confidence interval of approximately 3870°C² to 4660°C². Quetelet was well aware of this variation. He argued it was due to unobserved factors that influence growing conditions and change each year, and he dedicated significant space in his Letters to discuss them.\nThese unobserved factors limit the accuracy of predictions made using the law. To assess the predictive accuracy of the law, we temporarily ignore the bloom dates Quetelet observed. Instead, we apply the 4264°C² quota to the temperature records at Brussels Observatory to predict the bloom date. We then compare our predictions with the bloom date Quetelet observed. The R code below creates the function doy_prediction to estimate the day the lilac will bloom from temperature records. Table 5 summarizes the accuracy of Quetelet’s law by the mean absolute error and root mean squared error.\n```{r}\ndoy_prediction &lt;- function(temp, tmax)\n  doy_last_frost(tmax) + which.max(cumsum(pmax(temp[(doy_last_frost(tmax) + 1):365], 0, na.rm = TRUE)^2) &gt; 4264)\n\nquetelet %&gt;% \n  mutate(pred = map(data, ~ doy_prediction(.$temp, .$tmax))) %&gt;% \n  unnest(pred) %&gt;% \n  ungroup() %&gt;%\n  summarize(mae  = mean(abs(doy - pred)),\n            rmse = sqrt(mean((doy - pred)^2))) %&gt;%\n  kable(dig = 0,\n        align = \"c\",\n        col.names = c(\"mean absolute error (days)\", \"root mean squared error (days)\"),\n        caption = \"Table 5: Predictions using Quetelet's law are accurate within a week on average.\") %&gt;%\n  kable_styling()\n```\n\n\n\n\nTable 5: Predictions using Quetelet’s law are accurate within a week on average.\n\n\n\n\nmean absolute error (days)\n\n\nroot mean squared error (days)\n\n\n\n\n\n\n5\n\n\n6\n\n\n\n\n\n\nTable 5 indicates that predictions made using the law are accurate to within a week on average. For comparison purposes, we also predict the day the lilacs will bloom using the average bloom date between 1839 and 1852. That is, on average the lilac bloomed on April 30 (April 29 on leap years), and we check the accuracy of simply predicting this average date each year. Table 6 indicates the average bloom date yields predictions that are less accurate by an average of two days.\n```{r}\nquetelet %&gt;%\n  summarize(pred = mean(doy),\n            mae  = mean(abs(doy - pred)),\n            rmse = sqrt(mean((doy - pred)^2))) %&gt;%\n  select(mae, rmse) %&gt;%\n  kable(dig = 0,\n        align = \"c\",\n        col.names = c(\"mean absolute error (days)\",\n                      \"root mean squared error (days)\"),\n        caption = \"Table 6: Predictions using the average bloom date are off by a week or more on average.\") %&gt;%\n  kable_styling()\n```\n\n\n\n\nTable 6: Predictions using the average bloom date are off by a week or more on average.\n\n\n\n\nmean absolute error (days)\n\n\nroot mean squared error (days)\n\n\n\n\n\n\n7\n\n\n9"
  },
  {
    "objectID": "ideas/tutorials/posts/2023/flowers.html#quetelets-derivation-of-the-law-of-the-flowering-plants",
    "href": "ideas/tutorials/posts/2023/flowers.html#quetelets-derivation-of-the-law-of-the-flowering-plants",
    "title": "A demonstration of the law of the flowering plants",
    "section": "Quetelet’s derivation of the law of the flowering plants",
    "text": "Quetelet’s derivation of the law of the flowering plants\nQuetelet believed that, as in physics, universal laws govern social and biological phenomenon. Quetelet was not only inspired by physics to describe social and biological patterns using mathematical formulas. He often took his formulas directly from physics. In fact, you may have already recognized similarities between his law and Newton’s second law of motion.\nQuetelet reasoned that temperature exerts a “force” on plants in the same way that gravity exerts a force on a falling object. Newton’s second law states that acceleration is proportional to force. It follows that an object initially at rest and subject to a constant force will travel a distance proportional to time squared. Quetelet simply substituted temperature for time.\nWe briefly elaborate. Let \\(d(t)\\) denote the distance an object travels after time \\(t\\). Let \\(v(t) = d'(t)\\) denote its speed and \\(a(t) = v'(t)\\) its acceleration. If acceleration is constant, i.e. \\(a(t) = c\\),\n\n\\(v(t) = \\int_0^t a(s) \\, ds = \\int_0^t c \\, ds = c t\\)\n\nand\n\n\\(d(t) = \\int_0^t v(s) \\, ds = \\int_0^t c s \\, ds = \\tfrac{c}{2} t^2\\)\n\nQuetelet imagined plants experience time in temperature and bloom after “traveling” distance \\(d_*\\). If a plant is exposed to temperature \\(t_i\\) on day \\(i = 1, 2, \\ldots\\), then the bloom date, \\(n_*\\), is the first day \\(\\sum_{i=1}^{n_*} \\tfrac{c}{2} t_i^2 \\geq d_*\\). Multiplying both sides of the inequality by \\(\\tfrac{2}{c}\\), yields Quetelet’s law: the bloom is the first day, \\(n_*\\), that \\(\\sum_{i=1}^{n_*} t_i^2 \\geq \\tfrac{2}{c} d_*\\).\nThe derivation of laws like the law of the flowering plants was popular in the nineteenth century. But any similarities between the “force” of temperature and the force of gravity are likely coincidental. We are not aware of any biological mechanisms that justify Quetelet’s application of Newton’s law.\nToday, the law of the flowering plants is considered a heuristic, or rule of thumb, that approximates complicated biological mechanisms. Like Quetelet, scientists model plants as experiencing time in temperature instead of calendar time. These temperature units are typically called “growing degree days”. Scientists often find that plants may only be sensitive to temperatures in specific ranges or “modified growing degree days”. Although modern statistical methods can greatly improve the accuracy of predictions, laws like Quetelet’s remain popular because they are simple to communicate and easy to replicate, as we demonstrate in the next section."
  },
  {
    "objectID": "ideas/tutorials/posts/2023/flowers.html#replicating-quetelets-law-of-the-flowering-plants",
    "href": "ideas/tutorials/posts/2023/flowers.html#replicating-quetelets-law-of-the-flowering-plants",
    "title": "A demonstration of the law of the flowering plants",
    "section": "Replicating Quetelet’s law of the flowering plants",
    "text": "Replicating Quetelet’s law of the flowering plants\nIn the previous section, we explained how Quetelet derived the law of the flowering plants. Quetelet believed the law of the flowering plants was universal, describing the bloom date of all flowers around the world and in any year. Whether the law can in fact be considered universal requires replicating Quetelet’s results with new data collected at a different location in a different year.\nIn this section, we replicate the law of the flowering plants using lilac bloom dates observed by scientists between 1956 and 2009 at 53 locations throughout the Pacific Northwest (2015). The data can be downloaded from the USA National Phenology Network using the rnpn package (2022). For space considerations, the R code that downloads and cleans the data is provided in the Appendix. Running this code yields the tibble usa_npn. Each row of the tibble corresponds with a bloom date observed at a given site in a given year. There are 31 columns, only seven of which we use in our replication. The remaining columns are documented in the rnpn package, and we will not review them here.\n\nRosemartin, Alyssa H., Ellen G. Denny, Jake F. Weltzin, R. Lee Marsh, Bruce E. Wilson, Hamed Mehdipoor, Raul Zurita-Milla, and Mark D. Schwartz. 2015. “Lilac and Honeysuckle Phenology Data 1956-2014.” Scientific Data 2 (1). https://doi.org/10.1038/sdata.2015.38.\n\nRosemartin, Alyssa, Chamberlain Scott, Lee Marsh, and Kevin Wong. 2022. “Interface to the National ’Phenology’ Network ’API’ [r Package Rnpn Version 1.2.5].” The Comprehensive R Archive Network. Comprehensive R Archive Network (CRAN). https://cran.r-project.org/package=rnpn.\nTable 7 displays six of the seven columns (and only the first 10 rows of the full table). These columns are defined in the same way as the columns of Table 1, except for “site_id”, which denotes the site at which the observation was made. Table 1 does not have a “site_id” column because all observations were made at the same site, Brussels Observatory.\n```{r}\nload(url(\"https://github.com/jauerbach/miscellaneous/blob/main/usa_npn.RData?raw=true\"))\n\nusa_npn %&gt;%\n  transmute(site_id, \n            month = first_yes_month, \n            day   = first_yes_day, \n            year  = first_yes_year, \n            date  = as.Date(paste(month, day, year), format = \"%m %d %Y\"),\n            doy) %&gt;%\n  kable(align = \"c\",\n        caption = \"Table 7: Bloom dates of lilacs observed in pacific northwest between 1956 and 2009.\") %&gt;%\n  kable_styling() %&gt;%\n  scroll_box(width = \"100%\", \n             height = \"400px\")\n```\n\n\n\n\n\nTable 7: Bloom dates of lilacs observed in pacific northwest between 1956 and 2009.\n\n\n\n\nsite_id\n\n\nmonth\n\n\nday\n\n\nyear\n\n\ndate\n\n\ndoy\n\n\n\n\n\n\n150\n\n\n5\n\n\n25\n\n\n1956\n\n\n1956-05-25\n\n\n146\n\n\n\n\n150\n\n\n5\n\n\n22\n\n\n1957\n\n\n1957-05-22\n\n\n142\n\n\n\n\n150\n\n\n5\n\n\n12\n\n\n1958\n\n\n1958-05-12\n\n\n132\n\n\n\n\n150\n\n\n6\n\n\n3\n\n\n1959\n\n\n1959-06-03\n\n\n154\n\n\n\n\n150\n\n\n5\n\n\n27\n\n\n1960\n\n\n1960-05-27\n\n\n148\n\n\n\n\n150\n\n\n5\n\n\n27\n\n\n1961\n\n\n1961-05-27\n\n\n147\n\n\n\n\n150\n\n\n5\n\n\n26\n\n\n1962\n\n\n1962-05-26\n\n\n146\n\n\n\n\n150\n\n\n5\n\n\n24\n\n\n1963\n\n\n1963-05-24\n\n\n144\n\n\n\n\n150\n\n\n5\n\n\n28\n\n\n1964\n\n\n1964-05-28\n\n\n149\n\n\n\n\n150\n\n\n5\n\n\n26\n\n\n1966\n\n\n1966-05-26\n\n\n146\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n\n\n\nThe seventh column we review is “temp”. Each row of “temp” is a tibble of temperature records taken at the nearest station in the Global Historical Climatology Network. The first tibble (again, only the first 10 rows) is displayed in Table 8 below. The columns are defined in the same way as the columns of Table 2, except for “id”, which denotes the location at which the temperature record was made. Table 2 does not have an “id” column because all observations were made at the same site, Brussels Observatory.\n```{r}\nusa_npn %&gt;%\n  pull(temp) %&gt;%\n  .[[1]] %&gt;%\n  mutate(year = parse_number(format(date, \"%Y\"))) %&gt;%\n  select(id, year, date, tmax, tmin, temp) %&gt;%\n  kable(align = \"c\",\n        col.names = c(\"id\", \"year\", \"date\", \"maximum temperature (°C)\", \n                      \"minimum temperature (°C)\", \"midrange temperature (°C)\"),\n        caption = \"Table 8: Temperature observed at an example pacific northwest site in 1956.\") %&gt;%\n  kable_styling() %&gt;%\n  scroll_box(width = \"100%\", \n             height = \"400px\")\n```\n\n\n\n\n\nTable 8: Temperature observed at an example pacific northwest site in 1956.\n\n\n\n\nid\n\n\nyear\n\n\ndate\n\n\nmaximum temperature (°C)\n\n\nminimum temperature (°C)\n\n\nmidrange temperature (°C)\n\n\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-01\n\n\n5.6\n\n\n-5.6\n\n\n0.00\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-02\n\n\n1.7\n\n\n-7.2\n\n\n-2.75\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-03\n\n\n3.3\n\n\n-11.7\n\n\n-4.20\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-04\n\n\n4.4\n\n\n-10.0\n\n\n-2.80\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-05\n\n\n7.8\n\n\n0.0\n\n\n3.90\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-06\n\n\n4.4\n\n\n-11.1\n\n\n-3.35\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-07\n\n\n2.8\n\n\n-6.1\n\n\n-1.65\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-08\n\n\n4.4\n\n\n-4.4\n\n\n0.00\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-09\n\n\n1.7\n\n\n-9.4\n\n\n-3.85\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-10\n\n\n2.8\n\n\n-6.1\n\n\n-1.65\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n\n\n\nWe are now prepared to replicate Quetelet’s findings. We will use R code nearly identical to the code we used to reproduce Quetelet’s findings earlier. The main difference is due to the fact that temperature records are dependent across sites within a year. To account for this dependence, we compute the cumulative temperature squared from the last frost to the bloom date for each site and year. We then take the average across all sites within a year. Finally, we calculate the standard error and confidence interval using only the variation of the averages across years. Table 9 displays the results.\n```{r}\nusa_npn %&gt;%             \n  group_by(rownames(usa_npn)) %&gt;%\n  mutate(law = \n           map(temp, ~ sum(pmax(.$temp, 0, na.rm = TRUE)[(doy_last_frost(.$tmax, doy) + 1):(doy - 1)]^2))) %&gt;%\n  unnest(law) %&gt;% \n  group_by(year) %&gt;%    \n  summarize(law = mean(law)) %&gt;%\n  summarize(Quetelet = 4264, \n            est = mean(law), \n            se = sd(law) / sqrt(n()),\n            ci  = str_c(\"[\", round(est - 2 * se), \", \", round(est + 2 * se), \"]\")) %&gt;%\n  kable(dig = 0, \n        align = \"c\",\n        col.names = c(\"Quetelet's law (°C²)\", \"estimate (°C²)\",\n                      \"standard error (°C²)\", \"95% confidence interval (°C²)\"),\n        caption = \"Table 9: Replication of Quetelet's analysis.\") %&gt;%\n  kable_styling()\n```\n\n\n\n\nTable 9: Replication of Quetelet’s analysis.\n\n\n\n\nQuetelet’s law (°C²)\n\n\nestimate (°C²)\n\n\nstandard error (°C²)\n\n\n95% confidence interval (°C²)\n\n\n\n\n\n\n4264\n\n\n4329\n\n\n116\n\n\n[4098, 4560]\n\n\n\n\n\n\nTable 9 indicates that Quetelet’s findings are replicable in the sense that the confidence interval calculated using Quetelet’s data (Table 4) overlaps with the confidence interval calculated using the USA lilac data (Table 9). The standard error in Table 9 is smaller than Table 4 because the replication uses 54 years of data compared to Quetelet’s 14. Note that in the R code above, we subtract 1 from “doy” to correct for differences in how the bloom date is reported. This correction is not particularly important; the confidence intervals still overlap when this correction is removed.\nWe now investigate the accuracy of Quetelet’s law when applied to the USA lilac data. As before, we make use of the doy_prediction function.\n```{r}\nusa_npn &lt;- \n  usa_npn %&gt;% \n  mutate(pred = map(temp, ~ doy_prediction(.$temp, .$tmax))) %&gt;% \n  unnest(pred) %&gt;% \n  ungroup()\n\nusa_npn %&gt;% \n  summarize(mae  = mean(abs(doy - 1 - pred)),\n            rmse = sqrt(mean((doy - 1 - pred)^2))) %&gt;%\n  kable(dig = 0,\n        align = \"c\", \n        col.names = c(\"mean absolute error (days)\",\n                      \"root mean squared error (days)\"),\n        caption = \"Table 10: Predictions using Quetelet's law are accurate within about two weeks on average.\") %&gt;%\n  kable_styling()\n```\n\n\n\n\nTable 10: Predictions using Quetelet’s law are accurate within about two weeks on average.\n\n\n\n\nmean absolute error (days)\n\n\nroot mean squared error (days)\n\n\n\n\n\n\n10\n\n\n15\n\n\n\n\n\n\nTable 10 indicates that the predictions are accurate to within two weeks on average. Recall that the predictions using Quetelet’s own data were accurate to within one week on average (Table 5). We speculate that the decrease in accuracy is due in part to the fact that both Quetelet’s lilacs and the temperature were observed at the same site, Brussels Observatory. In some cases, the USA lilacs were a few miles from where the temperature was recorded.\nAlthough the accuracy of the predictions made using Quetelet’s law is lower when applied to the USA lilac data, Figure 5 indicates that the law produces the correct bloom date on average. The figure plots the predictions made by the law against the actual bloom dates scientists observed. Note that instead of representing prediction-observation pairs as points in a scatter plot, the data are represented using blue contours. We use contours because there are more than 1,500 observations – too many to study using a scatter plot.\n```{r}\n(usa_npn %&gt;% \n   mutate(doy = first_yes_doy) %&gt;%\n   unnest(pred) %&gt;% \n   ungroup() %&gt;%\n   mutate(predicted = as.Date(\"2020-01-01\") + pred,\n          observed = as.Date(\"2020-01-01\") + doy) %&gt;%\n   ggplot() + \n    aes(x = observed, y = predicted) +\n    geom_density2d(contour_var = \"ndensity\") +\n    geom_abline(intercept = 0, slope = 1, linetype = 2) +\n    labs(x = \"date observed\", \n         y = \"date predicted\",\n         title = \"Figure 5: Predictions using Quetelet's law are accurate within about two weeks on average.\") +\n    theme(legend.position = \"none\")) %&gt;%\n  ggplotly(tooltip = \"\") %&gt;%\n  config(displaylogo = FALSE)\n```\n\n\n\n\n\n\nFigure 5: Predictions using Quetelet’s law are accurate within about two weeks on average. Author provided, CC BY 4.0.\n\nThe contours are easy to interpret. The blue lines are much like a mountain range observed from above. The inner circles are peaks of high elevation in which many prediction-observation pairs co-occur. The outer circles are areas of low elevation in which few prediction-observation pairs co-occur.\nThe dotted line is the “y = x” line, having zero intercept and unit slope. Prediction-observation pairs that lie on the line indicate perfect predictions. The fact that the dotted line intersects the blue contours at their peak suggests the law derived from Quetelet’s data accurately predicts the typical bloom date of the USA data. This accuracy is impressive given the fact that the USA lilacs were observed more than a century later and on a different continent. The blue curves deviate from the line by about two weeks in the vertical direction, which is consistent with Table 10.\nAn average accuracy of two weeks might not sound impressive. But it is far more accurate than using the average bloom date Quetelet observed, April 30 (April 29 on leap years). The average bloom date yields predictions that are off by an additional eleven days on average.\n```{r}\nusa_npn %&gt;%\n  mutate(doy = first_yes_doy) %&gt;%\n  ungroup() %&gt;%\n  summarize(\n    pred = mean(quetelet$doy), \n    mae  = mean(abs(doy - pred)),\n    rmse = sqrt(mean((doy - pred)^2))) %&gt;%\n  select(mae, rmse) %&gt;%\n  kable(\n    dig = 0,\n    align = \"c\",\n    col.names = c(\"mean absolute error (days)\",\n                  \"root mean squared error (days)\"),\n    caption = \"Table 11: Predictions using the average bloom date are off by three weeks or more on average.\") %&gt;%\n  kable_styling()\n```\n\n\n\n\nTable 11: Predictions using the average bloom date are off by three weeks or more on average.\n\n\n\n\nmean absolute error (days)\n\n\nroot mean squared error (days)\n\n\n\n\n\n\n21\n\n\n24"
  },
  {
    "objectID": "ideas/tutorials/posts/2023/flowers.html#predicting-the-day-the-lilac-will-bloom-in-brussels-in-2023",
    "href": "ideas/tutorials/posts/2023/flowers.html#predicting-the-day-the-lilac-will-bloom-in-brussels-in-2023",
    "title": "A demonstration of the law of the flowering plants",
    "section": "Predicting the day the lilac will bloom in Brussels in 2023",
    "text": "Predicting the day the lilac will bloom in Brussels in 2023\nAny weather forecast can become a flower forecast by applying the law of the flowering plants. In this section, we use the AccuWeather forecast to predict the day a hypothetical lilac will bloom in Brussels in 2023. AccuWeather forecasts daily maximum and minimum temperatures three months into the future. We do not evaluate the quality of these forecasts. The purpose of this section is to simply convert them into flower forecasts.\nWe use the AccuWeather forecast as it appeared on the webpage AccuWeather.com on February 19, 2023. AccuWeather reports the forecast for each month on a separate webpage. For reproducibility, we saved each page on the Internet Archive. The following R code creates the function get_weather_table to retrieve each page we saved, extract the forecast contained within that page, and arrange the data as a tibble. The get_weather_table function combines several functions from the rvest package, which is yet another member of the “tidyverse”. In particular, the forecast on each page is contained within the div “monthly-calendar” and can be extracted with the html_nodes and html_text2 functions.\nApplying the get_weather_table function to the url for each page yields a five column tibble temp_br, with columns defined in the same way as the tibble temp, discussed in previous sections. The first 10 rows are below; the data are also available on the author’s GitHub.\n```{r}\n get_weather_table &lt;- function(url)\n  read_html(url) %&gt;% \n  html_nodes(\"div.monthly-calendar\") %&gt;% \n  html_text2() %&gt;%\n  str_remove_all(\"°|Hist. Avg. \") %&gt;%\n  str_split(\" \", simplify = TRUE) %&gt;%\n  parse_number() %&gt;%\n  matrix(ncol = 3, \n         byrow = TRUE,\n         dimnames = list(NULL, c(\"day\", \"tmax\", \"tmin\"))) %&gt;%\n  as_tibble() %&gt;%\n  filter(\n    row_number() %in%\n      (which(diff(day) &lt; 0) %&gt;% (function(x) if(length(x) == 1) seq(1, x[1], 1) else seq(x[1] + 1, x[2], 1))))\n\ntemp_br &lt;-\n  tibble(\n    base_url = \"https://web.archive.org/web/20230219151906/https://www.accuweather.com/en/be/brussels/27581/\",\n    month = month.name[1:5],\n    year = 2023,\n    url = str_c(base_url, tolower(month), \"-weather/27581?year=\", year, \"&unit=c\")) %&gt;%\n  mutate(temp = map(url, get_weather_table)) %&gt;%\n  pull(temp) %&gt;%\n  reduce(bind_rows) %&gt;%\n  transmute(date = seq(as.Date(\"2023-01-01\"), as.Date(\"2023-05-31\"), 1),\n            year = parse_number(format(date, \"%Y\")),\n            tmax,\n            tmin,\n            temp = (tmax + tmin) / 2)\n\ntemp_br %&gt;%\n  relocate(year) %&gt;%\n  kable(dig = 2,\n        align = \"c\", \n        col.names = c(\"year\", \"date\", \"maximum temperature (°C)\",\n                      \"minimum temperature (°C)\", \"midrange temperature (°C)\"),\n        caption = \"Table 12: Temperature forecast for Brussels, retrieved on February 19, 2023.\") %&gt;%\n  kable_styling() %&gt;%\n  scroll_box(width = \"100%\", height = \"400px\")\n```\n\n\n\n\n\nTable 12: Temperature forecast for Brussels, retrieved on February 19, 2023.\n\n\n\n\nyear\n\n\ndate\n\n\nmaximum temperature (°C)\n\n\nminimum temperature (°C)\n\n\nmidrange temperature (°C)\n\n\n\n\n\n\n2023\n\n\n2023-01-01\n\n\n15\n\n\n11\n\n\n13.0\n\n\n\n\n2023\n\n\n2023-01-02\n\n\n14\n\n\n5\n\n\n9.5\n\n\n\n\n2023\n\n\n2023-01-03\n\n\n9\n\n\n3\n\n\n6.0\n\n\n\n\n2023\n\n\n2023-01-04\n\n\n13\n\n\n8\n\n\n10.5\n\n\n\n\n2023\n\n\n2023-01-05\n\n\n12\n\n\n10\n\n\n11.0\n\n\n\n\n2023\n\n\n2023-01-06\n\n\n12\n\n\n10\n\n\n11.0\n\n\n\n\n2023\n\n\n2023-01-07\n\n\n11\n\n\n9\n\n\n10.0\n\n\n\n\n2023\n\n\n2023-01-08\n\n\n10\n\n\n6\n\n\n8.0\n\n\n\n\n2023\n\n\n2023-01-09\n\n\n8\n\n\n5\n\n\n6.5\n\n\n\n\n2023\n\n\n2023-01-10\n\n\n12\n\n\n4\n\n\n8.0\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n\n\n\nWe now predict the day the lilacs will bloom. The R code below uses the doy_prediction and doy_last_frost functions created in earlier sections and displays the prediction in Table 13. At the time of our writing, the predicted date is April 19. The forecast is easily updated by providing the url to the updated AccuWeather webpage. (You might use the url https://web.archive.org/save to save a webpage to the Internet Archive to ensure your work is reproducible.)\n```{r}\nbloom_day_br &lt;-\n  temp_br %&gt;%\n  summarize(date = doy_prediction(temp, tmax) + as.Date(\"2023-01-01\")) %&gt;%\n  pull(date)\n\nfrost_day_br &lt;- \n  temp_br %&gt;% \n  pull(tmax) %&gt;% \n  doy_last_frost() + as.Date(\"2023-01-01\") \n\ntibble(`last frost date` = frost_day_br, \n       `bloom date` = bloom_day_br) %&gt;%\n  kable(align = \"c\",\n        caption = \"Table 13: Last frost date and lilac bloom date in Brussels in 2023.\") %&gt;%\n  kable_styling()\n```\n\n\n\n\nTable 13: Last frost date and lilac bloom date in Brussels in 2023.\n\n\n\n\nlast frost date\n\n\nbloom date\n\n\n\n\n\n\n2023-01-27\n\n\n2023-04-19\n\n\n\n\n\n\nWe visualize the predictions in Figure 6, which has the same interpretation as Figure 4. If the temperature forecast and Quetelet’s law are correct, on January 27, 2023 the lilacs in Brussels began “collecting” temperature. The lilacs will continue to “collect” temperature until April 19, at which point they will exceed their 4264°C² quota and bloom.\n```{r}\n(temp_br %&gt;% \n  ggplot() + \n  aes(date, temp) + \n  geom_line() + \n  labs(\n    x = \"\",\n    y = \"midrange temperature (°C)\",\n    title =\n      \"Figure 6: According to Quetelet's law, the lilacs will bloom once exposed to 4264°C² following the last frost.\") +\n  geom_vline(xintercept = as.numeric(c(frost_day_br, bloom_day_br)), \n             linetype = \"dotted\")) %&gt;%\n  ggplotly() %&gt;% \n  add_annotations(x = as.numeric(c(frost_day_br, bloom_day_br)),\n                  y = c(14, 14),\n                  text = c(\"last\\nfrost\", \"first\\nbloom\"),\n                  font = list(size = 14),\n                  ay = 0,\n                  xshift = c(-14, -16)) %&gt;%\n  config(displaylogo = FALSE)\n```\n\n\n\n\n\n\nFigure 6: According to Quetelet’s law, the lilacs will bloom once exposed to 4264°C² following the last frost. Author provided, CC BY 4.0."
  },
  {
    "objectID": "ideas/tutorials/posts/2023/flowers.html#quetelets-legacy-advocate-mentor-and-perhaps-data-scientist",
    "href": "ideas/tutorials/posts/2023/flowers.html#quetelets-legacy-advocate-mentor-and-perhaps-data-scientist",
    "title": "A demonstration of the law of the flowering plants",
    "section": "Quetelet’s legacy: advocate, mentor, and perhaps data scientist",
    "text": "Quetelet’s legacy: advocate, mentor, and perhaps data scientist\nIn this tutorial, we stated the law of the flowering plants and explained how Quetelet derived it. We also reproduced and replicated Quetelet’s findings before using his law to predict the day the lilac will bloom in Brussels. We now conclude with a reflection on Quetelet’s legacy.\nThe law of the flowering plants surely stands the test of time. It continues to be used by scientists – with relatively few changes – to plan harvests, manage pests, and study ecosystems stressed by climate change. We speculate the law’s longevity is due to the fact that it balances simplicity with relatively accurate predictions.\nAlthough Quetelet did not discover the law, he did much to advance it. Quetelet founded an international network for “observations of the periodical phenomena” (in addition to numerous statistical societies and publications, including the precursor to the Royal Statistical Society). Quetelet’s network of 80 stations collected observations throughout Europe from 1841 until 1872. In particular, Quetelet collaborated with Charles Morren – who later coined the term phenology, the name of the field that now studies biological life-cycle events like the timing of flower blooms (Demarée and Rutishauser 2011).\n\nDemarée, Gaston R., and This Rutishauser. 2011. “From ‘Periodical Observations’ to ‘Anthochronology’ and ‘Phenology’ – the Scientific Debate Between Adolphe Quetelet and Charles Morren on the Origin of the Word ‘Phenology’.” International Journal of Biometeorology 55 (6): 753–61. https://doi.org/10.1007/s00484-011-0442-5.\nIn recent years, the observations collected through phenology networks have become an important resource for understanding the impacts of climate change. For example, the USA National Phenology Network calculates the Spring Bloom Index, which measures the “first day of spring” using the days lilacs are observed to bloom at locations across the United States. The index is then compared to previous years. Figure 7 shows one comparison, called the Return Interval. The Return Interval is much like a p-value, calculating how frequently more extreme spring indices were observed in previous decades. Bloom dates that are uncommonly early (green) or late (purple) may indicate environments stressed by changing climate. Scientists exploit the relationship between temperature and bloom date to extrapolate the index to areas with few observations.\n\n\n\n\n\n\nFigure 7: The Spring Bloom Index Return Interval measures whether spring is typical when compared to recent decades. Source: USA National Phenology Network.\n\nQuetelet’s emphasis on discovering the universal laws he believed govern social and biological phenomenon has not endured. But data scientists continue to appropriate laws from one area of science to study another. For example, data scientists use neural networks and genetic algorithms to study a wide variety of phenomenon unrelated to neuroscience or genetics. Perhaps Quetelet’s appropriation of Newton’s law, in addition to his careful use of data, make him among the first data scientists?"
  },
  {
    "objectID": "ideas/tutorials/posts/2023/flowers.html#your-turn-do-you-have-what-it-takes-to-beat-quetelets-law",
    "href": "ideas/tutorials/posts/2023/flowers.html#your-turn-do-you-have-what-it-takes-to-beat-quetelets-law",
    "title": "A demonstration of the law of the flowering plants",
    "section": "Your turn: Do you have what it takes to beat Quetelet’s law?",
    "text": "Your turn: Do you have what it takes to beat Quetelet’s law?\nQuetelet reported that a plant flowers when the sum of the daily temperatures squared exceeds a specific quantity. His prediction rule was state of the art in 1833. But surely you, a twenty-first century data scientist, can do better. Here are some ideas to get you started.\n\nQuetelet squared the temperature before calculating the sum. Would another function of temperature produce a more accurate prediction?\n\nRemove the square so that a plant flowers once the sum of the daily temperatures exceeds a (different) specific quantity. Does this version of the law produce more accurate predictions? What if you use the daily temperatures cubed? (Beginner)\nSuppose a lilac only registers temperatures between 0°C and 10°C. That is, a lilac experiences temperature below the lower limit, 0°C, as 0°C, and above the upper limit, 10°C, as 10°C. Does the accuracy of the predictions improve if you use the temperature the lilac experienced instead of the ambient temperature measured by a weather station? Write a program that finds the lower and upper limits that produce the most accurate predictions. (Intermediate)\nQuetelet used mean absolute error to evaluate the accuracy of his predictions. But his estimate of the specific quantity of heat needed to bloom, 4264°C², does not actually minimize mean absolute error. Write a program that finds the specific quantity that minimizes mean absolute error. Redo part i. and ii. using this function. (Advanced)\n\nQuetelet calculated the sum of the daily temperature squared between the day of last frost and the bloom date. Would another time interval produce more accurate predictions?\n\nWe estimated the day of last frost using the last day the maximum temperature was below 0°C. Try estimating the day of last frost by the last day the midrange temperature was below 0°C? Which estimate yields the most accurate predictions? What if you ignore the day of last frost and simply calculate the sum of the daily temperatures squared between February 1 and the bloom date? When you change the time interval, be sure to calculate the new specific quantity of heat needed to bloom. (Beginner)\nWrite a program that finds the time interval which yields the best predictions. (Intermediate)\nWrite a program that calculates the prediction rule for many different time intervals. Use cross-validation to combine these prediction rules into a single prediction rule. (Advanced)\n\nQuetelet’s law only considers the temperature. Would additional information provide more accurate predictions?\n\nIs the specific quantity of heat needed to bloom different in years with abnormally cold winters? Would the predictions be more accurate if you use one quantity of heat for years with cold winters and a different quantity of heat for years with warm winters? (Beginner)\nIs the estimated quantity of heat needed to bloom similar for locations close in space and time? Write a program that leverages spatial and temporal correlation to improve the accuracy of the predictions. (Intermediate)\nSome biologists report that a plant must be exposed to a fixed amount of cold temperature in the winter – in addition to a fixed amount of warm temperature in the spring – before it can bloom. Augment the law of the flowering plants to require the accumulation of a specific quantity of cold temperature before the accumulation of a specific quantity of warm temperature. Write a program that uses this new law to predict the day the lilac blooms. (Advanced)\n\n\nFeeling good about your prediction algorithm? Show it off at the annual Cherry Blossom Prediction Competition!"
  },
  {
    "objectID": "ideas/tutorials/posts/2023/flowers.html#appendix-preparing-usa-npn-data",
    "href": "ideas/tutorials/posts/2023/flowers.html#appendix-preparing-usa-npn-data",
    "title": "A demonstration of the law of the flowering plants",
    "section": "Appendix: Preparing USA NPN Data",
    "text": "Appendix: Preparing USA NPN Data\n```{r}\n# 1. download lilac data using `rnpn`\nusa_npn &lt;- \n  npn_download_individual_phenometrics(request_source = \"Jonathan Auerbach\",\n                                       year = 1900:2050,\n                                       species_ids = 36,                       \n                                       phenophase_ids = c(77, 412))            \n\n# 2. limit analysis to sites that report more than 25 times\nsite_ids &lt;- \n  usa_npn %&gt;% \n  group_by(site_id) %&gt;% \n  summarize(n = n()) %&gt;% filter(n &gt; 25) %&gt;% pull(site_id)\n\nusa_npn &lt;- \n  usa_npn %&gt;% \n  filter(site_id %in% site_ids)\n\n# 3. find nearest weather stations for each site\nlocations &lt;- \n  usa_npn %&gt;% \n  group_by(site_id) %&gt;% \n  summarize(latitude = first(latitude), \n            longitude = first(longitude))\n\nstations &lt;- \n  ghcnd_stations() %&gt;%\n  filter(first_year &lt;= min(usa_npn$first_yes_year),\n         last_year  &gt;= max(usa_npn$first_yes_year),\n         state != \"\") %&gt;%\n  group_by(id, latitude, longitude, state) %&gt;%\n  summarize(temp_flag = sum(element %in% c(\"TMIN\", \"TMAX\"))) %&gt;%            \n  filter(temp_flag == 2) %&gt;% \n  ungroup()\n\ndist &lt;- function(x, y = stations %&gt;% select(latitude, longitude)) \n  stations$id[which.min(sqrt((x[1] - y[,1])^2 + (x[2] - y[,2])^2)[,1])]\n\nlocations$station_id &lt;- apply(locations, 1, function(x) dist(c(x[\"latitude\"], x[\"longitude\"])))\n\n# 4. get weather data from nearest station using `rnoaa`\nget_station_data &lt;- function(station_id) \n  ghcnd_search(stationid = station_id,\n               var = c(\"tmin\", \"tmax\"),\n               date_min = \"1956-01-01\",\n               date_max = \"2011-12-31\") %&gt;%\n  reduce(left_join, by = c(\"id\", \"date\")) %&gt;%\n  transmute(id, \n            date, \n            tmax = tmax / 10,\n            tmin = tmin / 10)\n\nusa_npn &lt;- \n  locations %&gt;%\n  mutate(temp = map(station_id, get_station_data)) %&gt;%\n  right_join(usa_npn, by = c(\"site_id\", \"latitude\", \"longitude\")) %&gt;% \n  group_by(rownames(usa_npn)) %&gt;% \n  mutate(temp = map(temp, ~ .x %&gt;% \n                      filter(format(date, \"%Y\") == first_yes_year) %&gt;%\n                      mutate(temp = (tmin + tmax) / 2)),\n         num_obs = map(temp,~ sum(format(.x$date,\"%j\") &lt;= 150)),\n         doy = first_yes_doy, year = first_yes_year) %&gt;% \n  unnest(num_obs) %&gt;%  \n  filter(num_obs == 150) %&gt;%\n  ungroup()\n```\n\nExplore more Tutorials\n\n\n\n\n\nAbout the author\n\nJonathan Auerbach is an assistant professor in the Department of Statistics at George Mason University. His research covers a wide range of topics at the intersection of statistics and public policy. His interests include the analysis of longitudinal data, particularly for data science and causal inference, as well as urban analytics, open data, and the collection, evaluation, and communication of official statistics. He co-organizes the annual Cherry Blossom Prediction Competition with David Kepplinger and Elizabeth Wolkovich.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Jonathan Auerbach\n\n\n  Text and code are licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Images are not covered by this licence, except where otherwise noted.\n\n\n\nHow to cite\n\nAuerbach, Jonathan. 2023. “A demonstration of the law of the flowering plants.” Real World Data Science, April 13, 2023. URL"
  },
  {
    "objectID": "ideas/tutorials/posts/2021/comet-charts.html",
    "href": "ideas/tutorials/posts/2021/comet-charts.html",
    "title": "Comet charts in Python: visualizing statistical mix effects and Simpson’s paradox with Altair",
    "section": "",
    "text": "Zan Armstrong’s has been on my list of hobby projects for a while now. I think it is an elegant solution to visualize statistical mix effects and address Simpson’s paradox, and particularly useful when working with longitudinal data involving different sub-populations. Recently I found a good excuse to spend some time to actually use it as part of a exploratory data analysis on a project.\nSince I mostly work in Python and have recently fallen in love with Altair — for the same reasons as explained here — I wondered how the comet chart could be implemented using the grammar of interactive graphics. It took me a while to figure out how to actually plot the comets. In a previous version, I had drawn glyphs using Bokeh. While Altair allows you to plot any SVG path in a graph, this felt a bit hacky and not quite in line with the philosophy of using a grammar of graphics.\nThankfully Mattijn was quick to suggest using trail-marks, after which it was almost as easy as pie. So here’s an example using a dataset of 20,000 flights for 59 destination airports.\n\n\n\n\nIn the example shown here, each comet represents one destination airport. The head of the comet corresponds to the most recent observation of the number of flight arrivals (x-axis, shown as logarithmic scale to accommodate the wide range of observations) against the mean delay of those flights (y-axis). The tail of the comet represents a similar (x,y) datum, but from an earlier point in time. Finally, the colour of the comet is encoded to show the change in the mean delay for each airport. A tooltip with a summary of the data is shown when hovering over the head of the comet. So-called mix effects can often lead to misinterpretation of aggregate numbers. In the example of flight delays, the fact that only a small change is observed in the mean delay across all airports — visualized with the right-most comet outlined in black — hides the underlying variance between airports. Note that in this example the size of each sub-population (number of flights per airport) remains relatively constant, hence the comets here only go up and down. As explained in the original article, mix effects become harder to interpret when the relative size of the sub-populations change as well as their relative values. In the most extreme case this may lead to Simpson’s paradox.\nWith this base implementation of comet charts in Altair, you can really go to town and combine it with other interactive graphs. Using the overview-detail pattern, you could plot an accompanying density plot of all the flights for a given airport. That way you can quickly zoom in to the lowest level of detail and get a better understanding of the underlying mix effects.\nFor now, I will leave you with the Python code to make the plot.\n\nimport altair as alt\nimport pandas as pd\nimport vega_datasets\n\n\n# Use airline data to assess statistical mix effects of delays\nflights = vega_datasets.data.flights_20k()\naggregation = dict(\n    number_of_flights=(\"destination\", \"count\"),\n    mean_delay=(\"delay\", \"mean\"),\n    mean_distance=(\"distance\", \"mean\"),\n)\n\n# Compare delays by destination between month 1 and 3\ngrouped = flights.groupby(by=[flights.destination, flights.date.dt.month])\ndf = (\n    grouped.agg(**aggregation)\n    .loc[(slice(None), [1, 3]), :]\n    .assign(\n        change_mean_delay=lambda df: df.groupby(\"destination\")[\"mean_delay\"].diff(),\n    )\n    .fillna(method=\"bfill\")\n    .reset_index()\n    .round(2)\n)\n\n# Calculate weigthed average of delays for month 1 and 3\ntotal = (\n    flights.groupby(flights.date.dt.month)\n    .agg(**aggregation)\n    .loc[[1, 3], :]\n    .assign(\n        change_mean_delay=lambda df: df.mean_delay.diff(),\n        destination='TOTAL'\n    )\n    .fillna(method=\"bfill\")\n    .round(2)\n    .reset_index()\n    .loc[:, df.columns]\n)\n\n\ndef comet_chart(df, stroke=\"white\"):\n    return (\n    alt.Chart(df, width=700, height=500)\n    .mark_trail(stroke=stroke)\n    .encode(\n        x=alt.X(\"number_of_flights\", scale=alt.Scale(type=\"log\")),\n        y=alt.Y(\"mean_delay\"),\n        detail=\"destination\",\n        size=alt.Size(\"date\", scale=alt.Scale(range=[0, 10]), legend=None),\n        tooltip=[\n            \"destination\",\n            \"number_of_flights\",\n            \"mean_delay\",\n            \"change_mean_delay\",\n            \"mean_distance\",\n        ],\n        # trails don't support continuous color, see https://github.com/vega/vega/issues/1187\n        # hence use bins\n        color=alt.Color(\n            \"change_mean_delay:Q\",\n            bin=alt.Bin(step=2),\n            scale=alt.Scale(scheme=\"blueorange\"),\n            legend=alt.Legend(orient=\"top\"),\n        ),\n    )\n)\n\nchart = comet_chart(df) + comet_chart(total, stroke=\"black\")"
  },
  {
    "objectID": "ideas/tutorials/index.html",
    "href": "ideas/tutorials/index.html",
    "title": "Tutorials",
    "section": "",
    "text": "A demonstration of the law of the flowering plants\n\n\n\n\n\n\n\nprediction\n\n\nhistory of data science\n\n\nstatistics\n\n\n\n\nThe day a flower blooms is one of the earliest phenomena studied with systematic data collection and analysis. The prediction rule developed nearly three centuries ago is still used by scientists – with relatively few changes – to plan harvests, manage pests, and study ecosystems stressed by climate change. In this tutorial, Jonathan Auerbach introduces the “law of the flowering plants” and demonstrates how it can be applied to modern datasets.\n\n\n\n\n\n\nApr 13, 2023\n\n\nJonathan Auerbach\n\n\n\n\n\n\n  \n\n\n\n\nComet charts in Python: visualizing statistical mix effects and Simpson’s paradox with Altair\n\n\n\n\n\n\n\nvisualization\n\n\n\n\nZan Armstrong’s comet chart has been on my list of hobby projects for a while now. I think it is an elegant solution to visualize statistical mix effects and address Simpson’s paradox, and particularly useful when working with longitudinal data involving different sub-populations. Recently I found a good excuse to spend some time to actually use it as part of a exploratory data analysis on a project.\n\n\n\n\n\n\nMay 1, 2022\n\n\nDaniel Kapitan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "ts-and-cs.html",
    "href": "ts-and-cs.html",
    "title": "Terms and conditions",
    "section": "",
    "text": "Coming soon"
  }
]