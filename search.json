[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Custom jumbotron",
    "section": "",
    "text": "Custom jumbotron\n    Using a series of utilities, you can create this jumbotron, just like the one in previous versions of Bootstrap. Check out the examples below for how you can remix and restyle it to your liking.\n    Example button\n  \n\n\n\n\n\n\n\nHello, I am Daniel Kapitan.\n\n\n\nI am a data architect and AI advisor who helps organizations create sustainable change by systematically engineering data that is required for new AI applications. As a consultant, my approach is centered on delivering measurable results by developing customized solutions that meet the specific needs and goals of your organization. With my expertise in machine learning, I can help you extract valuable insights from your data that can inform strategic decisions, optimize operations, and identify new opportunities for growth."
  },
  {
    "objectID": "src/blog/posts/comet-chart.html",
    "href": "src/blog/posts/comet-chart.html",
    "title": "Comet charts in Python",
    "section": "",
    "text": "Zan Armstrong’s comet chart has been on my list of hobby projects for a while now. I think it is an elegant solution to visualize statistical mix effects and address Simpson’s paradox, and particularly useful when working with longitudinal data involving different sub-populations. Recently I found a good excuse to spend some time to actually use it as part of a exploratory data analysis on a project.\nSince I mostly work in Python and have recently fallen in love with Altair — for the same reasons as Fernando explains here — I wondered how the comet chart could be implemented using the grammar of interactive graphics. It took me a while to figure out how to actually plot the comets. In a previous version, I had drawn glyphs using Boke. While Altair allows you to plot any SVG path in a graph, this felt a bit hacky and not quite in line with the philosophy of using a grammar of graphics.\nThankfully Mattijn was quick to suggest using trail-marks, after which it was almost as easy as pie. So here’s an example using a dataset of 20,000 flights for 59 destination airports.\n\nimport altair as alt\nimport pandas as pd\nimport vega_datasets\n\n\n# Use airline data to assess statistical mix effects of delays\nflights = vega_datasets.data.flights_20k()\naggregation = dict(\n    number_of_flights=(\"destination\", \"count\"),\n    mean_delay=(\"delay\", \"mean\"),\n    mean_distance=(\"distance\", \"mean\"),\n)\n\n# Compare delays by destination between month 1 and 3\ngrouped = flights.groupby(by=[flights.destination, flights.date.dt.month])\ndf = (\n    grouped.agg(**aggregation)\n    .loc[(slice(None), [1, 3]), :]\n    .assign(\n        change_mean_delay=lambda df: df.groupby(\"destination\")[\"mean_delay\"].diff(),\n    )\n    .fillna(method=\"bfill\")\n    .reset_index()\n    .round(2)\n)\n\n# Calculate weigthed average of delays for month 1 and 3\ntotal = (\n    flights.groupby(flights.date.dt.month)\n    .agg(**aggregation)\n    .loc[[1, 3], :]\n    .assign(\n        change_mean_delay=lambda df: df.mean_delay.diff(),\n        destination='TOTAL'\n    )\n    .fillna(method=\"bfill\")\n    .round(2)\n    .reset_index()\n    .loc[:, df.columns]\n)\n\n\ndef comet_chart(df, stroke=\"white\"):\n    return (\n    alt.Chart(df, width=600, height=450)\n    .mark_trail(stroke=stroke)\n    .encode(\n        x=alt.X(\"number_of_flights\", scale=alt.Scale(type=\"log\")),\n        y=alt.Y(\"mean_delay\"),\n        detail=\"destination\",\n        size=alt.Size(\"date\", scale=alt.Scale(range=[0, 10]), legend=None),\n        tooltip=[\n            \"destination\",\n            \"number_of_flights\",\n            \"mean_delay\",\n            \"change_mean_delay\",\n            \"mean_distance\",\n        ],\n        # trails don't support continuous color, see https://github.com/vega/vega/issues/1187\n        # hence use bins\n        color=alt.Color(\n            \"change_mean_delay:Q\",\n            bin=alt.Bin(step=2),\n            scale=alt.Scale(scheme=\"blueorange\"),\n            legend=alt.Legend(orient=\"top\"),\n        ),\n    )\n)\n\n\ncomet_chart(df) + comet_chart(total, stroke=\"black\")\n\n\n\n\n\n\n\nIn the example shown here, each comet represents one destination airport. The head of the comet corresponds to the most recent observation of the number of flight arrivals (x-axis, shown as logarithmic scale to accommodate the wide range of observations) against the mean delay of those flights (y-axis). The tail of the comet represents a similar (x,y) datum, but from an earlier point in time. Finally, the colour of the comet is encoded to show the change in the mean delay for each airport. A tooltip with a summary of the data is shown when hovering over the head of the comet.\nSo-called mix effects can often lead to misinterpretation of aggregate numbers. In the example of flight delays, the fact that only a small change is observed in the mean delay across all airports — visualized with the right-most comet outlined in black — hides the underlying variance between airports. Note that in this example the size of each sub-population (number of flights per airport) remains relatively constant, hence the comets here only go up and down. As explained in the original article, mix effects become harder to interpret when the relative size of the sub-populations change as well as their relative values. In the most extreme case this may lead to Simpson’s paradox.\nWith this base implementation of comet charts in Altair, you can really go to town and combine it with other interactive graphs. Using the overview-detail pattern, you could plot an accompanying density plot of all the flights for a given airport. That way you can quickly zoom in to the lowest level of detail and get a better understanding of the underlying mix effects."
  },
  {
    "objectID": "src/blog/posts/analytical-problem-solving.html",
    "href": "src/blog/posts/analytical-problem-solving.html",
    "title": "Analytical problem solving",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "src/projects/project-1.html",
    "href": "src/projects/project-1.html",
    "title": "MyTomorrows",
    "section": "",
    "text": "Story about MyTomorrows …\n\n  \n    \n      \n    \n    \n      \n    \n  \n  \n    \n    Previous\n  \n  \n    \n    Next"
  },
  {
    "objectID": "images/altair_marks_encoding.out.html",
    "href": "images/altair_marks_encoding.out.html",
    "title": "Data Types, Graphical Marks, and Visual Encoding Channels",
    "section": "",
    "text": "A visualization represents data using a collection of graphical marks (bars, lines, points, etc.). The attributes of a mark — such as its position, shape, size, or color — serve as channels through which we can encode underlying data values.\nWith a basic framework of data types, marks, and encoding channels, we can concisely create a wide variety of visualizations. In this notebook, we explore each of these elements and show how to use them to create custom statistical graphics.\nThis notebook is part of the data visualization curriculum.\nimport pandas as pd\nimport altair as alt"
  },
  {
    "objectID": "images/altair_marks_encoding.out.html#global-development-data",
    "href": "images/altair_marks_encoding.out.html#global-development-data",
    "title": "Data Types, Graphical Marks, and Visual Encoding Channels",
    "section": "Global Development Data",
    "text": "Global Development Data\nWe will be visualizing global health and population data for a number of countries, over the time period of 1955 to 2005. The data was collected by the Gapminder Foundation and shared in Hans Rosling’s popular TED talk. If you haven’t seen the talk, we encourage you to watch it first!\nLet’s first load the dataset from the vega-datasets collection into a Pandas data frame.\n\nfrom vega_datasets import data as vega_data\ndata = vega_data.gapminder()\n\nHow big is the data?\n\ndata.shape\n\n(693, 6)\n\n\n693 rows and 6 columns! Let’s take a peek at the data content:\n\ndata.head(5)"
  },
  {
    "objectID": "images/altair_marks_encoding.out.html#data-types",
    "href": "images/altair_marks_encoding.out.html#data-types",
    "title": "Data Types, Graphical Marks, and Visual Encoding Channels",
    "section": "Data Types",
    "text": "Data Types\nThe first ingredient in effective visualization is the input data. Data values can represent different forms of measurement. What kinds of comparisons do those measurements support? And what kinds of visual encodings then support those comparisons?\nWe will start by looking at the basic data types that Altair uses to inform visual encoding choices. These data types determine the kinds of comparisons we can make, and thereby guide our visualization design decisions.\n\nNominal (N)\nNominal data (also called categorical data) consist of category names.\nWith nominal data we can compare the equality of values: is value A the same or different than value B? (A = B), supporting statements like “A is equal to B” or “A is not equal to B”. In the dataset above, the country field is nominal.\nWhen visualizing nominal data we should readily be able to see if values are the same or different: position, color hue (blue, red, green, etc.), and shape can help. However, using a size channel to encode nominal data might mislead us, suggesting rank-order or magnitude differences among values that do not exist!\n\n\nOrdinal (O)\nOrdinal data consist of values that have a specific ordering.\nWith ordinal data we can compare the rank-ordering of values: does value A come before or after value B? (A &lt; B), supporting statements like “A is less than B” or “A is greater than B”. In the dataset above, we can treat the year field as ordinal.\nWhen visualizing ordinal data, we should perceive a sense of rank-order. Position, size, or color value (brightness) might be appropriate, where as color hue (which is not perceptually ordered) would be less appropriate.\n\n\nQuantitative (Q)\nWith quantitative data we can measure numerical differences among values. There are multiple sub-types of quantitative data:\nFor interval data we can measure the distance (interval) between points: what is the distance to value A from value B? (A - B), supporting statements such as “A is 12 units away from B”.\nFor ratio data the zero-point is meaningful and so we can also measure proportions or scale factors: value A is what proportion of value B? (A / B), supporting statements such as “A is 10% of B” or “B is 7 times larger than A”.\nIn the dataset above, year is a quantitative interval field (the value of year “zero” is subjective), whereas fertility and life_expect are quantitative ratio fields (zero is meaningful for calculating proportions). Vega-Lite represents quantitative data, but does not make a distinction between interval and ratio types.\nQuantitative values can be visualized using position, size, or color value, among other channels. An axis with a zero baseline is essential for proportional comparisons of ratio values, but can be safely omitted for interval comparisons.\n\n\nTemporal (T)\nTemporal values measure time points or intervals. This type is a special case of quantitative values (timestamps) with rich semantics and conventions (i.e., the Gregorian calendar). The temporal type in Vega-Lite supports reasoning about time units (year, month, day, hour, etc.), and provides methods for requesting specific time intervals.\nExample temporal values include date strings such as “2019-01-04” and “Jan 04 2019”, as well as standardized date-times such as the ISO date-time format: “2019-01-04T17:50:35.643Z”.\nThere are no temporal values in our global development dataset above, as the year field is simply encoded as an integer. For more details about using temporal data in Altair, see the Times and Dates documentation.\n\n\nSummary\nThese data types are not mutually exclusive, but rather form a hierarchy: ordinal data support nominal (equality) comparisons, while quantitative data support ordinal (rank-order) comparisons.\nMoreover, these data types do not provide a fixed categorization. Just because a data field is represented using a number doesn’t mean we have to treat it as a quantitative type! For example, we might interpret a set of ages (10 years old, 20 years old, etc) as nominal (underage or overage), ordinal (grouped by year), or quantitative (calculate average age).\nNow let’s examine how to visually encode these data types!"
  },
  {
    "objectID": "images/altair_marks_encoding.out.html#encoding-channels",
    "href": "images/altair_marks_encoding.out.html#encoding-channels",
    "title": "Data Types, Graphical Marks, and Visual Encoding Channels",
    "section": "Encoding Channels",
    "text": "Encoding Channels\nAt the heart of Altair is the use of encodings that bind data fields (with a given data type) to available encoding channels of a chosen mark type. In this notebook we’ll examine the following encoding channels:\n\nx: Horizontal (x-axis) position of the mark.\ny: Vertical (y-axis) position of the mark.\nsize: Size of the mark. May correspond to area or length, depending on the mark type.\ncolor: Mark color, specified as a legal CSS color.\nopacity: Mark opacity, ranging from 0 (fully transparent) to 1 (fully opaque).\nshape: Plotting symbol shape for point marks.\ntooltip: Tooltip text to display upon mouse hover over the mark.\norder: Mark ordering, determines line/area point order and drawing order.\ncolumn: Facet the data into horizontally-aligned subplots.\nrow: Facet the data into vertically-aligned subplots.\n\nFor a complete list of available channels, see the Altair encoding documentation.\n\nX\nThe x encoding channel sets a mark’s horizontal position (x-coordinate). In addition, default choices of axis and title are made automatically. In the chart below, the choice of a quantitative data type results in a continuous linear axis scale:\n\nalt.Chart(data2000).mark_point().encode(\n    alt.X('fertility:Q')\n)\n\n\n\n\n\n\n\n\nY\nThe y encoding channel sets a mark’s vertical position (y-coordinate). Here we’ve added the cluster field using an ordinal (O) data type. The result is a discrete axis that includes a sized band, with a default step size, for each unique value:\n\nalt.Chart(data2000).mark_point().encode(\n    alt.X('fertility:Q'),\n    alt.Y('cluster:O')\n)\n\n\n\n\n\n\nWhat happens to the chart above if you swap the O and Q field types?\nIf we instead add the life_expect field as a quantitative (Q) variable, the result is a scatter plot with linear scales for both axes:\n\nalt.Chart(data2000).mark_point().encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q')\n)\n\n\n\n\n\n\nBy default, axes for linear quantitative scales include zero to ensure a proper baseline for comparing ratio-valued data. In some cases, however, a zero baseline may be meaningless or you may want to focus on interval comparisons. To disable automatic inclusion of zero, configure the scale mapping using the encoding scale attribute:\n\nalt.Chart(data2000).mark_point().encode(\n    alt.X('fertility:Q', scale=alt.Scale(zero=False)),\n    alt.Y('life_expect:Q', scale=alt.Scale(zero=False))\n)\n\n\n\n\n\n\nNow the axis scales no longer include zero by default. Some padding still remains, as the axis domain end points are automatically snapped to nice numbers like multiples of 5 or 10.\nWhat happens if you also add nice=False to the scale attribute above?\n\n\nSize\nThe size encoding channel sets a mark’s size or extent. The meaning of the channel can vary based on the mark type. For point marks, the size channel maps to the pixel area of the plotting symbol, such that the diameter of the point matches the square root of the size value.\nLet’s augment our scatter plot by encoding population (pop) on the size channel. As a result, the chart now also includes a legend for interpreting the size values.\n\nalt.Chart(data2000).mark_point().encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q')\n)\n\n\n\n\n\n\nIn some cases we might be unsatisfied with the default size range. To provide a customized span of sizes, set the range parameter of the scale attribute to an array indicating the smallest and largest sizes. Here we update the size encoding to range from 0 pixels (for zero values) to 1,000 pixels (for the maximum value in the scale domain):\n\nalt.Chart(data2000).mark_point().encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000]))\n)\n\n\n\n\n\n\n\n\nColor and Opacity\nThe color encoding channel sets a mark’s color. The style of color encoding is highly dependent on the data type: nominal data will default to a multi-hued qualitative color scheme, whereas ordinal and quantitative data will use perceptually ordered color gradients.\nHere, we encode the cluster field using the color channel and a nominal (N) data type, resulting in a distinct hue for each cluster value. Can you start to guess what the cluster field might indicate?\n\nalt.Chart(data2000).mark_point().encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000])),\n    alt.Color('cluster:N')\n)\n\n\n\n\n\n\nIf we prefer filled shapes, we can can pass a filled=True parameter to the mark_point method:\n\nalt.Chart(data2000).mark_point(filled=True).encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000])),\n    alt.Color('cluster:N')\n)\n\n\n\n\n\n\nBy default, Altair uses a bit of transparency to help combat over-plotting. We are free to further adjust the opacity, either by passing a default value to the mark_* method, or using a dedicated encoding channel.\nHere we demonstrate how to provide a constant value to an encoding channel instead of binding a data field:\n\nalt.Chart(data2000).mark_point(filled=True).encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000])),\n    alt.Color('cluster:N'),\n    alt.OpacityValue(0.5)\n)\n\n\n\n\n\n\n\n\nShape\nThe shape encoding channel sets the geometric shape used by point marks. Unlike the other channels we have seen so far, the shape channel can not be used by other mark types. The shape encoding channel should only be used with nominal data, as perceptual rank-order and magnitude comparisons are not supported.\nLet’s encode the cluster field using shape as well as color. Using multiple channels for the same underlying data field is known as a redundant encoding. The resulting chart combines both color and shape information into a single symbol legend:\n\nalt.Chart(data2000).mark_point(filled=True).encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000])),\n    alt.Color('cluster:N'),\n    alt.OpacityValue(0.5),\n    alt.Shape('cluster:N')\n)\n\n\n\n\n\n\n\n\nTooltips & Ordering\nBy this point, you might feel a bit frustrated: we’ve built up a chart, but we still don’t know what countries the visualized points correspond to! Let’s add interactive tooltips to enable exploration.\nThe tooltip encoding channel determines tooltip text to show when a user moves the mouse cursor over a mark. Let’s add a tooltip encoding for the country field, then investigate which countries are being represented.\n\nalt.Chart(data2000).mark_point(filled=True).encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000])),\n    alt.Color('cluster:N'),\n    alt.OpacityValue(0.5),\n    alt.Tooltip('country')\n)\n\n\n\n\n\n\nAs you mouse around you may notice that you can not select some of the points. For example, the largest dark blue circle corresponds to India, which is drawn on top of a country with a smaller population, preventing the mouse from hovering over that country. To fix this problem, we can use the order encoding channel.\nThe order encoding channel determines the order of data points, affecting both the order in which they are drawn and, for line and area marks, the order in which they are connected to one another.\nLet’s order the values in descending rank order by the population (pop), ensuring that smaller circles are drawn later than larger circles:\n\nalt.Chart(data2000).mark_point(filled=True).encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000])),\n    alt.Color('cluster:N'),\n    alt.OpacityValue(0.5),\n    alt.Tooltip('country:N'),\n    alt.Order('pop:Q', sort='descending')\n)\n\n\n\n\n\n\nNow we can identify the smaller country being obscured by India: it’s Bangladesh!\nWe can also now figure out what the cluster field represents. Mouse over the various colored points to formulate your own explanation.\nAt this point we’ve added tooltips that show only a single property of the underlying data record. To show multiple values, we can provide the tooltip channel an array of encodings, one for each field we want to include:\n\nalt.Chart(data2000).mark_point(filled=True).encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000])),\n    alt.Color('cluster:N'),\n    alt.OpacityValue(0.5),\n    alt.Order('pop:Q', sort='descending'),\n    tooltip = [\n        alt.Tooltip('country:N'),\n        alt.Tooltip('fertility:Q'),\n        alt.Tooltip('life_expect:Q')\n    ]   \n)\n\n\n\n\n\n\nNow we can see multiple data fields upon mouse over!\n\n\nColumn and Row Facets\nSpatial position is one of the most powerful and flexible channels for visual encoding, but what can we do if we already have assigned fields to the x and y channels? One valuable technique is to create a trellis plot, consisting of sub-plots that show a subset of the data. A trellis plot is one example of the more general technique of presenting data using small multiples of views.\nThe column and row encoding channels generate either a horizontal (columns) or vertical (rows) set of sub-plots, in which the data is partitioned according to the provided data field.\nHere is a trellis plot that divides the data into one column per `cluster` value:\n\nalt.Chart(data2000).mark_point(filled=True).encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000])),\n    alt.Color('cluster:N'),\n    alt.OpacityValue(0.5),\n    alt.Tooltip('country:N'),\n    alt.Order('pop:Q', sort='descending'),\n    alt.Column('cluster:N')\n)\n\n\n\n\n\n\nThe plot above does not fit on screen, making it difficult to compare all the sub-plots to each other! We can set the default width and height properties to create a smaller set of multiples. Also, as the column headers already label the cluster values, let’s remove our color legend by setting it to None. To make better use of space we can also orient our size legend to the 'bottom' of the chart.\n\nalt.Chart(data2000).mark_point(filled=True).encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000]),\n             legend=alt.Legend(orient='bottom', titleOrient='left')),\n    alt.Color('cluster:N', legend=None),\n    alt.OpacityValue(0.5),\n    alt.Tooltip('country:N'),\n    alt.Order('pop:Q', sort='descending'),\n    alt.Column('cluster:N')\n).properties(width=135, height=135)\n\n\n\n\n\n\nUnderneath the hood, the column and row encodings are translated into a new specification that uses the facet view composition operator. We will re-visit faceting in greater depth later on!\nIn the meantime, can you rewrite the chart above to facet into rows instead of columns?\n\n\nA Peek Ahead: Interactive Filtering\nIn later modules, we’ll dive into interaction techniques for data exploration. Here is a sneak peak: binding a range slider to the year field to enable interactive scrubbing through each year of data. Don’t worry if the code below is a bit confusing at this point, as we will cover interaction in detail later.\nDrag the slider back and forth to see how the data values change over time!\n\nselect_year = alt.selection_single(\n    name='select', fields=['year'], init={'year': 1955},\n    bind=alt.binding_range(min=1955, max=2005, step=5)\n)\n\nalt.Chart(data).mark_point(filled=True).encode(\n    alt.X('fertility:Q', scale=alt.Scale(domain=[0,9])),\n    alt.Y('life_expect:Q', scale=alt.Scale(domain=[0,90])),\n    alt.Size('pop:Q', scale=alt.Scale(domain=[0, 1200000000], range=[0,1000])),\n    alt.Color('cluster:N', legend=None),\n    alt.OpacityValue(0.5),\n    alt.Tooltip('country:N'),\n    alt.Order('pop:Q', sort='descending')\n).add_selection(select_year).transform_filter(select_year)"
  },
  {
    "objectID": "images/altair_marks_encoding.out.html#graphical-marks",
    "href": "images/altair_marks_encoding.out.html#graphical-marks",
    "title": "Data Types, Graphical Marks, and Visual Encoding Channels",
    "section": "Graphical Marks",
    "text": "Graphical Marks\nOur exploration of encoding channels above exclusively uses point marks to visualize the data. However, the point mark type is only one of the many geometric shapes that can be used to visually represent data. Altair includes a number of built-in mark types, including:\n\nmark_area() - Filled areas defined by a top-line and a baseline.\nmark_bar() - Rectangular bars.\nmark_circle() - Scatter plot points as filled circles.\nmark_line() - Connected line segments.\nmark_point() - Scatter plot points with configurable shapes.\nmark_rect() - Filled rectangles, useful for heatmaps.\nmark_rule() - Vertical or horizontal lines spanning the axis.\nmark_square() - Scatter plot points as filled squares.\nmark_text() - Scatter plot points represented by text.\nmark_tick() - Vertical or horizontal tick marks.\n\nFor a complete list, and links to examples, see the Altair marks documentation. Next, we will step through a number of the most commonly used mark types for statistical graphics.\n\nPoint Marks\nThe point mark type conveys specific points, as in scatter plots and dot plots. In addition to x and y encoding channels (to specify 2D point positions), point marks can use color, size, and shape encodings to convey additional data fields.\nBelow is a dot plot of fertility, with the cluster field redundantly encoded using both the y and shape channels.\n\nalt.Chart(data2000).mark_point().encode(\n    alt.X('fertility:Q'),\n    alt.Y('cluster:N'),\n    alt.Shape('cluster:N')\n)\n\n\n\n\n\n\nIn addition to encoding channels, marks can be stylized by providing values to the mark_*() methods.\nFor example: point marks are drawn with stroked outlines by default, but can be specified to use filled shapes instead. Similarly, you can set a default size to set the total pixel area of the point mark.\n\nalt.Chart(data2000).mark_point(filled=True, size=100).encode(\n    alt.X('fertility:Q'),\n    alt.Y('cluster:N'),\n    alt.Shape('cluster:N')\n)\n\n\n\n\n\n\n\n\nCircle Marks\nThe circle mark type is a convenient shorthand for point marks drawn as filled circles.\n\nalt.Chart(data2000).mark_circle(size=100).encode(\n    alt.X('fertility:Q'),\n    alt.Y('cluster:N'),\n    alt.Shape('cluster:N')\n)\n\n\n\n\n\n\n\n\nSquare Marks\nThe square mark type is a convenient shorthand for point marks drawn as filled squares.\n\nalt.Chart(data2000).mark_square(size=100).encode(\n    alt.X('fertility:Q'),\n    alt.Y('cluster:N'),\n    alt.Shape('cluster:N')\n)\n\n\n\n\n\n\n\n\nTick Marks\nThe tick mark type conveys a data point using a short line segment or “tick”. These are particularly useful for comparing values along a single dimension with minimal overlap. A dot plot drawn with tick marks is sometimes referred to as a strip plot.\n\nalt.Chart(data2000).mark_tick().encode(\n    alt.X('fertility:Q'),\n    alt.Y('cluster:N'),\n    alt.Shape('cluster:N')\n)\n\n\n\n\n\n\n\n\nBar Marks\nThe `bar` mark type draws a rectangle with a position, width, and height.\nThe plot below is a simple bar chart of the population (`pop`) of each country.\n\nalt.Chart(data2000).mark_bar().encode(\n    alt.X('country:N'),\n    alt.Y('pop:Q')\n)\n\n\n\n\n\n\nThe bar width is set to a default size. We will discuss how to adjust the bar width later in this notebook. (A subsequent notebook will take a closer look at configuring axes, scales, and legends.)\nBars can also be stacked. Let’s change the x encoding to use the cluster field, and encode country using the color channel. We’ll also disable the legend (which would be very long with colors for all countries!) and use tooltips for the country name.\n\nalt.Chart(data2000).mark_bar().encode(\n    alt.X('cluster:N'),\n    alt.Y('pop:Q'),\n    alt.Color('country:N', legend=None),\n    alt.Tooltip('country:N')\n)\n\n\n\n\n\n\nIn the chart above, the use of the color encoding channel causes Altair / Vega-Lite to automatically stack the bar marks. Otherwise, bars would be drawn on top of each other! Try adding the parameter stack=None to the y encoding channel to see what happens if we don’t apply stacking…\nThe examples above create bar charts from a zero-baseline, and the y channel only encodes the non-zero value (or height) of the bar. However, the bar mark also allows you to specify starting and ending points to convey ranges.\nThe chart below uses the x (starting point) and x2 (ending point) channels to show the range of life expectancies within each regional cluster. Below we use the min and max aggregation functions to determine the end points of the range; we will discuss aggregation in greater detail in the next notebook!\nAlternatively, you can use x and width to provide a starting point plus offset, such that x2 = x + width.\n\nalt.Chart(data2000).mark_bar().encode(\n    alt.X('min(life_expect):Q'),\n    alt.X2('max(life_expect):Q'),\n    alt.Y('cluster:N')\n)\n\n\n\n\n\n\n\n\nLine Marks\nThe line mark type connects plotted points with line segments, for example so that a line’s slope conveys information about the rate of change.\nLet’s plot a line chart of fertility per country over the years, using the full, unfiltered global development data frame. We’ll again hide the legend and use tooltips instead.\n\nalt.Chart(data).mark_line().encode(\n    alt.X('year:O'),\n    alt.Y('fertility:Q'),\n    alt.Color('country:N', legend=None),\n    alt.Tooltip('country:N')\n).properties(\n    width=400\n)\n\n\n\n\n\n\nWe can see interesting variations per country, but overall trends for lower numbers of children per family over time. Also note that we set a custom width of 400 pixels. Try changing (or removing) the widths and see what happens!\nLet’s change some of the default mark parameters to customize the plot. We can set the strokeWidth to determine the thickness of the lines and the opacity to add some transparency. By default, the line mark uses straight line segments to connect data points. In some cases we might want to smooth the lines. We can adjust the interpolation used to connect data points by setting the interpolate mark parameter. Let’s use 'monotone' interpolation to provide smooth lines that are also guaranteed not to inadvertently generate “false” minimum or maximum values as a result of the interpolation.\n\nalt.Chart(data).mark_line(\n    strokeWidth=3,\n    opacity=0.5,\n    interpolate='monotone'\n).encode(\n    alt.X('year:O'),\n    alt.Y('fertility:Q'),\n    alt.Color('country:N', legend=None),\n    alt.Tooltip('country:N')\n).properties(\n    width=400\n)\n\n\n\n\n\n\nThe line mark can also be used to create slope graphs, charts that highlight the change in value between two comparison points using line slopes.\nBelow let’s create a slope graph comparing the populations of each country at minimum and maximum years in our full dataset: 1955 and 2005. We first create a new Pandas data frame filtered to those years, then use Altair to create the slope graph.\nBy default, Altair places the years close together. To better space out the years along the x-axis, we can indicate the size (in pixels) of discrete steps along the width of our chart as indicated by the comment below. Try adjusting the width step value below and see how the chart changes in response.\n\ndataTime = data.loc[(data['year'] == 1955) | (data['year'] == 2005)]\n\nalt.Chart(dataTime).mark_line(opacity=0.5).encode(\n    alt.X('year:O'),\n    alt.Y('pop:Q'),\n    alt.Color('country:N', legend=None),\n    alt.Tooltip('country:N')\n).properties(\n    width={\"step\": 50} # adjust the step parameter\n)\n\n\n\n\n\n\n\n\nArea Marks\nThe area mark type combines aspects of line and bar marks: it visualizes connections (slopes) among data points, but also shows a filled region, with one edge defaulting to a zero-valued baseline.\nThe chart below is an area chart of population over time for just the United States:\n\ndataUS = data.loc[data['country'] == 'United States']\n\nalt.Chart(dataUS).mark_area().encode(\n    alt.X('year:O'),\n    alt.Y('fertility:Q')\n)\n\n\n\n\n\n\nSimilar to line marks, area marks support an interpolate parameter.\n\nalt.Chart(dataUS).mark_area(interpolate='monotone').encode(\n    alt.X('year:O'),\n    alt.Y('fertility:Q')\n)\n\n\n\n\n\n\nSimilar to bar marks, area marks also support stacking. Here we create a new data frame with data for the three North American countries, then plot them using an area mark and a color encoding channel to stack by country.\n\ndataNA = data.loc[\n    (data['country'] == 'United States') |\n    (data['country'] == 'Canada') |\n    (data['country'] == 'Mexico')\n]\n\nalt.Chart(dataNA).mark_area().encode(\n    alt.X('year:O'),\n    alt.Y('pop:Q'),\n    alt.Color('country:N')\n)\n\n\n\n\n\n\nBy default, stacking is performed relative to a zero baseline. However, other stack options are available:\n\ncenter - to stack relative to a baseline in the center of the chart, creating a streamgraph visualization, and\nnormalize - to normalize the summed data at each stacking point to 100%, enabling percentage comparisons.\n\nBelow we adapt the chart by setting the y encoding stack attribute to center. What happens if you instead set it normalize?\n\nalt.Chart(dataNA).mark_area().encode(\n    alt.X('year:O'),\n    alt.Y('pop:Q', stack='center'),\n    alt.Color('country:N')\n)\n\n\n\n\n\n\nTo disable stacking altogether, set the stack attribute to None. We can also add opacity as a default mark parameter to ensure we see the overlapping areas!\n\nalt.Chart(dataNA).mark_area(opacity=0.5).encode(\n    alt.X('year:O'),\n    alt.Y('pop:Q', stack=None),\n    alt.Color('country:N')\n)\n\n\n\n\n\n\nThe area mark type also supports data-driven baselines, with both the upper and lower series determined by data fields. As with bar marks, we can use the x and x2 (or y and y2) channels to provide end points for the area mark.\nThe chart below visualizes the range of minimum and maximum fertility, per year, for North American countries:\n\nalt.Chart(dataNA).mark_area().encode(\n    alt.X('year:O'),\n    alt.Y('min(fertility):Q'),\n    alt.Y2('max(fertility):Q')\n).properties(\n    width={\"step\": 40}\n)\n\n\n\n\n\n\nWe can see a larger range of values in 1995, from just under 4 to just under 7. By 2005, both the overall fertility values and the variability have declined, centered around 2 children per familty.\nAll the area mark examples above use a vertically oriented area. However, Altair and Vega-Lite support horizontal areas as well. Let’s transpose the chart above, simply by swapping the x and y channels.\n\nalt.Chart(dataNA).mark_area().encode(\n    alt.Y('year:O'),\n    alt.X('min(fertility):Q'),\n    alt.X2('max(fertility):Q')\n).properties(\n    width={\"step\": 40}\n)"
  },
  {
    "objectID": "images/altair_marks_encoding.out.html#summary-1",
    "href": "images/altair_marks_encoding.out.html#summary-1",
    "title": "Data Types, Graphical Marks, and Visual Encoding Channels",
    "section": "Summary",
    "text": "Summary\nWe’ve completed our tour of data types, encoding channels, and graphical marks! You should now be well-equipped to further explore the space of encodings, mark types, and mark parameters. For a comprehensive reference – including features we’ve skipped over here! – see the Altair marks and encoding documentation.\nIn the next module, we will look at the use of data transformations to create charts that summarize data or visualize new derived fields. In a later module, we’ll examine how to further customize your charts by modifying scales, axes, and legends.\nInterested in learning more about visual encoding?\n\nBertin’s taxonomy of visual encodings from Sémiologie Graphique, as adapted by Mike Bostock.\n\nThe systematic study of marks, visual encodings, and backing data types was initiated by Jacques Bertin in his pioneering 1967 work Sémiologie Graphique (The Semiology of Graphics). The image above illustrates position, size, value (brightness), texture, color (hue), orientation, and shape channels, alongside Bertin’s recommendations for the data types they support.\nThe framework of data types, marks, and channels also guides automated visualization design tools, starting with Mackinlay’s APT (A Presentation Tool) in 1986 and continuing in more recent systems such as Voyager and Draco.\nThe identification of nominal, ordinal, interval, and ratio types dates at least as far back as S. S. Steven’s 1947 article On the theory of scales of measurement."
  },
  {
    "objectID": "src/projects/project-2.html",
    "href": "src/projects/project-2.html",
    "title": "PharmAccess Foundation",
    "section": "",
    "text": "Story about data commons in Africa …"
  },
  {
    "objectID": "src/blog/listing.html",
    "href": "src/blog/listing.html",
    "title": "Blog posts",
    "section": "",
    "text": "The Voltron stack\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nAnalytical problem solving\n\n\nbased on causal, correlational and deductive models\n\n\n\n\nmodeling\n\n\n\n\nI co-authored a peer-reviewed paper that compares the different modeling approaches for analytical problem solving.\n\n\n\n\n\n\nMar 10, 2022\n\n\nDaniel Kapitan\n\n\n\n\n\n\n  \n\n\n\n\nComet charts in Python\n\n\nVisualizing statistical mix effects and Simpson’s paradox with Altair\n\n\n\n\nvisualization\n\n\ncoding\n\n\n\n\n\n\n\n\n\n\n\nJan 29, 2021\n\n\nDaniel Kapitan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "src/blog/posts/some-post.html",
    "href": "src/blog/posts/some-post.html",
    "title": "The Voltron stack",
    "section": "",
    "text": "Data engineering isn’t sexy, yet it is a sine qua non to extract value from data. I have had my fair share of data engineering projects that did not quite live up to expectations. Data engineering, or data warehousing, is quite easy to get wrong. How hard can it be to integrate data from different sources, joining them on relevant business keys, and you are ready to develop funky machine learning models for your client. Right? Wrong. In this blog post I will sum up the most important lessons I have stumbled upon in the past 10 years. It is work in progress, so stay tuned for updates."
  },
  {
    "objectID": "src/blog/posts/some-post.html#functional-data-engineering-as-a-starting-point.",
    "href": "src/blog/posts/some-post.html#functional-data-engineering-as-a-starting-point.",
    "title": "The Voltron stack",
    "section": "Functional data engineering as a starting point.",
    "text": "Functional data engineering as a starting point.\nIn this excellent article (pdf included below), Maxime Beauchemin explains how the tenets of functional programming can be applied to data engineering. Particularly relevant are:\n\nusing pure tasks in your data pipeline\nregard table partitions as immutable objects\nusing a persistent and immutable staging areas\nGoogle BigQuery as your datawarehouse storage platform\n\nI am a big fan of BigQuery. You can throw structured and unstructured data at it. It performs well with little management overhead compared to e.g. PostgreSQL data warehouses I had to maintain. And pricing is attractive: the cost for data-at-rest is the lowest of all cloud platform (as far as I know), hence I don’t have no qualms to really ingest as much data as possible.\nGoogle’s solution design of BigQuery for datawarehouse practitioners is a good starting point. With the functional engineering approach in mind, my current best practice for data warehousing which I have applied e.g. for the Happi project consists of the following components:\n\nUse BigQuery’s date partitioning to ingest data into your staging schemas. Doing so, you can make optimal use of the parallel processing of BigQuery. Data generated by apps can even by ingested real-time by pushing data in newline delimited json format to BigQuery.\nUse a lightweight workflow management library like Prefect to orchestrate and schedule all your idempotent tasks.\nUse templated SQL for efficiently and reliably generating datamarts. Given the underlying Dremmel storage engine, we don’t need to bother with creating dimension and fact tables per se, although we still apply - Kimball’s concept of dimensional modeling)."
  },
  {
    "objectID": "src/about.html",
    "href": "src/about.html",
    "title": "About me",
    "section": "",
    "text": "As a data scientist, AI expert, architect, advisor, lecturer and mentor, I help people and organizations master data and AI in different roles, ultimately to create sustainable change with digital technologies."
  },
  {
    "objectID": "src/about.html#roles-that-i-fulfill",
    "href": "src/about.html#roles-that-i-fulfill",
    "title": "About me",
    "section": "Roles that I fulfill",
    "text": "Roles that I fulfill\nDaniel works as an advisor, architect, lecturer, applied researcher and mentor. He is one of the few specialists in the Netherlands who has the unique combination of corporate experience, scientific research and executive education. Through these various roles he aims to apply ‘AI for good’. He lectures on data and AI, publishes regularly and is an active member of the Data Sharing Coalition to foster data solidarity and secure data sharing for tackling societal issues."
  },
  {
    "objectID": "src/about.html#areas-of-expertise",
    "href": "src/about.html#areas-of-expertise",
    "title": "About me",
    "section": "Areas of expertise",
    "text": "Areas of expertise\n\ndata-centric AI\nknowledge science\ndata architecture\nfederated learning\n\nIndustry focus: healthcare, public sector, as well as scale-ups for which data and AI are a core element of their strategy."
  }
]